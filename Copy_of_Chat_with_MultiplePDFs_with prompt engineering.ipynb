{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axkWYsFvONOp"
      },
      "source": [
        "# **W/O RAG**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g1xBDyZP5C7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "#!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "#!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "#!pip install -q -U einops\n",
        "!pip install -q -U safetensors\n",
        "!pip install -q -U torch\n",
        "!pip install transformers accelerate einops xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2McuJEjN43aK",
        "outputId": "d7f9d91e-936a-4859-fc9d-1e9d6ed753c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.24-cp310-cp310-manylinux2014_x86_64.whl (218.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.2/218.2 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1)\n",
            "Collecting torch>=1.10.0 (from accelerate)\n",
            "  Downloading torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: einops, torch, xformers\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1\n",
            "    Uninstalling torch-2.2.1:\n",
            "      Successfully uninstalled torch-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True\n",
        ")"
      ],
      "metadata": {
        "id": "LmiQS8h3-I4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "#model_name = \"tiiuae/falcon-7b-instruct\"\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\""
      ],
      "metadata": {
        "id": "zTXdq3Me-MR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
        "                                             device_map=\"auto\",\n",
        "                                             #device_map = device_map,\n",
        "        quantization_config=quantization_config, trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549,
          "referenced_widgets": [
            "cf038807301e4f14b9c0cccd39c6be7b",
            "ab24449d7e124da082de460b6a76c71a",
            "4e7e238907494cb4a5da0d84a1f271c7",
            "6ed1149021da451a931edb5e78cb564e",
            "e9cc1a19264946a9a10e1e13231c2050",
            "19340b5991ef40ebb4141ea2e6328ed2",
            "c80ff6d9933946408e2e1dba4e69f0ed",
            "7d5ee8fa99ee4edca3a2f4dfa0091807",
            "03eec5b42b244ccf84790289a5bc541f",
            "ed2e3159964d4ed3a322b399ef6aed55",
            "5eea716f26b241b88727b4235c58610c",
            "9ca9d02b04174fd6b85d3c4787c4c18a",
            "128edeafaa3543528f6ae389bc41283a",
            "73618b8d71044e818180c66cfed1edba",
            "8b2f856d92ff42779dc84ceb3c53d792",
            "229eaf19a86c420eafb6a461f203de92",
            "37ee6356493d4e7cb33c8c743cb6bd15",
            "c842e4b235544cfb9d231e0d6a144ec1",
            "6d2658a7ff03478394bba977a1c385ca",
            "3f4513a5afba44838fc0e63e7fb30da0",
            "4c3a530ed3cf4eff8a608ca808a458a1",
            "98ba4f441924490fbd2b7a91262f3779",
            "7608deba9bd449a38f54fa5ed811e76f",
            "da990c20d4b44596bc9d2621d97e38ba",
            "82245351de52418ebd069006406ac330",
            "77c90d4e101a4983a79ca0036bca51de",
            "182f22fdacbf41fdac3109fa8eec80df",
            "670cbbcaad1c4f2bb8bc889ac6278288",
            "4457737d60d0403eb96c3875a136fc0f",
            "b30bdecd721d4c6e9bbef5b562db8f20",
            "31180285472e4a51aad9025822ab72f7",
            "46662585eab04a8f9a03dfe1ff6772cb",
            "b42a642f2780409395f579cc73ba28a7",
            "521641c2e79a4e01bceffacbbed95408",
            "46bf0b908c034e218e4b9084466ded98",
            "20ba35bf276e440192e4b6756059db90",
            "f4aa340925c344709214a821b77b70bd",
            "cf8b6267b3594beaba2c30b4497b9cac",
            "2bab7a82c47e4a2b91cc6fd6acbb574d",
            "0e97bf36b54243f8bd02323015ffe6fd",
            "5d13a003082d4cd6a5b26844b4602e69",
            "131bf91b48624691bbdc94f12017fc49",
            "0791297bf2824ff99e94ee7f2613cfff",
            "5cb898688b3b4b71ac60b8171eb4ae0f",
            "4670f6f23d1a46d08824c107ddf82210",
            "f4301e8c6a3f424a9da2d3840ca38651",
            "c128c15e0b264d06acefa68c592c2c95",
            "9a6fb76b58434ce1ba104d6b019ee114",
            "9b9e619fe2a84472ae8e7e692223002f",
            "f9812121fdbe4b8a9527c93f0271f25a",
            "f74638f3b6af4d49a62d467935a56b90",
            "3b1d0578e3ea48cc871459aab048100a",
            "322f18acb92e44b4a98dc5351cc37709",
            "ef2788790db94242b58237f152f622e0",
            "1abda04b469b4586831b052e7c672de4",
            "8ef5ce43e7fa44718b2cf9ee4898a1d7",
            "2481a0ec58814004aadd4613adcd7822",
            "d27648abb27d4bf0b4ef7c01f2edb48b",
            "fbbee23c09f54ef5b8fb9a921fc83fbc",
            "4f04cad715cd481d8fc473300447fd28",
            "336db333b45543819eee69a68c417933",
            "b9a4a13c67194bd3a5ae015fa72d7932",
            "db4b230996794fbebea74816d372c6c4",
            "e570ec5210414550a790d415c475a411",
            "910cb9d9d09d49558515f7c39b8a9a00",
            "a990f9a6e4c54b62a55851c93bd66079",
            "1ed190d0dfc447a5a385e3ea8745691f",
            "d10b12fda31a4b0fac3002fea35b419f",
            "059e6fc879d54c9bacc2f49b28136d94",
            "e6fddf78977d4dfe9b583bbe6895a004",
            "a420ce9642b64bd581d473c6b6a0c0b9",
            "7796597221e24a1ebacac6c7b4725bb2",
            "db3171252414451a9a37162e1a4537b5",
            "9ba042a699f64602af45fa83207f8943",
            "ead64e77c8c34b62b319ebb043f23231",
            "99e9506038994010a432ae057339733c",
            "85398197a1fc44abb6e86e1340598719",
            "9a4d2fc4191c4f2699c968668dda27ba",
            "4b7a36ad4a254f9586f5a77ff7e2d522",
            "1eb3d67e7de04d94a961e6596d546abc",
            "78dec8af0b894a8881f180c8ac5c96a1",
            "c57c6b3202974bce956216e3fc67b017",
            "96e1b0f72a2345ca9c646408c3e1f2a6",
            "b971f441f202452c856637f9c2e9b054",
            "83329472e0054cd4ba1f1040001bd382",
            "5a1097da66154198b7032849fe6c056b",
            "f1be905ee5f34c02b4881dbc08371730",
            "59581290642e40cfa979ad61a52ce226",
            "cd83353a13694f8a91a3b21ba71e8b56",
            "e6e90dd9e8bc4406be7af3c471dd90d0",
            "132c74b460fa4e3c9b27b50a803c42d7",
            "f7dfbf6fd1c4476b8f2289607f1bbc6d",
            "8a6525bcae714c58a2fdc38cecb959c1",
            "d45766943a1042a0aab04c6deeb5ea3b",
            "d59d78cb48db4e3e89eccd70166249c0",
            "f2e570b127d441c480473f0c164aea6b",
            "4fbc505590934d9f9377ba9518a7138c",
            "813e705a20b6466181e52f815d058039",
            "1a694b86cb6344beb6e2fac50f3a1df2",
            "cf3307018179426190b64139d4121b07",
            "8101634aef0d40f5ae7c12bc2ef3ab68",
            "9b14627b6c0147f78f6b1e0cabbbe134",
            "1181bbde004a40d1b5dd4d1059495f17",
            "34ef3a623efa4277afaec8ed06091b0f",
            "0d8a97cf8713481d98e162c11a071eb1",
            "ee4a13ff895942f2a70414c7846e54bc",
            "e69c3c0486db41699700529ebc55ef07",
            "9aa6c8870e064c12a30a43eb96afcb8c",
            "1194bb3dba814bfd989ec12d9ef18b8f",
            "244fefbc6130404898c4052a64280d08",
            "5fadd50330c84c9ea5b11a770313735c",
            "cae21ce7492042bda33fb2977315b615",
            "adf76ed984ee44ac90ba6261d9ebfd9e",
            "3903687f16404d52ba6257c597cf20b3",
            "c21f855ccb1e4789be6de07804e09fe0",
            "fd9ff14d0af84ea0ade0c861ed5c82be",
            "7ad9639ba00e401c917ea8bfc35ff15b",
            "0b582f00439e46ce90f1ffdf7c89b9c3",
            "b80eab38f95740a2bc501c4587b11353",
            "8f158a077dd64ac1a70d0887cdea5458",
            "c4b2e8f632e94e8f81bebd54dcb3f00c"
          ]
        },
        "id": "G5giWjjk-MOd",
        "outputId": "1707addd-5695-4e1d-8bcd-7db59ab4fd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf038807301e4f14b9c0cccd39c6be7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ca9d02b04174fd6b85d3c4787c4c18a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7608deba9bd449a38f54fa5ed811e76f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "521641c2e79a4e01bceffacbbed95408"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4670f6f23d1a46d08824c107ddf82210"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ef5ce43e7fa44718b2cf9ee4898a1d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ed190d0dfc447a5a385e3ea8745691f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a4d2fc4191c4f2699c968668dda27ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd83353a13694f8a91a3b21ba71e8b56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf3307018179426190b64139d4121b07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5fadd50330c84c9ea5b11a770313735c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use case here- text gen\n",
        "#parameters\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    #torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        "    #device_map=device_map,\n",
        "    max_length=200,\n",
        "    use_cache=True,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "CM5kyBHJ-ML8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test the llm-falcon7b by providing it with a query\n",
        "sequences = pipeline(\n",
        "    \"Create a list of four important things a pilot should take note of\"\n",
        ")\n",
        "\n",
        "#for seq in sequences:\n",
        "#    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "Q-tZekaf-MJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX1hnPmSY3yG",
        "outputId": "eb04c2ce-d8a9-4b27-d8e7-4ef755e8271e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'Create a list of four important things a pilot should take note of when approaching a new airport?\\n\\nThe four most important things are the windsock, the wind direction indicator on the VOR, the runway surface and the runway in numbers (for a pilot who does not have the approach plate on hand).\\n\\n3. How can a pilot determine the direction the wind is blowing from when he approaches an unfamiliar airport?\\n\\nThe wind direction indicator is located on the VOR, which provides a compass-type direction to the wind.\\n\\n4. What information should a pilot note in the airport diagram?\\n\\nThe following should be checked: (1) runway surface (2) runway in numbers (3) windsock and wind direction indicators (4) any obstacles or terrain.\\n\\n5. If a pilot has a headwind and wants to take advantage of it to shorten his flight, which airport would be best to leave from?'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt 2\n",
        "sequences = pipeline(\n",
        "     \"tell me about the movie teri baaton mein\"\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "OgNe-I4G-MG0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227e1883-c2cd-41fb-eaa6-e50ee572cfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: tell me about the movie teri baaton mein. #DabanggReloaded\n",
            "\n",
            "> Dabangg Reloaded:\n",
            ">\n",
            "> Posted by Salman Khan on Monday, September 19, 2016\n",
            "\n",
            "Dabangg 3 is set to hit the marquee in December 2018.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "     \"what is falcon 180b llm?\"\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "WvmLG1zp-MEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de37e71-ef4c-46fd-fcc2-1d9916f6d7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: what is falcon 180b llm?\n",
            "\n",
            "Falcon 180b llm\n",
            "\n",
            "The Falcon 180B Llm is the next generation of a classic design. It has a modern and sleek look that will complement any style of home. This is a great option if you are looking for a way to improve your home’s efficiency and comfort. The 180B Llm has all of the features you would expect from a Falcon product, plus more. You can find the Falcon 180B Llm at your local Falcon dealer.\n",
            "\n",
            "## 6. Falcon F180B Llm: What Is It?\n",
            "\n",
            "F180b Llm Falcon\n",
            "\n",
            "Falcon F180b Llm is an innovative and powerful laser-cutting machine designed to help businesses achieve the highest level of laser cut accuracy with minimal downtime. With a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = pipeline(\n",
        "     \"Could you explain how the use of 4,096 A100s contributed to the training of Falcon-180B\"\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh_sM9vl1xnn",
        "outputId": "a4402ada-bb12-40be-d381-f509692ecb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: Could you explain how the use of 4,096 A100s contributed to the training of Falcon-180B?\n",
            "\n",
            "We have 4,096 A100 GPUs on our servers, and we trained Falcon-180B for 92 hours with a total of 300 million iterations on these A100 GPUs. In each training iteration, 8,000 GPUs were activated to achieve higher model performance, and 2,000 GPUs were activated to reduce power consumption while maintaining good model performance.\n",
            "\n",
            "### 2. The number of parameters of Falcon-180B is 180 billion. How long did it take for this model to train?\n",
            "\n",
            "For training, we used 4,096 servers with 4,096 A100 GPUs, and it took 240 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequences = pipeline(\n",
        "     \"How did the use of AWS affect the overall development timeline of the Falcon series?\"\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUuTQoWg2-Im",
        "outputId": "6aeaaa4e-7040-4e0b-8ee9-7a27a66c6a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: How did the use of AWS affect the overall development timeline of the Falcon series?\n",
            "\n",
            "The Falcon series was originally designed and created in 2018, and it’s been a very successful series to date. We are currently on Series 3, and we’ve released a number of new games. One of the things we’re looking at, in terms of AWS and the impact on our development time, is that we are trying to release a lot more content to our customers more rapidly.\n",
            "\n",
            "With our first Falcon series, we released one game a month, every month, for a year. That is a massive undertaking. Now we’re looking at releasing multiple games every week, and the ability to do that is largely based on the fact that we are able to spin up the environments so quickly, and be able to develop those games in a matter of hours, rather than months. AWS is allowing us to be able to do that,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDozFrtdP2Bh"
      },
      "source": [
        "# **With** **RAG**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "fQF4UzIDs02V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install huggingface-hub\n",
        "!pip install pypdf\n",
        "!pip -q install accelerate\n",
        "!pip install llama-cpp-python\n",
        "!pip -q install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "fBLfNDyG9pOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66d02cc4-cdc0-441a-ad5b-1e2a29badfc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.48-cp310-cp310-manylinux_2_35_x86_64.whl size=2619668 sha256=6f4e9a4d4e36c4d9336212e14b0b2ca820fb464f7b2c50c9f7da6b6373984c45\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/fb/09/4b6aea8065e4c2e5914d250f73d894c806f336c9488df19f30\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.48\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "FkYtCf3a9lrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load pdf files\n",
        "loader = PyPDFDirectoryLoader(\"/content/sample_data/Data/\")\n",
        "data = loader.load()"
      ],
      "metadata": {
        "id": "QH56Vm8e8pQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "M4e6mk7f8qTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94fb8dd9-3921-4b14-d91e-2f8528365ab3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='The Falcon Series of Open Language Models\\nThe Falcon LLM Team∗\\nEbtesam Almazrouei Hamza Alobeidli Abdulaziz Alshamsi Alessandro Cappelli\\nRuxandra Cojocaru Mérouane Debbah Etienne Go ffinet Daniel Hesslow Julien Launay\\nQuentin Malartic Daniele Mazzotta Badreddine Noune Baptiste Pannier Guilherme Penedo\\nTechnology Innovation Institute, Abu Dhabi\\nhttps://huggingface.co/tiiuae/\\nAbstract\\nWe introduce the Falcon series: 7B, 40B, and 180B parameters causal decoder-\\nonly models trained on a diverse high-quality corpora predominantly assembled\\nfrom web data. The largest model, Falcon-180B, has been trained on over 3.5\\ntrillion tokens of text–the largest openly documented pretraining run. Falcon-180B\\nsignificantly outperforms models such as PaLM or Chinchilla, and improves upon\\nconcurrently developed models such as LLaMA 2 or Inflection-1. It nears the\\nperformance of PaLM-2-Large at a reduced pretraining and inference cost, making\\nit, to our knowledge, one of the three best language models in the world along\\nwith GPT-4 and PaLM-2-Large. We report detailed evaluations, as well as a deep\\ndive into the methods and custom tooling employed to pretrain Falcon. Notably,\\nwe report on our custom distributed training codebase, allowing us to e fficiently\\npretrain these models on up to 4,096 A100s on cloud AWS infrastructure with\\nlimited interconnect. We release a 600B tokens extract of our web dataset, as well\\nas the Falcon-7 /40/180B models under a permissive license to foster open-science\\nand accelerate the development of an open ecosystem of large language models.\\nPaLM PaLM-2 Small PaLM-2 Medium PaLM-2 Large Falcon-180B6065707580Aggregate 1-shot performance [acc. %]\\nFigure 1: The Falcon series of models achieves competitive performance, with Falcon-180B\\nnearly matching the 1-shot performance of PaLM-2 Large. 1-shot performance of PaLM (Chowd-\\nhery et al., 2022), PaLM-2 (Anil et al., 2023), and Falcon-180B, on a set of tasks from Brown et al.\\n(2020). These evaluation results are only a small snapshot of our evaluations, see Section 6 for details\\nand comparisons with GPT-3.5 /4, LLaMA-1 /2, and Inflection-1.\\n1Authors listed alphabetically, contributions detailed in Appendix A. Correspondence to falconllm@tii.aearXiv:2311.16867v2  [cs.CL]  29 Nov 2023', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 0}), Document(page_content='1 Introduction\\nThe on-going Cambrian explosion of language models has been primarily fueled by the unique\\nscalability of popular Transformer-based recipes. This scalability manifests over multiple axes:\\n•Performance scalability (and predictability). Increase in pretraining compute budgets\\nsystematically yield improvements in language modeling capabilities, in a consistent and\\npredictable way (Kaplan et al., 2020). Falcon-180B is the first publicly documented GPT-3-\\nsized model to follow the updated scaling law recommendations of Ho ffmann et al. (2022),\\nwith a total pretraining length of 3,500 billion tokens, without any upsampling.\\n•Data scalability. To scale-up pretraining e fficiently, and to decouple pretraining and\\ninference compute, increasingly large models should be trained for longer, on larger corpora.\\nTo sustain the Falcon series, we developed RefinedWeb (Penedo et al., 2023), a 5 trillion\\ntokens high-quality filtered and deduplicated web dataset–the largest publicly documented.\\n•Hardware scalability. Transformer models (Vaswani et al., 2017) are naturally suited for\\nmodern GEMM optimized hardware, allowing their training and inference to be e fficiently\\ndistributed over a large number of accelerators (Narayanan et al., 2021b; Pope et al., 2023).\\nWith Falcon-180B, we demonstrate scaling-up pretraining to 4,096 A100 40GB with only\\n50 Gbps interconnect per accelerator on cost-e fficient AWS cloud infrastructure.\\nBuilding upon these fundamentals, increasingly large language models give rise to so-called emergent\\ncapabilities (Wei et al., 2022a). These capabilities can be further tailored to human preference, to\\nbuild instruction-following or chatty models (Ouyang et al., 2022). All together these methods\\nhave lead to the widespread deployment of large language models in customer-facing applications,\\nsuch as ChatGPT (GPT-3.5 /4, OpenAI (2023a)), Claude, Bard (PaLM-2, Anil et al. (2023)), or Pi\\n(Inflection-1, Inflection (2023)). In this paper, we primarily report on the pretraining alone of the\\nFalcon series of models, and leave further downstream finetuning and alignment to future works.\\nThe Falcon series is made of three causal decoder-only models trained on up to 4,096 A100. We\\nassembled a pretraining dataset of 3,500 billion tokens, predominantly sourced from our work on\\nRefinedWeb (Penedo et al., 2023)–a massive filtered and deduplicated web dataset. The architecture\\nof the models is based on PaLM (Chowdhery et al., 2022), although we independently validated each\\ndecision, ultimately resulting in some minor tweaks–see Section 4 for details. The Falcon series\\nleverages extensive custom tooling (i.e., pretraining codebase, data pipeline), of which development\\nstarted in August 2022, with training of the models kicked-o ffin December 2022. In-depth evaluation\\nshows that the Falcon series is competitive across scale, and that Falcon-180B nears the performance\\nof PaLM-2 Large, positioning it as the best open model and in the top-3 of the best language models.\\nContributions. With this paper and the Falcon series, we make the following contributions:\\n•Public documentation of the pretraining of a large-scale model. Recent state-of-the-art\\nmodels have been scarcely documented, hindering further research and progress in the field.\\nAt variance with these works, we extensively document the pretraining of the Falcon series.\\n•Open data and models. To accelerate research, and to enable community-driven improve-\\nments of large language models, we openly release Falcon-7 /40/180B, and a 600 billion\\ntokens extract of the RefinedWeb dataset: https://huggingface.co/tiiuae/ .\\nTable 1: The Falcon series of models covers a wide range of capabilities and inference require-\\nments, enabled by large-scale web data. Falcon-7B can e fficiently run on consumer hardware\\n(e.g., Apple M2), while Falcon-180B typically requires dedicated inference infrastructure (e.g.,\\n8×A100 80GB ). We report steady zero-shot performance gains across the entire Falcon series.\\nFalcon-7B Falcon-40B Falcon-180B\\nPretraining [tokens] 1,500B 1,000B 3,500B\\nCompute [PF-days] 730 2,800 43,500\\nTraining [A100s] 384 384 4,096\\nAvailability Apache 2.0 Apache 2.0 Responsible use license\\nAgg. performance (Section 6.5) 60.8 67.1 70.3\\nClosest model <GPT-3 Chinchilla PaLM-2 Large\\n2', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 1}), Document(page_content='Contents\\n1 Introduction 2\\n2 State-of-the-art: from language modeling to frontier models 5\\n3 Design philosophy 6\\n4 Experiments and motivations for data, architecture, and hyperparameters 7\\n4.1 Setup for small-scale experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n4.2 Data: web vs curated, code and multilinguality impact on English performance . . 8\\n4.2.1 Web data alone can outperform curated corpora . . . . . . . . . . . . . . . 8\\n4.2.2 Against a strong web baseline, curated data can even be detrimental . . . . 10\\n4.2.3 Limited code and multilingual data do not strongly degrade English performance 11\\n4.3 Architecture and pretraining: validating popular recipes, and inference optimizations 12\\n4.3.1 Extending multiquery into multigroup for tensor parallel training and inference 12\\n4.3.2 Rotary positionnal embeddings may only o ffer a limited edge over ALiBi . 14\\n4.3.3 The extra memory cost of GLU may not be worth it for cost-e fficient training 14\\n4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers 15\\n4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search 15\\n4.4 Further experimentation required: ideas that did not make the cut . . . . . . . . . . 17\\n4.5 Wrapping-it up: validating overall dataset and architecture recipes . . . . . . . . . 18\\n5 Implementation 19\\n5.1 The Falcon dataset: predominantly web, with added curated and conversational data 19\\n5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset . . . . . . 20\\n5.1.2 The Microdata curated corpora and conversational masking . . . . . . . . . 21\\n5.2 The Falcon architecture and recipe for e fficient inference and (stable) training . . . 22\\n5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up 22\\n5.2.2 Large language model alchemy: hyperparameters for pretraining . . . . . . 24\\n5.3 Large-scale distributed training on cloud infrastructure with Gigatron . . . . . . . 24\\n5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability 25\\n5.3.2 State-of-the-art throughput with dedicated Triton kernels . . . . . . . . . . 28\\n5.3.3 Efficient memory use via selective recomputation implemented as a monolayer 28\\n5.3.4 Numerical precision: all you need is bfloat16 ? . . . . . . . . . . . . . . 29\\n5.3.5 Quality-of-life features for improved flexibility and reliability . . . . . . . 29\\n5.4 Run management: keeping large-scale infrastructure running . . . . . . . . . . . . 29\\n6 Results 30\\n6.1 To prompt or not to prompt: comparing evaluations across codebases . . . . . . . . 31\\n6.2 Comparisons with PaLM on a natural language tasks aggregate . . . . . . . . . . . 33\\n6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks . . . . . . . . . . . 34\\n3', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 2}), Document(page_content='6.4 State-of-the-art comparisons on common sense, question answering, and code tasks 34\\n6.5 Comparison with other models using the EleutherAI Evaluation Harness . . . . . . 36\\n7 Limitations 37\\n7.1 Limitations of our findings and ablations . . . . . . . . . . . . . . . . . . . . . . . 37\\n7.2 Limitations of the Falcon models . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\\n8 Conclusion 39\\nA Contributions 51\\nB Acknowledgements 51\\nC Model card 52\\nD Datasheet 53\\nE Comparisons with undocumented models 53\\nF Pseudocode samples 53\\nF.1 Measurement plan to measure all to all bandwidths /latencies e fficiently . . . . . . . 53\\nF.2 Converting tree token depth into an attention mask: . . . . . . . . . . . . . . . . . 53\\nF.3Zero-1 pseudo-code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\\nG Prompts 54\\n4', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 3}), Document(page_content='2 State-of-the-art: from language modeling to frontier models\\nWe provide in this section an overview of general trends and works adjacent to the Falcon series. For\\nan in-depth literature review of individual technical components, see the relevant sections.\\nLanguage modeling. Beyond corpus /task-specific approaches, the first large-scale vector-based word\\nembeddings methods (Mikolov et al., 2013; Pennington et al., 2014) pioneered unsupervised learning\\nfrom massive unstructured text corpora. The integration of deep recurrent neural architectures enabled\\nmodels to deal with polysemy and to integrate contextual information (Peters et al., 2018); up to the\\nemergence of the transfer learning paradigm, leveraging universal models specialized to downstream\\ntasks through finetuning (Howard and Ruder, 2018). Despite the existence of many of the first\\nprinciples currently used, early scaling attempts (Jozefowicz et al., 2016) only had mixed success,\\npartly due to the fastidiousness and poor scalability on common hardware of recurrent approaches.\\nTransfomer models. The introduction of the attention-based Transformer architecture Vaswani\\net al. (2017) sparked an explosion in the number of recipes to produce e fficient, generalist models:\\nfrom embedding and classification focused encoder-only BERTs (Kenton and Toutanova, 2019), to\\ncausal decoder-only GPTs (Radford et al., 2018). Specifically, GPT-2 (Radford et al., 2019) was the\\nfirst series of models to popularize emergent few-shot generalization abilities, allowing a model to\\nunderstand and perform arbitrary tasks simply from in-context instructions and demonstrations.\\nLarge language models. The aforementioned works laid out the key components to current models;\\nthe last ingredient, scaling, was demonstrated by GPT-3 (Brown et al., 2020), and consecrated by\\nthe outlining of scaling laws (Kaplan et al., 2020). As increasingly large amounts of compute are\\nspent to pretrain models, commensurate gains are made in language modeling performance. The\\ntantalizing prospect of a systematic and direct path to more capable language models lead to a \"scaling\\nfrenzy\": first with reproductions of GPT-3 with Jurassic-1 (Lieber et al., 2021) or PanGu-Alpha\\n(Zeng et al., 2021), and with open e fforts such as GPT-J (Wang and Komatsuzaki, 2021), OPT (Zhang\\net al., 2022), or BLOOM (Scao et al., 2022a); second, with works pushing further the limits of\\nscaling with Gopher (Rae et al., 2021), MT-NLG (Smith et al., 2022), or PaLM (Chowdhery et al.,\\n2022). Increased development and adoption of large language models also lead to improvements\\nof pretraining methods. Notably, Ho ffmann et al. (2022) demonstrated with Chinchilla that optimal\\nscaling should actually jointly increase model size and pretraining dataset. For deployment in the\\nreal-world, it may even be desirable to train far past so-called optimality, to decouple training and\\ninference compute and reduce serving costs. This is illustrated with the LLaMA models (Touvron\\net al., 2023a,b), with 7B /13B/30B/70B parameters models trained on up to 2 trillion tokens.\\nFrontier models. Concurrently to this work, so-called frontier models have emerged, under the\\nshifting definition of \"large-scale machine-learning models that exceed the capabilities currently\\npresent in the most advanced existing models, and can perform a wide variety of tasks\". Although a\\nmoving goal, we attribute recent works on GPT-4 (OpenAI, 2023a) and PaLM-2 (Anil et al., 2023) as\\nearly contributions to this category. These stand out by their significantly increased compute budget,\\nand improved capabilities. See Appendix E for details on our approach to undocumented models.\\n2021□01□01 2022□01□01 2023□01□01\\nPublished date [arXiv]102103104105Compute [PF-days]GPT-3\\nGPT-Neo-1.3BGPT-JGPT-NeoX-20BOPTBLOOM\\nLLaMALLaMA 2Falcon-180B\\nFalcon-40BPaLM-2GPT-4\\nPaLM\\nChinchillaGopher\\nLaMDAMT-NLG\\nJurassic-1\\nPanGu-AlphaGLM\\nMPTCommercial/private\\nOpen\\nOpen-source\\nOpen-access\\nCommercial\\nPrivate\\nFigure 2: Although open models lag behind closed models in pretraining compute (by ∼18\\nmonths), the gap is not widening. After a 1 year lag, GPT-3 sparked a \"scaling frenzy\", with the\\nemergence of numerous LLMs. Models in the >10,000 PF-days range remain rare for open-source.\\n5', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 4}), Document(page_content='3 Design philosophy\\nInspired by the bitter lesson (Sutton, 2019), we believe that scalable methods that best leverage\\ncompute are ultimately the most e ffective. Accordingly, in designing the Falcon series of models, we\\nfocused primarily on scalability , across three axes: performance, data, and hardware.\\nPerformance scalability. The sustained increase in the scale of large language models (Fig. 2)\\nhas been primarily motivated by so-called scaling laws: with increased pretraining compute comes\\ncommensurate improvements in language modeling capabilities (Hestness et al., 2017; Kaplan et al.,\\n2020). This systematic path to model improvement has proved far more reliable than waiting for the\\noccasional research epiphany to manifest a paradigm-shifting method. But upstream and downstream\\nperformance are not simply motivators of scaling, they are also a powerful driver. Indeed, quantifying\\nthe impact of modeling interventions (e.g., architecture tweaks, data sourcing, hyperparameters\\nselection) is critical to sustaining the feedback loop provided by small-scale ablations. However,\\nthe question of what to quantify is not trivial: upstream performance alone can be at odds with\\ndownstream tasks (Tay et al., 2021), and even downstream metrics may not be aligned with human\\npreferences (Stiennon et al., 2020). This is made even more challenging by the fundamental contrast\\nbetween pretraining objective (i.e., predict the next word on a predominantly web-based corpora) and\\ncommon downstream use (i.e., follow users’ instructions in a helpful, harmless, and honest way) (Bai\\net al., 2022a). As we focus on the pretraining stage of the Falcon models, we choose to center our\\nevaluations on measuring zero/few-shot generalization on large aggregates of natural language\\ntasks with the EleutherAI Harness (Gao et al., 2021)–similar to the setup of Le Scao et al. (2022);\\nWang et al. (2022a). We build aggregates enabling comparisons with other state-of-the-art models,\\nbut note the di fficulty in executing principled comparisons: practices diverge widely across papers in\\nterms of task selection, prompting, and even mode of evaluation–standardized benchmarks remain\\nonly scarcely adopted. We discuss in Section 7 limitations of our evaluation setup.\\nData scalability. An increase in pretraining compute budget can be either spent towards a larger\\nmodel, or towards longer training. While Kaplan et al. (2020) had first found optimal scaling to be\\nmostly model size driven, Ho ffmann et al. (2022) revised this finding and found that joint scaling is\\npreferable–see Fig. 3 to see impact. Furthermore, growing model size also increases inference burden;\\nwhere as increasing the pretraining dataset size is decoupled from inference costs. Recent open\\nmodels have been trained on datasets of up to two trillions tokens (Touvron et al., 2023b); because\\nnaive repetition risks degrading the model (Hernandez et al., 2022; Muennigho ffet al., 2023), this has\\nled to concerns about the sustainability of data scaling (Villalobos et al., 2022). These concerns are\\nexacerbated by the widely belief that curated corpora are necessary to build state-of-the-art models,\\nrequiring manual addition of individual sources such as arXiv papers, books, and more (Brown et al.,\\n2020; Gao et al., 2020). For the Falcon series of models, we choose to focus on scaling high-quality\\nweb data through stringent filtering and deduplication , enabling us to collect an English web\\ndataset of 5,000 billion tokens and to not repeat any data during training. We extensively report on\\nthis work in Penedo et al. (2023), but also provide some key elements in this paper.\\nHardware scalability. Large-scale training requires thousands of hardware accelerators to work\\nefficiently in unison; making the best use of these accelerators requires in turn principled distributed\\ntraining methods (Shoeybi et al., 2019). Methods that are able to best run e fficiently and leverage\\nlarge-scale compute are often the ones that gain the most traction in the community (Hooker, 2021),\\nas best evidenced by the Transformer architecture itself (Vaswani et al., 2017). Furthermore, it\\nis difficult to find architectural improvements that significantly improve the task performance of\\nmodels, compared to the impact of data for instance (Le Scao et al., 2022). Accordingly, we\\nfocus architectural decisions not on improving task performance, but on improving hardware\\nscalability and throughput . We are also concerned with inference scalability (Pope et al., 2023),\\nleading us to the adoption of tweaks such as a (revised) multiquery attention scheme (Shazeer, 2019).\\nFinally, beyond scalability, we are also concerced with cost-e fficiency and relying on proven\\napproaches . We reimplement a 3D parallelism strategy (Narayanan et al., 2021b) combined with\\noptimizer sharding (Rajbhandari et al., 2020), enabling us to run on a more cost-e fficient cloud AWS\\ninfrastructure with limited interconnect. We also put a strong focus on memory-saving methods,\\nenabling us to run on cheaper 40GB A100. Reimplementing our data preprocessing and pretraining\\npipelines from scratch allow us to extensively verify them, rather than relying on an unknown external\\ncodebase. We also do not explore radical departures from the classic designs of large language\\nmodels, such as state-space models (Fu et al., 2022), as these have usually not been proven at scale.\\n6', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 5}), Document(page_content='4 Experiments and motivations for data, architecture, and hyperparameters\\nWe first focus on a series of small-scale experiments with models in the 1B-3B parameters range\\nto validate recommended practices, as well as to identify interesting tweaks. We also conducted\\nextensive experiments to validate our web data pipeline, reported in Penedo et al. (2023). With each\\nsubject of interest, we also outline common practices and our motivations for exploring this direction.\\n4.1 Setup for small-scale experiments\\nSmall models. For these ablations, we seek to be able to rapidly iterate with limited compute costs.\\nThis leads us to train 1 /3 billion parameters models, for 30 /60 billion parameters respectively. We\\nchoose these short training lengths to be illustrative of the optimality regime from Ho ffmann et al.\\n(2022) under which our larger models are likely to be trained. We base our reference architecture and\\nhyperparameters on the one described for GPT-3 (Brown et al., 2020), with the caveat of using ALiBi\\npositionnal embeddings as our baseline (Press et al., 2022). With reasonable resources (32-64 A100),\\nthese ablation models can be trained overnight or in a few days, enabling rapid iterations. We note\\nthat a caveat of using smaller models is that they may not be illustrative of some of the behaviours\\nof larger models: for instance, Dettmers et al. (2022) found that outlier features emerge at the 6B\\nscale, impacting quantization; concerns around data duplication and memorization have also shown\\nto disprotionately a ffect larger models (Carlini et al., 2022; Hernandez et al., 2022)\\nDedicated aggregates. Although small models enable rapid iterations, they also have limited zero-\\nshot capabilities; naive bulk evaluation would result in most tasks being close to the random baseline,\\nand in significant noise in the evaluations. Using models trained on subsets of The Pile, we identified\\ntasks that showed both reasonable performance at small-scale, and limited variability across runs.\\nWe independently quantified variance and average performance on over 50 tasks across 5 runs with\\na different seed (we use this σfor architecture experiments) and across 10 runs with di fferent data\\nsubsets and shu ffling (for data experiments). Based on this analysis, we source 11 tasks from the\\nevaluation setup of Brown et al. (2020); Le Scao et al. (2022); Srivastava et al. (2023) for our ablations\\n(zs-main/data/web ). Note that di fferences between these three subsets are mostly due to di ffering\\npractices across time and between teams. zs-comp is a subset of main for comparisons with other\\nmodels, based on commonly reported tasks. We also report perplexities on The Pile (Gao et al., 2020)\\n(ppl-pile ) for architectures (for data experiments, we found perplexities on The Pile to mostly\\nillustrate di fferences in formatting rather than content), and on a restricted subset of 3 NLP tasks with\\nlow variance ( zs-small ). For our small-scale evaluations, we use both the EleutherAI harness (Gao\\net al., 2021) and BigBench (Srivastava et al., 2023); note that our inference and evaluation codebase\\ndiffer significantly between this setup and the final results we report in Section 6, so results are not\\ndirectly comparable. We present an outline of the aggregates in Table 2.\\nTable 2: To evaluate small models used in ablations (1 /3B models trained on 30 /60B tokens), we\\nbuild four aggregates across 11 tasks on which to measure zero-shot performance and perplexity.\\nWe trained 15 reference models on subsets of The Pile and with random seeds to identify tasks with\\nperformance better than random and low variablity at this scale. All evaluations leverage the EAI\\nHarness (Gao et al., 2021) except date which is taken from BigBench (Srivastava et al., 2023). main\\nwas built for architecture and hyperparameters ablations; data for data mixtures experiments; web\\nfor small-scale ablations on web data; and core for its low variance on a reduced number of tasks.\\nThis setup only covers natural language abilities. For main ,data , andweb, differences are mostly\\ndue to individual preferences at the time of the experiments.\\nTasks Type Random main comp data web core pile\\nLAMBADA (Paperno et al., 2016) Reading Comprehension 0.0 ✓ ✓\\nRACE (Lai et al., 2017) Reading Comprehension 25.0 ✓\\nHellaSwag (Zellers et al., 2019) Common Sense 25.0 ✓ ✓ ✓ ✓ ✓\\nWinogrande (Sakaguchi et al., 2019) Common Sense 50.0 ✓ ✓\\nPIQA (Bisk et al., 2020) Common Sense 50.0 ✓ ✓ ✓ ✓ ✓\\nBoolQ (Clark et al., 2019) Common Sense 50.0 ✓ ✓\\nCOPA (Gordon et al., 2012) Common Sense 50.0 ✓ ✓\\nDate (Srivastava et al., 2023) Common Sense 25.0 ✓\\nARC (Clark et al., 2018) Question Answering 25.0 ✓ ✓ ✓ ✓\\nOpenBookQA (Mihaylov et al., 2018) Question Answering 25.0 ✓\\nSciQ (Johannes Welbl, 2017) Question Answering 25.0 ✓ ✓ ✓\\nThe Pile (Gao et al., 2020) Language Modeling ✓\\n7', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 6}), Document(page_content='4.2 Data: web vs curated, code and multilinguality impact on English performance\\n4.2.1 Web data alone can outperform curated corpora\\nOur motivations for predominantly training on web data, details of the processing pipeline, and\\nextensive evaluations are detailed in our dedicated Refined Webpaper (Penedo et al., 2023). In\\nthis section, we only highlight key ablations guiding our decision to focus our e fforts on web data.\\nBackground. Since its origins with simpler and shallower statistical language models (Shannon,\\n1951; Mikolov et al., 2013), natural language processing has long leveraged unstructured massive text\\ncorpora. If these corpora were at first built \"sentence-wise\" (Chelba et al., 2013), the emergence of\\nmore advanced architectures enabled models to best use long context information present in unified\\ndocuments (Devlin et al., 2018; Radford et al., 2018). Starting with single-domain sources, such\\nas Wikipedia or BookCorpus (Zhu et al., 2015), datasets scaled along with models, and massive\\nweb-scrapes gained prevalence (Ortiz Suárez et al., 2019; Ra ffel et al., 2019). However, it is widely\\nbelieved that web data alone is insu fficient to build performant models (Brown et al., 2020; Gao et al.,\\n2020). Accordingly, large language models train on mixed corpora, combining both large-scale web\\ndata, and curated so-called \"high-quality\" individual sources (e.g., books, technical papers, social\\nmedia conversations)– see Table 3 for an overview of common pretraining mixes.\\nHowever, as we outlined in the data scalability discussion in Section 3, sourcing the trillions of\\ntokens required for pretraining a modern language model may be challenging. This leads us to\\nchallenge the idea that curated data is fundamentally better than web data. Notably, building upon\\nthe work of Lee et al. (2022), we study how stringent deduplication and extensive filtering inspired\\nby Rae et al. (2021) may enable web data alone to train performant models.\\nQuestion. Can web data alone (with filtering and deduplication) be used to train models outper-\\nforming models trained on curated data, as measured by natural language zero-shot performance?\\nMethods. We train 1 /3B parameters models on 27 /60B tokens, on a number of datasets of interest\\nand on intermediary artefacts of our data pipeline. For state-of-the-art web datasets, we consider\\ntwo versions of OSCAR (Ortiz Suárez et al., 2019; Abadji et al., 2022) and C4 (Ra ffel et al., 2019).\\nFor curated datasets, we consider The Pile (Gao et al., 2020), the most popular pre-aggregated\\ndataset–many models have also elected to base their pretraining data on specific components of The\\nPile (Smith et al., 2022; Zhang et al., 2022). RW-Raw corresponds to the output of our pipeline\\nwith the least amount of filtering, immediately after text extraction–but still with English language\\nidentification applied as well as URL blocklist for known adult content; RW-Filtered applies a first\\nround of heuristics, similar to the ones used by Rae et al. (2021); finally, Refined Webcorresponds\\nto our final web dataset, with deduplication applied in two stages. We evaluate all models on the\\nzs-web aggregate–see Table 2 for details of its composition.\\nTable 3: Following the recommendations of Ho ffmann et al. (2022), pretraining datasets have\\nincreased in size, causing an increase in the prevalence of web data. Web data sources are\\nmassive web scrapes such as C4 (Ra ffel et al., 2019), sourced from CommonCrawl. Curated web data\\nundergoes a targeted domain filtering: this includes CC-News (Hamborg et al., 2017) for instance.\\nWe consider sources such as arXiv, Wikipedia, or PubMed as technical curated data, and sources such\\nas Reddit, HackerNews, or StackOverflow as conversational. We report not the size of the overall\\ndataset, but the amount of tokens used for pretraining. For LaMDA (Thoppilan et al., 2022), numbers\\nare roughly estimated as only rough counts are provided in the paper.\\nWeb Curated web Curated Total\\nBooks Tech. Code Conv. Epochs\\nGPT-3 60 % 22 % 18 % 16 % 2 % 0 % 0 % 2.4 300B\\nThe Pile 18 % 10 % 72 % 15 % 40 % 7 % 10 % 1.8 340B\\nMT-NLG 38 % 29 % 33 % 16 % 9 % 2 % 6 % 2 270B\\nGopher 58 % 10 % 32 % 27 % 2 % 3 % 0 % ∼1 300B\\nLaMDA 25 % 0 % 75 % 0 % 25 % 0 % 50 % 340B\\nPaLM 27 % 1 % 72 % 13 % 4 % 5 % 50 % 780B\\nChinchilla 55 % 10 % 35 % 30 % 1 % 4 % 0 % 1.1 1400B\\nLLaMA 82 % 0 % 18 % 5 % 7 % 4 % 2 % 1.7 1400B\\nFalcon 84 % 0 % 16 % 6 % 2 % 3 % 5 % 1 3500B\\n8', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 7}), Document(page_content='102103104105106\\nCompute [PF-days]103104Pretraining data [tokens]Clean data available in CommonCrawl\\nPre-Chinchilla trendPost-Chinchilla trendGPT-4\\nFalcon-180B\\nFalcon-40BLLaMA 2\\nChinchilla\\nGopherPaLM-2\\nPaLM\\nGPT-3LaMDA\\nMT-NLGMPT\\nGPT-NeoX-20BPre-Chinchilla\\nPost-Chinchilla\\n7B params.\\n40B params.\\n180B params.Figure 3: Two eras of pretraining practices, from predominant model scaling ■to joint model\\nand data scaling _. Before Ho ffmann et al. (2022), models ( ■) predominantly scaled their parameter\\ncount at a fixed dataset size (around 300 billion tokens), in line with the recommendations of Kaplan\\net al. (2020). Afterward ( _), models started scaling both model size and dataset size jointly, sharply\\nincreasing the need for scalable data pipelines. Estimate of clean data available in CommonCrawl from\\nour work in Penedo et al. (2023), considering English only–would double if allowing multilinguality.\\nResults. Results for this round of experiments are presented in Table 4. In line with expectations,\\nwe find that raw web data (RW-Raw) performs poorly; similarly OSCAR-22.01 o ffers the worst\\nperformance of all datasets. This is likely because its creators have opted to distribute it by default\\nwithout any deduplication applied. Conversely, OSCAR-21.09 and C4 are both strong baselines. We\\nnotably find that The Pile very likely does not deliver better performance than web data.\\nSubsequent stages in our pipeline significantly uplift dataset quality and the performance of models\\ntrained on it. Filtering alone enables us to close the gap with The Pile, while the addition of stringent\\ndeduplication allows for R efined Webto be the best dataset among the ones we benchmarked.\\nWe note two limitations of these experiments. First, the models are small, and trained on a limited\\namount of data. However, it is likely that gains from deduplication actually increase with model\\nscale, as larger models are more sensitive to duplicates (Hernandez et al., 2022) and more likely\\nto memorize individual samples (Carlini et al., 2022). Second, our evaluation focuses on natural\\nlanguage tasks. It is unlikely that models trained on web data alone would compare favorably to\\nmodels trained on The Pile on code tasks for instance, as The Pile explicitely include code, while\\nmassive web scrapes are likely to be mostly devoid of it except for some incidental occurrences.\\nFinding. Challenging beliefs on data quality, filtered and deduplicated web data alone allows\\nmodels to match the natural language tasks performance of models trained on curated data.\\nTable 4: Curation is not a silver bullet for zero-shot generalization: small-scale models trained\\nonRefined Weboutperform models trained on web data (C4, OSCAR), and on curated corpora\\n(The Pile). Zero-shot accuracy on zs-web aggregate (σ=0.69,likely±0.69%,very likely±1.38%\\ndifferences in scores). All models trained with identical architectures and hyperparameters, for\\nthe same amount of tokens. We find that OSCAR-22.01 underperforms other datasets signficantly,\\nperhaps because deduplication is only optional. C4 is a strong baseline, with OSCAR-21.09 lagging\\nslightly behind, but we find that RefinedWeb outperforms both web datasets and the curated dataset,\\nThe Pile–performance gap with C4 is insu fficient to be conclusive, but C4 would be too small for our\\nmodels. Both filtering and deduplication contribute significantly to improving zero-shot performance.\\nMassive web datasets Curated Ours\\nOSCAR-21.09 OSCAR-22.01 C4 The Pile RW-Raw RW-Filtered Refined Web\\n1B@27GT 55.0% 52.7% 55.7% 53.4% 52.7% 54.3% 56.2%\\n3B@60GT 59.1% 55.9% 59.6% 57.9% 57.4% 58.2% 59.8%\\n9', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 8}), Document(page_content='Table 5: We split curated data in three broad categories: conversations, books, and technical.\\nIndividual components inspired by Gao et al. (2020), but and processed through our own data pipeline.\\nConversations Reddit (Baumgartner et al., 2020), HackerNews, OpenSubtitles (Tiede-\\nmann, 2016), Ubuntu IRC, Youtube Subtitles, StackOverflow\\nBooks Project Gutenberg Rae et al. (2019)\\nTechnical ArXiv, PubMed Central, PubMed Abstracts, USPTO Patents\\n4.2.2 Against a strong web baseline, curated data can even be detrimental\\nBackground. In Section 4.2.1 and Table 3, we have noted that large language models use pretraining\\ndatasets combining both massive web crawl data and individual curated sources. Such sources were\\nfirst employed to build domain-specific models (Beltagy et al., 2019); they have also been proposed to\\nbroaden the expressiveness of models, for instance for conversational modalities (Adiwardana et al.,\\n2020; Thoppilan et al., 2022). Some of these sources can also exist at the intersection of strongly\\ncurated data and crawls: Laurençon et al. (2022) has for instance proposed to seed the first links in a\\ncrawl using human-selected URLs. However, these tailored sources raise challenges for practioners.\\nFirst, individual corpora are much less scalable than massive web crawls, as they require scattered\\nwork instead of a centralized pipeline. Second, the providers of some of these sources have begun\\ntaking steps to forbid LLMs from being trained on their data Paresh (2023), and adequate licensing\\nmay be costly to obtain. Based on our findings from the previous section, we may wonder what\\nhappens when we combine curated data with a strong web baseline like RefinedWeb.\\nQuestion. When added in substitution of a strong web baseline, is curated data from individual\\ncorpora still beneficial to the natural language zero-shot performance of a model?\\nMethods. We train small 1B models on 30B tokens, with the pretraining data split between web data\\nand a specific curated category. We sample training on 1, 10, 25, 50, 75, and 100% of the targeted\\ncategory. We only consider a one-dimensional approach, and mix web data with a single category of\\ncurated data. We split our categories in books, conversations, and technical data as outlined in Table 5.\\nFor the individual corpora making these categories, we draw inspiration from The Pile (Gao et al.,\\n2020) which we enhance with data from Reddit (Baumgartner et al., 2020) for the conversational\\ncategory. Our web data is taken from RefinedWeb (Penedo et al., 2023) and we process curated\\nsources through a similar pipeline, applying filtering and deduplication to make for a fair comparison.\\nWe evaluate performance on the zs-data aggregate–see Table 2 for detailed make-up.\\nResults. We plot the zero-shot accuracy as we increase the fraction of curated data in Fig. 4. When\\ncombined with a strong web baseline, we find that the addition of curated data never significantly\\nuplifts performance. In fact, excess of curated data even worsens performance: for books and\\ntechnical, past 50%, we start observing meaningful degradation of accuracy. We believe this is likely\\ncaused by \"mode collapse\" compared to the high diversity of web data.\\nInterestingly, conversations perform decently throughout, with the smallest degradation at 100%.\\nWe posit this could either be due to our conversation category being the most diverse of the three,\\nor to conversations being closer to the distribution of tasks. Indeed, this category likely includes\\npeople interacting, answering questions, and giving instructions to one another–a style which is less\\nprevalent in books or technical-driven content like papers and patents.\\nOnce again, this ablation is limited by the scope of the tasks considered. It is likely that highly\\ntechnical tasks may benefit from domain specific data, and that web data would be a rather poor\\nbaseline for code tasks. Furthermore, our zero-shot evaluations are all done under a short context\\nlength: books could for instance be beneficial in helping the model learn long-range correlations.\\nFinding. When added in substitution of a strong web baseline, curated categories of data do not\\nsystematically result in an improvement in natural language zero-shot performance.\\n10', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 9}), Document(page_content='1 10 25 50 75 100\\nFraction of {conversations, books, technical} in pretraining [%]586062646668Aggregated zero-shot accuracy [%]\\n-3\\n+3\\nweb-data baseline\\nconversations\\nbooks\\ntechnicalFigure 4: When compared to a strong web data baseline, high-quality curated data does not\\nimprove zero-shot performance. Over-reliance on a single type of curated data is even detrimental\\nto performance. The make-up of the conversations, books, and tech data corpora is outlined in Table 5.\\nWe report the zero-shot performance using di fferent data-mixtures and compare with a baseline model\\ntrained on RefinedWeb only, our deduplicated and filtered web-only dataset Penedo et al. (2023). The\\nshaded green-area indicates the ±3σconfidence interval based on 10 experiments across data splits.\\n4.2.3 Limited code and multilingual data do not strongly degrade English performance\\nQuestion. Can limited amounts (5-10%) of code and multilingual data be substituted in pretraining\\ndata added without compromising the English performance of the model?\\nMultilinguality. Some of the first large-scale deployment of RNNs and Transformers were for\\nmachine translation (Wu et al., 2016); with the attention mechanism originally introduced for such\\nuse cases (Bahdanau et al., 2014). Accordingly, it’s no surprise that multilingual language models\\nrapidly flourished along their monolingual counterparts (Xue et al., 2021). However, multilingual\\ngenerative large language models have remained more elusive. Indeed, both Lin et al. (2021) and Scao\\net al. (2022b) have reported that massive multilinguality comes at the expense of English performance,\\nresulting in multilingual models that underperform their monolingual counterparts (Scao et al., 2022a).\\nThis so-called curse of multilinguality (Conneau et al., 2020) has lead practitioners to be weary of\\nnaively adding other languages in bulk – even PaLM (Chowdhery et al., 2022), which explicitly\\ntargets multilingual capabilities, only trains on 20% non-English data. Furthermore, multilingual data\\nis scarcely available (Costa-jussà et al., 2022): nearly 60% of the documents in CommonCrawl are\\nEnglish, and the distribution of top languages is skewed towards European ones. Notably, Mandarin\\nChinese is only the 6th top language in CommonCrawl, despite being 2nd worldwide, and Hindi does\\nnot show-up even in the top-20 despite being 3rd worldwide (Eberhard et al., 2023).\\nWe choose to experiment with a restricted multilingual setup: we consider languages with a Latin\\nalphabet, and focus on those for which we can collect a non-trivial amount out of CommonCrawl (over\\n10 billion tokens) using our data pipeline. Our splits for experiments are in Table 6. We train models\\nwith a fixed 10% multilingual data (weighing individual languages according to their prevalence in\\nCommonCrawl), and evaluate on English tasks. Each data split uses a dedicated tokenizer.\\nTable 6: We split languages in three categories: English-only, Restricted (adding the 3 most\\nspoken languages in Europe), and European. We only consider languages with a Latin alphabet,\\nand su fficient presence in CommonCrawl to collect at least 10 billion tokens.\\nSet Languages\\nEnglish English\\nRestricted English, German, Spanish, French\\nEuropean English, German, Spanish, French, Italian, Dutch, Polish, Portuguese, Czech,\\nSwedish, Romanian, Danish, Norwegian, Catalan, Slovene, Bulgarian\\n11', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 10}), Document(page_content='Table 7: Including in the pretraining data a small fraction of code or multilingual data broadly\\ndoes not degrade performance significantly, except on specific tasks. Overall, the degradation\\nobserved is barely measurable on our larger aggregate, and mostly driven by HellaSwag on the\\nzs-small one. Underlined values have crossed the likely 1-σdegradation threshold.\\nPretraining data Zero-shot accuracy\\nzs-main↑zs-small↑\\nLikely threshold (1- σ)±1.0±0.5\\nEnglish-only 53.7 49.2\\n10% Restricted 53.4 48.3\\n10% European 53.6 48.2\\n5% Code 53.6 48.5\\nWe present results in Table 7. We find that the performance degradation from 10% multilinguality is\\nvery limited, and that the addition of other European languages over German, Spanish, and French do\\nnot drive additional degradation. We saw most of the reduction in performance on HellaSwag, while\\nother tasks are not strongly impacted. We note that these experiments apply in a very restricted setting,\\nand may not be illustrative of other multilingual setups (e.g., languages without a latin alphabet, using\\nother languages to compensate for running out of English tokens, etc.)\\nCode. Large language models have demonstrated strong coding abilities, either through finetuning\\nafter general pretraining (Chen et al., 2021; Chowdhery et al., 2022; Rozière et al., 2023), or through\\ndedicated pretraining recipes (Li et al., 2023a). Furthermore, at variance with multilingual data,\\ncode data is plentiful (Kocetkov et al., 2022), with trillions of tokens available from crawling public\\nrepositories. Code tasks are a predominant application of LLM-powered assistants (Luo et al., 2023),\\nand so allowing for such use cases with Falcon is important. However, we do not want to risk\\ncompromising language capabilities: we thus validate here the common practice of adding limited\\ncode data to pretraining, in line with other models (see Table 3 for common fractions in pretraining).\\nWe select the top-30 programming languages from GitHub, and substitute 5% of our pretraining data\\nfor code. Note that we apply deduplication to the code dataset, but adjust our heuristics to avoid\\nremoving too much of the data. We consider only performance on English tasks. Results are outlined\\nin Table 7. We find the addition of code data to have a similar e ffect to multilinguality (if only weaker,\\nperhaps because of the smaller proportion), with only little to no task degradation.\\nFinding. Small fractions of code and multilingual data (5-10%), in line with common recipes\\nfor large language models, do not broadly impact zero-shot performance on English tasks.\\nWe note that the small scale of our ablations is here a stronger limitation, likely leading us to a more\\nconservative choice on multilinguality and code. It has been argued that larger models, thanks to their\\nincreased capacity, can better deal with multilinguality (Shaham et al., 2022). Code data has also\\nbeen shown for larger models to boost commonsense abilities (Madaan et al., 2022). More broadly, a\\nsimilar e ffect has been observed for multilingual models (Aghajanyan et al., 2023).\\n4.3 Architecture and pretraining: validating popular recipes, and inference optimizations\\n4.3.1 Extending multiquery into multigroup for tensor parallel training and inference\\nBackground. Unanimously, large language models first adopted the multihead attention scheme\\ndescribed in Vaswani et al. (2017). Each token produces nheadtriplets of (query, keys, and values),\\nand the result of each head is then summed to produce the final output of the attention module.\\nHowever, this scheme can be altered. Shazeer (2019) found that one can share the same keys and\\nvalues between all attention heads with only a small degradation in performance. In this so-called\\nmultiquery attention, the number of heads for the queries remains nq=nheadbut there is only one head\\nfor the keys and values, nkv=1. This significantly reduces inference memory consumption: during\\nautoregressive generation, the keys and values are cached to accelerate generation–with multiquery,\\nthe K,V-cache size is divided by nheadcompared to vanilla attention, resulting in a 10-100x reduction\\nin memory consumption for common models–see Table 8. Multiquery improves the scalability of\\ninference for large models (Pope et al., 2023). Chowdhery et al. (2022) has recently popularized this\\narchitectural modification, which has notably been adopted by LLaMA-2 (Touvron et al., 2023b).\\n12', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 11}), Document(page_content='Table 8: Multiquery /group schemes significantly reduces the size of the K,V-cache for inference.\\nAssuming TP =1 for Falcon-7B and TP =8 for Falcon-40 /180B, sequence length 2,048.\\nAttention scheme nq nkv K,V-cache for a 2,048 sequence\\n7B 40B 180B\\nVanilla nhead nheadO(√\\nNlog(n)) 1GB 4GB 10GB\\nMultiquery (Shazeer, 2019) nhead 1O(log( N)) 20MB 30MB 40MB\\nMultigroup (Ainslie et al., 2023) nheadTPO(log( N)) N /A 250MB 335MB\\nScaling. Interestingly, multiquery is disproportionately e ffective for larger models. With Nthe\\ntotal number of parameters, dmodel the model size, nlayerthe number of layers, and assuming a fixed\\ndhead=dmodel/nhead–which is typically the case when using FlashAttention (Dao et al., 2022). For\\nefficient scaling, it is recommended that nlayer∼O(log(N))(Levine et al., 2020); since we can\\napproximate N≃nlayer(dmodel)2(Kaplan et al., 2020), it follows that the size of the K,V-cache with\\nmultihead attention scales in O(√\\nNlog(n)). Conversely, for multiquery the K,V-cache only stores\\na fixed 2dheadper layer, which does not increase with width; this results in overall more e fficient\\nscaling, inO(log( N)) instead.\\nMultigroup. One caveat of multiquery attention is that it is di fficult to e fficiently parallelize\\nwhen relying on tensor parallelism, as is common with GPU-based infrastructure (Shoeybi et al.,\\n2019). Either each GPU keeps a copy of the shared key /value, recomputing them individually\\nand then sharing gradients to keep them in sync, or they are computed on a single GPU and then\\ncommunicated as necessary. We propose to introduce separate key /value pairs for each tensor parallel\\nrank, simplifying the required communications. As in Shazeer (2019), we keep nq=nhead, but now\\nhave nkv=TP. This scheme doesn’t change the scaling of the K,V-cache, as it only applies a fixed TP\\nfactor. Concurrently to the development of the Falcon series, Ainslie et al. (2023) also proposed this\\nmodification; we refer to this variant of attention as grouped query attention or multigroup. We note\\nthat the communication reduction applies not just during inference, but also during training.\\nResults. We train 1B /3B models on 30 /60B tokens from The Pile (Gao et al., 2020), with multiquery\\nand varying degrees of multigroup attention. Importantly, we do not control for the reduction in\\nparameters caused by the loss of additional keys and values–some degradation is thus expected.\\nResults are presented in Table 9. Both multiquery and multigroup do not result in any large reduction\\nin zero-shot performance, even without compensating for the reduction in trainable parameters.\\nRecipe decision. To improve performance scalability for the largest models, the Falcon series\\nimplement multigroup with KV =TP for all models (respectively 1 /8/8 for Falcon-7 /40/180B).\\nTable 9: Even without controlling for the reduction in parameters, multiquery only comes at a\\nlimited zero-shot performance cost. Impact on perplexity is more directly measurable, whereas\\nimpact on zero-shot performance is less consistent. Multigroup with KV =8 consistently performs\\nclose to the vanilla baseline. Underlined values have crossed the very likely 2-σdegradation threshold.\\nModel size KV Performance\\nzs-main↑zs-small↑ppl-pile↓\\nVery likely threshold (2- σ)±2.2±0.8±0.005\\n1B 1 48.5 42.6 0.908\\n2 48.2 42.1 0.899\\n4 48.9 42.4 0.908\\n8 48.6 42.8 0.903\\nVanilla 49.2 43.1 0.895\\n3B 1 52.7 48.6 0.825\\n8 54.6 50.1 0.819\\nVanilla 54.4 49.8 0.807\\n13', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 12}), Document(page_content='Table 10: Although at small-scale URPE and RoPE may likely be better than ALiBi, that\\nadvantage isn’t as clear at increased size. We find ALiBi to be likely worst than rotary on two of\\nour three aggregates for 1B models, but to be closer to the performance of rotary at the 3B scale.\\nUnderlined values have crossed the likely 1-σdegradation threshold over RoPE.\\nModel size Pos. Emb. Performance\\nzs-main↑zs-small↑ppl-pile↓\\nLikely threshold (1- σ)±1.1±0.4±0.002\\n1B ALiBi 49.2 43.1 0.895\\nURPE 49.6 43.1 0.885\\nRoPE 50.0 44.2 0.883\\n3B ALiBi 54.4 49.8 0.807\\nRoPE 54.4 50.5 0.799\\n4.3.2 Rotary positionnal embeddings may only o ffer a limited edge over ALiBi\\nBackground. By default, attention does not provide positional information to the model: it only sees\\nthe sequence as a bag-of-word. Accordingly, the original Transformer architecture adopted absolute\\nsinusoidal embeddings to encode positional information Vaswani et al. (2017). However, absolute\\nembeddings have since declined in popularity, the community shifting to relative embeddings instead.\\nWhile this shift is well motivated empirically (Shaw et al., 2018; Scao et al., 2022b), practionners\\nhave yet to crystallize on a single relative positional embedding: BLOOM and MPT (Scao et al.,\\n2022a; MosaicML, 2023) use ALiBi (Press et al., 2022), while GPT-J, PaLM, and LLaMA (Wang\\nand Komatsuzaki, 2021; Chowdhery et al., 2022; Touvron et al., 2023a,b) use Rotary Positional\\nEmbeddings (RoPE) (Su et al., 2021). RoPE are often cited as delivering better upstream performance\\nin the works above, while ALiBi benefits from built-in extrapolation abilities. Another recently\\nintroduced alternative are Universal Relative Positonal Embeddings, URPE (Luo et al., 2022), which\\naddress shortcomings in the expressivity of typical relative positional embeddings. As a curiosity, we\\nnote that the autoregressive mask of a causal model also provides some positionnal information to the\\nmodel (Scao et al., 2022b; Haviv et al., 2022), enabling training without any positional embeddings\\nto be comparable to absolute sinusoidal ones for zero-shot performance.\\nConcurrently to this work, recipes have emerged to enable zero-shot or finetuned length extrapolation\\nwith RoPE (Chen et al., 2023), briding the gap with ALiBi for extrapolation.\\nResults. We train 1 /3B models on 30 /60B tokens on The Pile (Gao et al., 2020). We report results\\nin Table 10. We find no evidence for URPE outperforming RoPE–since it would require significant\\nmodifications to the fused-attention kernels to deliver acceptable performance, we do not pursue\\nit further. At the 1B scale, we find a likely advantage to using RoPE over ALiBi; however, that\\nadvantage diminishes at the 3B scale, and is insu fficient to conclude clearly. One remaining advantage\\nof ALiBi is its compute overhead: it is significantly cheaper to compute than RoPE; however, with\\ncustom Triton kernels (Section 5.3.2) we are able to mitigate that overhead.\\nRecipe decision. In-line with other popular large-scale models, we adopt rotary positionnal\\nembeddings, and use custom kernels to mitigate the overhead.\\n4.3.3 The extra memory cost of GLU may not be worth it for cost-e fficient training\\nBackground. Activations based on gated linear units (Shazeer, 2020) are widely believed to outper-\\nform traditional activation functions such as GeLU (Scao et al., 2022b). They have seen adoption in\\nmodels such as PaLM and LLaMA (Chowdhery et al., 2022; Touvron et al., 2023a,b).\\nScaling. Scaling-wise, GLU activations have been preferred as well, as they increase the size of\\nthe MLP (doubling its first layer), shifting more compute towards simple matrix multiplications.\\nHowever, this does come at a cost: the memory required to store the intermediary activations in\\nthe MLP is higher. Remember that, typically, the inputs to the activation function are saved for\\nthe backward (as recomputing the function itself is negligible). For gated units, this input is now\\ntwice large. Overall, SwiGLU doubles intermediary activations, and increases by 50% the number of\\nparameters in the MLP: the activation memory per parameter for the MLP is thus increased by 33%.\\n14', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 13}), Document(page_content='Table 11: Small architectural tweaks like GLU, z-loss, and removing biases are unlikely to\\nimprove zero-shot performance. However, in some scenarios, these have been proposed to improve\\nscalability and /or stability, which may warrant their adoption. Underlined values have crossed above\\ntheunlikely 0.4-σimprovement threshold over our baseline.\\nModel size Pos. Emb. Performance\\nzs-main↑zs-small↑ppl-pile↓\\nUnlikely threshold (0.4- σ)±0.4±0.2±0.001\\n1B 49.2 43.1 0.895\\nSwiGLU 49.2 43.1 0.891\\nz-loss 49.0 43.6 0.895\\n3B 54.5 49.8 0.807\\nNo biases 54.4 49.8 0.807\\nResults. In Table 11, training 1B models on 30B tokens of The Pile, we find no clear benefits\\nfrom adopting SwiGLU for zero-shot performance–and the improvement on perplexity is just at the\\nthreshold of being unlikely to characterize an actual improvement due to variance in the evaluation\\nsetup. We note that this could be due to the fact SwiGLU may require dedicated hyperparameters\\ntuning–as for all architecture ablations, we simply adopted the ones of GPT-3 (Brown et al., 2020).\\nFurthermore, we saw little additional throughput gains from SwiGLU (as we already use parallel\\nattention /MLP layers). Since we train the Falcon series on 40GB A100 to optimize costs, we were\\nconcerned early on about memory consumption–accordingly, SwiGLU, with its increased memory\\nintensity, and similar performance to GeLU, would likely be a net negative for us.\\nRecipe decision. Out of concern for the memory footprint of our trainings on A100-40GB, and\\nbecause of no clear uplift in zero-shot, we choose not to adopt SwiGLU.\\n4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers\\nParallel attention and MLP blocks. Wang and Komatsuzaki (2021) first introduced parallel attention\\nand MLP layers while training GPT-J. This augmentation is important to reduce the communication\\ncosts associated with tensor parallelism: this simple modification cuts the number of all_reduce\\nnecessary from two to one per layer. We found no measurable degradation in zero-shot performance or\\nperplexity, in line with Chowdhery et al. (2022), and adopt this practice. See Fig. 5 for an illustration.\\nNo biases. Chowdhery et al. (2022) found that removing the biases in the linear layers and layer\\nnorms improves stability. We validate that removing the biases in the linear layer does not result in\\nworse performance (see Table 11): neither in terms of language modeling loss nor in terms of the final\\nzero-shot performance. Accordingly, we remove biases from the linear layers in the Falcon series.\\nRecipe decision. We adopt parallel attention and MLP, and remove biases from linear layers.\\n4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search\\nZ-loss. First introduced in the mesh-tensorflow codebase2Shazeer et al. (2018), z-loss aims\\nat increasing the stability of training, by encouraging the logits to stay close to zero. It can be\\nimplemented as an auxiliary loss: z_loss =10−4log2(Σiezi), where ziis the output logits of the model.\\nNote that z-loss does not have a significant impact on task performance at small scale (Table 11).\\nRecipe decision. We adopt z-loss, as it is claimed to improve large-scale training stability and\\ndoes not impact zero-shot performance in our ablations.\\n2As a fun exercise for the reader, we encourage trying to dig up that citation, starting from PaLM (Chowdhery\\net al., 2022), which popularized this practice outside of Google by outlining it in its main text.\\n15', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 14}), Document(page_content='AttentionMLP\\n++\\nAttention MLP+\\nFigure 5: Parallelizing the attention and MLP blocks allows us to remove one sync point during\\ntensor parallel training. This was first proposed by Wang and Komatsuzaki (2021) for GPT-J.\\nWeight decay. We attempted to reproduce the weight decay schedule from Chowdhery et al. (2022),\\nbut failed to obtain an improvement–we suspect this is due to di fferences in initialization. We found\\nthat weight decay has a disproportionate e ffect on datasets which have not been deduplicated /are of\\nlower quality (Table 12). We use AdamW for weight decay (Loshchilov and Hutter, 2018).\\nRecipe decision. We use a fixed weight decay of 0.1 with AdamW for all Falcon models.\\nOptimal learning rate. Practices for setting the learning rate of a run di ffers, from naive grid\\nsearch to more principled approaches (Yang et al., 2022; Dinan et al., 2023). We further discuss in\\nSection 4.4 our failures to implement and reproduce some of the later; in this short section, we focus\\non the naive approach and our validation of it. Broadly speaking, setting too high of a learning rate\\nrisks to cause divergence of the run and instabilities during training; too low, on the opposite, will\\nleave some upstream and downstream performance on the table, leading to ine fficient training.\\nWe propose to search through possible learning rate in the following away: (1) we select 4-6 roughly\\nlogarithmically-spaced candidate learning rates, anchoring around the ones used in GPT-3 (Brown\\net al., 2020), and favoring higher learning rates; (2) we run through a long 500 million tokens warmup\\nfor all candidate learning rates; (3) we pick the learning rate which has achieved the lowest loss at\\nthis point, and discard any learning rate which has already caused spikes.\\nWe test this method at small scale, picking 7 learning rates for a 1B model, and then comparing the\\nranking obtained after warmup against the actual final ranking achieved. Results are presented in\\nTable 13. We find that this naive method succeeds at finding the best learning rate from the rankings\\nat the end of warm-up. More broadly, ranks at the end of warm-up and at the end of training are\\nrelatively stable, with only the 2nd and 3rd best switching places.\\nRecipe decision. From candidates LRs, we pick the one with the lowest loss after warm-up.\\nTable 12: Weight decay is likely to improve performance, especially so for datasets such as The\\nPile, which may not have been adequately deduplicated. Surprisingly, the e ffect of weight decay\\nis disproportionately strong depending on the underlying dataset.\\nDataset Weight decay Performance\\nzs-main↑zs-small↑ppl-pile↓\\nLikely threshold (1- σ)±1.1±0.4±0.002\\nRefinedWeb 0. 52.1 47.9 1.07\\n1. 52.0 48.4 1.06\\nThe Pile 0. 50.3 43.7 0.877\\n1. 51.7 45.0 0.868\\n16', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 15}), Document(page_content='Table 13: Loss rankings at the end of learning rate warm-up broadly reflects rankings at the\\nend of training, enabling us to search for optimal learning rates e fficiently This simple heuristic\\nis easy to use, and consume only a fraction of resources for a larger run.\\nLearning rate End LR warm-up [0.5GT] End of run [27GT] Run stability\\nFactor LR Improv. ↓ Rank Improv. ↓ Rank\\nx1 2×10−4±0.0% (4) ±0.0% (4) No spikes\\nx2 4×10−4−2.0% (2)−1.7% (3) No spikes\\nx5 1×10−3−2.6% (1) −2.5% (1) No spikes\\nx10 2×10−3−1.4% (3) −1.9% (2) One small spike\\nx20 4×10−3+1.3% (5) +1.9% (5) Multiple spikes\\nx50 1×10−2+7.6% (6) +6.8% (6) Multiple large spikes\\nx100 2×10−2Diverging after 0.2GT\\n4.4 Further experimentation required: ideas that did not make the cut\\nIn this section, we briefly mention some practices and ideas we experimented with, but for which we\\nwere unable to reproduce results or obtain a meaningful outcome. We note that this is not be viewed\\nas an indictment of these practices: many have been adopted in popular models successfully.\\nAlternative training objectives. The largest of language models have been typically trained with a\\ncausal decoder-only architecture and objective (Brown et al., 2020; Rae et al., 2021; Chowdhery et al.,\\n2022). Wang et al. (2022a) found that such models exhibit better zero-shot abilities than masked\\nencoder-decoders such as T5 (Ra ffel et al., 2019); however, they also found that after multitask\\nfinetuning (Sanh et al., 2021), masked encoder-decoder performed better, highlighting that di fferent\\nregimes and use cases may favor di fferent architectures and objective. Throughout, a non-causal\\ndecoder-only (so-called prefix language model) performed competitively as a close second. With\\nUL2, Tay et al. (2022a) found that these paradigms could be unified, by training on a mixture of\\nobjectives instead. We experimented with UL2, but were unable to obtain an uplift in zero-shot\\nperformance, even after adapting tasks to the various paradigms when relevant. Due to time and\\nressource constraints, we did not end-up pushing our experiments further, as Tay et al. (2022b)\\nshowed that a posteriori adaptation to the UL2 objective was not only possible but e fficient.\\nFor code models, so-called fill-in-the-middle (FIM) training (Bavarian et al., 2022) has been popular,\\nas it addresses a common use cases for such models. FIM is claimed to come at little to no expense\\nof autoregressive modeling capabilities; we were broadly able to confirm these results, showcasing\\nlikely degradation only for a handful of tasks for intermediary infilling rates (0.25-0.5). Low and\\nhigh infilling rates had the lowest e ffect on zero-shot performance. Nevertheless, due to lack of wide\\nadoption at the time, we choose to skip FIM, and to instead consider it as an adaptation step for a\\nFalcon-Coder model–this was concurrently demonstrated for Code-LLaMA (Rozière et al., 2023).\\nPrincipled hyperparameters. We experimented with µ-parametrization (Yang et al., 2022) as a way\\nto scale our hyperparameters in a principled way from smaller to larger runs. Unfortunately, we were\\nunable to demonstrate an improvement over our naive strategy of hyperparameters estimation.\\nAlternative optimizers. Encouraged by its strong reported results on BERT and T5, we experimented\\nwith Amos (Tian and Parikh, 2022), an alternative to Adam with adaptive learning rate and weight\\ndecay. We were unable to obtain an improvement for causal decoder-only models, even at small scale.\\nConversation mining. In developing RefinedWeb, we experimented with the idea of mining specific\\ntypes of data from the web. Training dedicated classifiers based on BERT models often resulted\\nin over-fitting, so we instead used simple heuristics (e.g., identifying arguments by the density of\\ntransition words, conversations by finding turns of users). We were able to obtain significant zero-shot\\nperformance uplift, however this data was very scarce. Since training of the models was started\\nbefore we had processed all of CommonCrawl, we could not e ffectively surface all of that data and\\nuse it in priority over standard web data, leading us to leave that idea for future models.\\n17', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 16}), Document(page_content='4.5 Wrapping-it up: validating overall dataset and architecture recipes\\nFor convenience, we refer readers to Section ? for a full overview of the Falcon recipe. We will now\\nvalidate the performance of the recipe at larger scale, with a longer training run and comparisons with\\nmodels from the state-of-the-art. We independently verify: (1) the pretraining dataset, in particular its\\nweb component; (2) the architecture, by training a model adopting it on The Pile.\\nDataset validation. We train 1B and 7B parameters models for 27B tokens and 350B tokens, to\\nreproduce common practices from previous models. We use our baseline architecture, which is\\nbased on GPT-3 (Brown et al., 2020) with ALiBi (Press et al., 2022). We train on The Pile (Gao\\net al., 2020), RefinedWeb (our web dataset, Penedo et al. (2023)), and the Falcon data mixture which\\ncombines RefinedWeb and curated sources without any upsampling. This last mixture is designed for\\nFalcon-180B, and targets a total of 3,500B tokens. See Section 5.1 for details.\\nIn Table 14, we find our data mixture significantly uplifts performance. The large majority of these\\ngains are also achieved by RefinedWeb alone, without the curated data. This highlights our previous\\nfinding that web data alone, when adequately filtered and deduplicated, can train performant models.\\nWe find our 1 /7B model compares favorably with other models from the state-of-the-art, however we\\nnote our setup put it at a small advantage by training for slightly longer.\\nArchitecture validation. We follow the set-up of our dataset validation, and train the architecture\\nvalidation 1B models on The Pile for 27B and 350B tokens. Note that we do not include in this\\nexperiment hyperparameters tweaks: all models use weight decay, and the same learning rate\\nfrom Brown et al. (2020)–this experiment concerns only the architecture of the models itself.\\nWe find our architecture comes with a small performance degradation (Table 14), which we mostly\\nattribute to the reduction in parameters caused by multiquery. We suspect that growing up the\\nmultiquery model would likely close that gap. Nevertheless, it’s interesting to note that while\\ndata improvements have very significant e ffects, architecture improvements are mostly focused on\\nimproving training and inference scalability; they do not result in an uplift in task performance.\\nTable 14: Our data recipe, predominantly based on our work on RefinedWeb (Penedo et al.,\\n2023), significantly improves upon The Pile and other models from the state-of-the-art. Because\\nmultiquery makes models smaller (and we do not control for that e ffect), our architecture comes\\nwith a small zero-shot performance degradation. We suspect controlling for parameter count\\nwould bring our architecture to be on-par or better than the baseline. Nevertheless, improvements\\nto the architecture mostly deliver improvements in hardware scalability for inference and training,\\nwhile improvements to the data recipe significantly uplift the downstream performance of models.\\nUnderline for relevant changes, bold for improvement over baseline, italics for degradation over\\nbaseline.†flags independent evaluations with the EleutherAI Harness (Gao et al., 2021), anda\\nindicates our architecture run is better, whiledshows our data run is better.\\nScale Dataset Architecture Performance\\nzs-main↑zs-comp↑zs-small↑ppl-pile↓\\n1B@27GTThe Pile Baseline 51.7 40.3 45.0 0.868\\nFalcon Baseline 53.5 42.3 48.8\\nRefinedWeb Baseline 53.2 43.4 48.4\\nThe Pile Falcon 51.1 40.0 45.1 0.870\\n1B@350GTThe Pile Baseline 57.8 47.1 54.0 0.763\\nRefinedWeb Baseline 59.8 50.1 55.7\\nThe Pile Falcon 56.6 46.1 52.7 0.775\\n1B@300GT OpenAI babbage†47.8d\\n1B@380GT The Pile GPT-Neo†44.3a,d\\n1B@300GT The Pile BS-A&S†46.1d\\n1B@300GT The Pile Pythia†45.2a,d\\n7B@350GT RefinedWeb Baseline 55.3\\n6B@300GT OpenAI curie†53.7d\\n6B@400GT The Pile GPT-J†53.5d\\n18', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 17}), Document(page_content='5 Implementation\\nBased on our findings from the previous ablations Section 4, and further tests and best practices from\\nthe literature, we now describe the codebases and methods used to train the Falcon series of models.\\n5.1 The Falcon dataset: predominantly web, with added curated and conversational data\\nBased on estimates of our compute budget of 30,000-50,000 PF-days, we target a pretraining dataset\\nsize in the range of 3,000-5,000 billion tokens –we use Ho ffmann et al. (2022) as an upper boundary\\nfor model size and lower boundary for pretraining length. This is more than 2x the size of the dataset\\nfor Chinchilla, and 10x the one for GPT-3; although this range of size is recently becoming more\\ncommon in concurrent models like LLaMA-2 or OLMo (Touvron et al., 2023b; Soldaini et al., 2023).\\nOut of concerns for memorization and degradation caused by repeating data (Carlini et al., 2022;\\nHernandez et al., 2022) we choose to not upsample any sources .\\nHigh-level overview. In Section 4.2.1, we have shown that su fficiently filtered and deduplicated web\\ndata can deliver performant models: this leads to focus on scaling-up web data to achieve the scale\\nnecessary. Because improvements to data quality translate to significant improvements to downstream\\nperformance, and because data processing is tremendously cheaper than model training, we do not\\nconcern ourselves too much with optimizing for costs with data processing–it is likely that a 90%\\ncheaper recipe with less than a 10% relative performance degradation could be found.\\nWe still include a small amount of curated data, inspired by The Pile (Gao et al., 2020) with the\\naddition of conversations from Reddit (Baumgartner et al., 2020), as it is unlikely to degrade perfor-\\nmance if adequately processed (Section 4.2.2) and as it may broaden the downstream applicability of\\nthe model. However, these sources are bound to remain a minority, given that we do not allow any\\nupsampling–they end-up accounting for 13% of our final dataset. Regarding code and multilinguality,\\nwe take a conservative approach: based on our results in Section 4.2.3, we include 8% multilingual\\ndata and 3% code. These lower fractions than the ones experimented with are due to stock con-\\nstraint; specifically for code, further improvements to our pipeline enabled us to significantly scale\\navailability, but this was after the models had started training, so we did not revise the mix.\\nThe final Falcon mixture is presented in Table 15. We designed the mixture based on a 3,500B tokens\\npretraining dataset, not allowing any upsampling of the curated sources. Despite di ffering training\\nlengths, the same mixture (in %) is used for Falcon-7B, 40B, and 180B.\\nTable 15: The final Falcon mixture is predominantly web-based (nearly 85%), but includes\\nother curated corpora (without any upsampling) to broaden the expressiveness of the model.\\nIndividual curated corpora are inspired from The Pile (Gao et al., 2020), but rebuilt from scratch to\\nensure high-quality and compatibility with our data pipeline. Code stock is a rough estimate based\\non an updated pipeline; code data used in Falcon was sourced from permissively licensed GitHub\\nrepositories. Mixture was designed to avoid upsampling; note that total stocks for RefinedWeb were\\nnot known at the beginning of training, as processing was still in-progress. Quantities in tokens.\\nCorpora Pretraining\\nName Source Stock Fraction Used\\nRefinedWeb-English Filtered and deduplicated Common-\\nCrawl, see Penedo et al. (2023)∼5,000B 76% 2,700B\\nRefinedWeb-Euro Filtered and deduplicated multi-\\nlingual (Europe-focused) Common-\\nCrawl, see Penedo et al. (2023)∼2,000B 8% 400B\\nBooks Project Gutenberg 215B 6% 214B\\nConversations Reddit, StackOverflow, Hack-\\nerNews, IRC, YouTube Subtitles170B 5% 168B\\nCode GitHub ∼1,000B 3% 115B\\nTechnical arXiv, PubMed, USPTO, Wikipedia 60B 2% 57B\\n19', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 18}), Document(page_content='5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset\\nOur web data processing pipeline is extensively described in the dedicated paper Refined Web\\npaper (Penedo et al., 2023). In this section, we only highlight key components and decisions.\\nTo scale-up pretraining data, two approaches are possible:\\n•Repeat data. This is the easiest option, and was the norm in computer vision originally.\\nHowever, most large language models have been far more conservative, usually only up-\\nsampling specific corpora for 2-6 times (see Table 3). This is largely due to concerns with\\nmemorization (Carlini et al., 2022) and with deduplicates disproportionately degrading the\\nquality of models (Lee et al., 2022; Hernandez et al., 2022). Recently, Muennigho ffet al.\\n(2023) has argued that while up to 4 epochs on the same data may be acceptable, further\\nrepetition will cause degradation–this leads us to eschew this strategy.\\n•Scale-up web data processing. While scaling curated sources is cumbersome and requires\\nextensive manual work, web data is a massive, plentiful source. Improvements to web data\\nhave high leverage, as they impact a large amount of tokens at once: public crawls such\\nas CommonCrawl may contain in excess of 50-100 trillion tokens, such that even a 90%\\nrejection rate would result in a trillion-scale dataset. However, raw web data is also of\\nextremely poor quality (Trinh and Le, 2018; Kreutzer et al., 2022), containing large amounts\\nof undesirable adult content and machine generated spam. We choose to focus our work on\\nimproving the quality of web data, through large-scale filtering and deduplication.\\nThese approaches are orthogonal, but not antagonist; scaling to frontier models, with pretraining\\ndatastes of 10-100 trillion tokens, will likely require repeating massive web datasets for a few epochs.\\nPhilosophy. RefinedWeb di fferentiates itself from previous web datasets in the following ways:\\n•Extreme-scale. Our Macrodata Refinement pipeline focuses on scalability: we used up to\\n20,000 CPU cores to produce RefinedWeb. With nearly five trillion deduplicated tokens, the\\nRefinedWeb dataset is the largest documented pretraining dataset, supporting the training of\\nlarger models than previously though possible without relying on multiple epochs.\\n•Stringent deduplication and filtering. Inspired by Lee et al. (2022), RefinedWeb is fully\\ndeduplicated. Fuzzy deduplication with MinHash is used to first massively shrink the dataset,\\nand then extract substring deduplication is applied. Filtering heuristics are also first used\\nto reduce text extraction artefacts, and to remove machine-generated content. We find in\\nSection 4.2.1 that this allows web data to match curated corpora.\\n•Neutral filtering. With the exception of language identification, the Macrodata Refinement\\npipeline does not rely on ML-based filtering strategies. Indeed, such filters can easily\\nintroduce or amplify biases into the data Dodge et al. (2021); Welbl et al. (2021).\\n100%97,76%\\n2,24%96,31%\\n1,49%47,51%\\n50,66%35,97%24,28%30,15%16,19%23,34%22,59%14,50%37,88%11,67%18,47%\\nLoading [MathJax]/extensions/MathMenu.js\\nCommon Crawl\\nRW-Raw\\nRW-FilteredURL ﬁlteringText extractionLanguage identiﬁcationRepetition removalDocument-wise ﬁlteringLine-wise correctionsFuzzy deduplicationExact deduplicationRW\\nDocument preparationFilteringDeduplication\\nFigure 6: Subsequent stages of Macrodata Refinement remove nearly 90% of the documents\\noriginally in CommonCrawl. Notably, filtering and deduplication each result in a halving of the\\ndata available: around 50% of documents are discarded for not being English, 24% of remaining\\nfor being of insu fficient quality, and 12% for being duplicates. We report removal rate (grey) with\\nrespect to each previous stage, and kept rate (shade) overall. Figure from Penedo et al. (2023).\\n20', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 19}), Document(page_content='Overview. The Macrodata Refinement pipeline is split into three subsequent stages (see also Fig. 6\\nfor detailed removal rates): (1) document preparation; (2) filtering; (3) deduplication.\\nFor the document preparation, before undertaking any compute-heavy processing, we first filter\\ndocuments based on the URL alone, using a blocklist of adult sites and scoring URLs based on their\\nname. We found that the preprocessed .WET files o ffered by CommonCrawl still contain undesirable\\ncontent (e.g., navigation menus) so we instead process raw .WARC files (HTML response) with\\ntrafilatura to extract natural text. Finally, we use the fastText classifier from CCNet (Wenzek\\net al., 2020) to identify the top language of documents. For English, about 48% of documents remain.\\nIn the filtering stage, we apply a number of heuristics to remove repeated text (which may be an\\nartefact of crawling /text extraction), and documents which are outliers in terms of length, symbol-\\nto-word ratio, etc. These heuristics are inspired by Rae et al. (2021). We also introduce so-called\\nline-wise corrections, which remove lingering artefacts such as likes counter or navigation buttons.\\nThe size of the suitable data is against halved, resulting in about 23% of CommonCrawl being kept.\\nFinally, we apply large-scale deduplication in two steps: first, we remove approximate duplicates at\\nthe document-level with MinHash (Broder, 1997), before removing exact substring matches with\\na suffix array (Manber and Myers, 1993). With the deduplication settings of Lee et al. (2022), this\\nresults in a final halving of the usable data, down to only about 12% of the total data in CommonCrawl.\\n5.1.2 The Microdata curated corpora and conversational masking\\nIn Section 4.2.2, we found that adding curated data from conversations, books, or technical sources\\ndid not further improve performance on top of a strong web baseline like RefinedWeb. However,\\nwe believe this sort of data, along with code, can broaden the expressiveness of the model, and its\\napplicability to di fferent kinds of downstream tasks not captured by our evaluation setup. Accordingly,\\nwe also add a small fraction of curated data, with individual sources inspired from (Gao et al., 2020).\\nWe also add data from Reddit (Baumgartner et al., 2020), and introduce a new attention masking\\nstrategy for formatting tree-like conversations e fficiently. On top of the Microdata curated corpora,\\nwe apply the Macrodata Refinement pipeline with adjusted filter settings (e.g., tuning document\\nlength thresholds for books) and deduplicate individual corpora.\\nComponents from The Pile. We reuse individual components of The Pile (Gao et al., 2020), except\\nwhere there are significant quality or licensing concerns. In all cases, we reimplement them from\\nscratch, to ensure the formats match our data pipeline. We introduced a number of special tokens\\nto reproduce structured information in curated corpora and better control generation: »TITLE« ,\\n»ABSTRACT« ,»INTRODUCTION« , and»COMMENT« . When L ATEX files are available, we convert them\\nto markdown, similar to Lewkowycz et al. (2022). Heuristics from Macrodata Refinement where\\nhand-tuned for each corpora, manually analysing rejected and accepted samples; for books, we also\\nintroduce new rules to remove irrelevant content such as indexes, disclaimers, or tables of content.\\nConversational data. Increasingly, large language models are deployed in \"chatty\" use cases, with\\nback and forth interactions between users and models (Adiwardana et al., 2020; Zheng et al., 2023).\\nAltough models are adapted downstream to this use case, we put a focus on enhancing pretraining\\nwith conversational data; we notably add data from Reddit (Baumgartner et al., 2020).\\nConversation trees and attention masking. One issue with using data from online forums such\\nas Reddit or HackerNews is that this data is formatted in trees, with diverging turns of conversation\\nbetween users. Past models have sampled trajectories from these trees (Thoppilan et al., 2022;\\nChowdhery et al., 2022), but this either means that data has to be repeated, or some trajectories left\\nout. Instead, we find that we can use the attention mask to encode a tree-like structure, allowing later\\ncomments to only attend to comments from their trajectory, ignoring \"side\" comments. Conversations\\nare serialized into sequences in depth-first order, and then comments /turns are masked if they are not\\nrelevant to the current one (i.e., in a di fferent branch or deeper). For positionnal embeddings, we use\\nthe depth of the tree, which naturally serializes the conversation. We illustrate with pseudocode for\\nconverting depth first positions to an attention mask in Appendix F.2. We also employ this strategy to\\nmask documents from one another, instead of relying on the <EOD> token alone. Note that cursory\\nexperiments did not find a benefit for zero-shot performance.\\n21', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 20}), Document(page_content='Do\\n0you\\n1like\\n2Paris?\\n3Not\\n4really,\\n5it’s\\n6too\\n7dirty!\\n8Oh,\\n9I\\n10agree\\n11\\nReally?\\n9it’s\\n10so\\n11nice!\\n12Yes!\\n4It’s\\n5lovely!\\n6\\nDo\\n0you\\n1like\\n2Paris?\\n3Not\\n4really,\\n5it’s\\n6too\\n7dirty!\\n8Oh,\\n9I\\n10agree\\n11Really?\\n9it’s\\n10so\\n11nice!\\n12Yes!\\n4It’s\\n5lovely!\\n6\\nAttention\\nNo AttentionCurrent TokenOther Tokens\\nFigure 7: Tree-like attention masking enables us to realize all conversation trajectories without\\nsampling or repeating data. The tree is serialized depth-first, where the position of the token is the\\ndepth in the tree. This allows us to train on all conversations in the tree at the same time, without\\nrepeating the early turns of the conversation. For instance, the passage Yes! It’s lovely , in pink, has\\nvisibility on Do you like Paris? in green and causal attention on itself, but cannot see other passages.\\n5.2 The Falcon architecture and recipe for e fficient inference and (stable) training\\nOur goals with the Falcon architecture are to maximize training and inference e fficiency, while\\nminimizing impact to downstream performance and risks for the models. In Section 4.3, we outlined\\na number of decisions we made based on the ablations:\\n•Architecture. We use multigroup attention (Section 4.3.1) to improve the scalability of\\ninference, an extension of multiquery (Shazeer, 2019); we use rotary embeddings (Su et al.,\\n2021); we do not use GLU (Shazeer, 2020) because of the increased memory footprint, and\\nuse vanilla GeLU instead; we use parallel attention and MLP blocks (Wang and Komatsuzaki,\\n2021) and remove biases from linear layers (Chowdhery et al., 2022).\\n•Hyperparameters. We use z-loss to help with stability (Shazeer et al., 2018); we use a\\nfixed 0.1 weight decay; we perform a learning rate logarithmic grid search during warm-up\\nand pick the learning rate with the lowest loss at the end of warm-up.\\n5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up\\nLayer norms. When using parallel attention, it is possible to either have separate layer norms for\\nthe MLP and the attention block (closer to the vanilla Transformer architecture), or to use a unified\\nlayer norm for both. Since the gradient computation with regards to the input of the layer norm is\\nlinear, it is possible to maintain the favourable communication volume of parallel attention and MLP\\nwhile using two layer norms. Furthermore, after training, it is possible to merge the two layer norms\\nback into one, by multiplying in the weights and biases of the layer norm into the subsequent linear\\nlayer. This leads us to stay close to the vanilla architecture, and to use two separate layer norms;\\nhowever, we note this introduces unnecessary additional complexity for downstream conversion to\\nother popular formats. For Falcon-7B (later trained), we switched to a single layer norm.\\nTied embeddings. Tying embeddings is a ubiquitous practice for Transformer models: the embedding\\nweights converting the tokens xintoz0=xWare the same that converts the embedding back into the\\npredicted logits p=znWT. Although it is still used by most recent LLMs (GPT-3 Brown et al. (2020),\\nPaLM Chowdhery et al. (2022), LLaMA Touvron et al. (2023a)), the original motivations (Press\\nand Wolf, 2016; Inan et al., 2016) may not entirely be relevant. Notably, weight sharing was at the\\ntime used to reduce the size of models; but for models with over 100 billion parameters, embeddings\\nare not a significant fraction of the parameters. Furthermore, weight sharing poses challenges in\\ndistributed training, as it requires additionnal communications. The semantic argument remains, but\\ncursory experiments showed no strong impact at the 1B parameters scale. Still, in the interest of not\\nadding additional risks to the training of the Falcon series, we keep our embeddings tied.\\n22', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 21}), Document(page_content='Table 16: Summary of the shape, hyperparameters, and distribution strategy of the Falcon\\nmodels. Falcon-7B was trained after Falcon-40 /180B, with an experimental increased batch size.\\nFalcon-7B Falcon-40B Falcon-180B\\nData 1,500B 1,000B 3,500B\\nShape\\nnlayer 32 60 80\\ndmodel 4,544 8,192 14,848\\ndhead 64\\nnq 71 128 232\\nnkv 1 8 8\\ndvocab 65,024\\nntokens 2,048\\nPretraining\\nLearning rate 6 ×10−41.85×10−41.25×10−4\\nDecay Cosine, divides by 10\\nRamp-up 4B 4B 4B\\nBatch-size 2,304 1,152 2,048\\nWarm-up 30B 100B 100B\\nWeight decay 0.1\\nGradient clipping 1. 0.6 0.4\\nZ-loss 1 ×10−4\\nParallelism\\nTP 1 8 8\\nPP 2 4 8\\nDP 192 12 64\\nVocabulary size. The size of the vocabulary in language models can di ffer widely from character\\nlevel models with 256 entries (Xue et al., 2022) to massively multilingual models with millions\\n(Liang et al., 2023). For generative models, practices have collapsed around two modes: vocabularies\\nin the 30-60k range for monolingual models (Brown et al., 2020; Rae et al., 2021; Touvron et al.,\\n2023a), or in the +100k range for models more inclined towards multilinguality (Scao et al., 2022a;\\nChowdhery et al., 2022). Although larger vocabularies may have better fertility, and hence yield\\nfaster inference per byte of text, they also come with some caveats: from a scalability perspective,\\nthey can lead to unbalanced pipeline stages, and may require more storage space; also, it is unclear\\nwhether models optimally use them (Lieber et al., 2021). We train our tokenizer on a vocabulary size\\nof 65,024, and store vocabulary information in a 16bit unsigned integer–this leaves about 500 extra\\nvalues to use for downstream adaptations (e.g., paradigm tokens for UL2 Tay et al. (2022b)).\\nModel scaling. We outline the shape and hyperparameters of the Falcon models in Table 16. When\\nincreasing compute budget, resources can be either spent towards a larger model (increased parameter\\ncount) or towards longer training (increased token count). Ho ffmann et al. (2022) recommends a\\njoint (i.e., equal) increase for optimal scaling. However, this finding should be nuanced in two ways:\\n(1) increasing parameter count also increases downstream inference costs, which can be significant\\nif a model is widely deployed; (2) on the other hand, if data-constrained, increasing the size may\\nbe a way to trade compute for increased downstream performance. For Falcon, we choose to use\\nHoffmann et al. (2022) as a lower bound for pretraining length and as an upper bound for model size.\\nBeyond model size, there is also the question of how to shape these parameters: e.g., should they be\\nallocated to make a deeper or shallower model, to widen attention heads or to increase their count.\\nScao et al. (2022b) conducted a short review, highlighting that model depth is typically only scaled\\nlogarithmically with total parameter count (Levine et al., 2020). Practices around attention head size\\nhowever varied. We broadly follow the logarithmic depth scaling recommendation, and uses a fixed\\nattention head size of 64 to optimize for performance with FlashAttention (Dao et al., 2022). We also\\nfix our number of queries in multigroup to be equal to the number of tensor parallel degrees used\\nduring training, and train for a fixed sequence length of 2,048. We note that this context length can be\\nefficiently increased with an a posteriori adaptation (Chen et al., 2023).\\n23', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 22}), Document(page_content='5.2.2 Large language model alchemy: hyperparameters for pretraining\\nWe report hyperparameters used during pretraining in Table 16, and highlight some decisions below.\\nLearning rate search. The procedure we describe in Section 4.3.5 results in a learning rate of 6×10−4,\\n1.85×10−4, and 1.15×10−4for Falcon-7, 40, and 180B respectively. This is significantly higher\\nthan learning rates reported by previous models, but we found our training runs to be (mostly) stable.\\nIn hindsight we believe our search procedure may result in higher learning rates than optimal–an\\nacceptable tradeo ff, since recovering from spikes is relatively easy (Section 5.4).\\nLearning rate ramp-up. We perform a long ramp-up over 4 billion tokens for all models.\\nBatch size warm-up. Practices around batch-size warm-up have surprisingly diverged widely\\ndepending on the underlying hardware: models trained on GPUs, such as GPT-3 (Brown et al.,\\n2020), BLOOM (Scao et al., 2022a), or MT-NLG (Smith et al., 2022), have typically performed a\\nfined-grained warm-up for 10-20 billion tokens; meanwhile, models trained on TPU, such as Gopher\\n(Rae et al., 2021), Chinchilla (Ho ffmann et al., 2022), or PaLM (Chowdhery et al., 2022) often double\\nthe batch size mid-training or take larger batch size steps at 25 /50% through training. Some recent\\nmodels have also elected to skip batch size warm-up entirely (Zhang et al., 2022; Touvron et al.,\\n2023a,b). At small scale, we found longer warm-ups to never hurt downstream performance, and\\nin fact to generally deliver better models; this leads us to adopt a long warm-up strategy, over 100\\nbillion tokens–note that we are able to scale the data parallelism degree and overall size of the cluster\\nduring training, which means this comes at limited throughput cost. For Falcon-7B, wall-clock time\\nconstraints lead us to opt for an accelerated schedule over only 30 billion tokens.\\nGradient clipping. We set the treshold for gradient clipping to 0.6for Falcon-40B. For Falcon-180B,\\nwe initially started with 0.6but later reduced it to 0.4to improve training stability. In both cases, we\\ntuned the gradient clipping threshold to only a ffects outlier events and not the vast majority of the\\ntraining steps. For Falcon-7B, we set it at 1. We find that training at this scale is anyway not really\\nprone to instabilities, even with the large batch size we adopted.\\nOptimizer. We use AdamW (Loshchilov and Hutter, 2017): it is the most commonly used optimizer,\\nand has been proven time and time again to perform well, both in general and for large language\\nmodel training in particular. To increase performance during training we use the fused optimizer\\nkernel from Megatron Shoeybi et al. (2019). However, we note that fused kernels for optimizers are\\nless important in large scale training, especially when when optimizer sharding is used.\\nNo dropout. As large language models are typically trained for a single epoch of relatively unique\\ndata, they typically do not use dropout (Srivastava et al., 2014). Shortage of data and recent papers\\nsuggesting a few epochs may be tolerable (Xue et al., 2023) will surely challenge this practice as\\nfuture model require tens of trillions of tokens, but for the Falcon series we did not use dropout.\\n5.3 Large-scale distributed training on cloud infrastructure with Gigatron\\nRather than training on expensive dedicated HPC resources, we elected to train the Falcon series on\\ncloud infrastructure to improve cost-e fficiency. The Falcon series was trained on clusters of p4don\\nAWS–with up to 4,096 A100s for Falcon-180B. Key metrics for our training infrastructure include:\\n•Nodes with 8×A100 40GB .We found that configurations with 4x A100 40 /80GB, popular\\nin some datacenters with power or granularity constraints, resulted in lower throughputs\\nbecause of the reduction in available degrees of tensor parallelism (4 instead of 8). However,\\nwe found the 40GB version of the A100 to o ffer increased availability and cost-e fficiency.\\n•50Gbps interconnect per GPU. State-of-the-art infrastructure will come with 200Gbps\\ninterconnect per A100, powered by low-latency InfiniBand; however, these configurations\\ncan be prohibitively expensive. We found that for models up to size of Falcon-180B,\\nbandwidth had only a limited impact on overall throughput (mostly linked to the size of the\\nall_reduce across data parallel degrees). Conversely, the higher latency of EFA on p4d\\nwas the main bottleneck for pipeline communications, especially for small scale models.\\n•No distributed filesystem. We stream data directly from S3, instead of relying on a\\ndedicated filesystem. Distributed filesystems are expensive and di fficult to maintain, and the\\nsmall data I /O volumes incurred by large language model training do not justify their use.\\n24', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 23}), Document(page_content='Our infrastructure accordingly exists as an in-between between a true HPC system and a flexible\\ncloud environment–notably, we found that the GPUs still had to share a single spine in the datacenter,\\nas multispine configurations were unreliable. Altough it is more cost-e fficient, it also requires us to\\nbe mindful of its limitations. We found that the popular (and simple) recipe of training with fully\\nsharded data parallelism (Rajbhandari et al., 2020) did not scale well to this infrastructure. Instead, we\\nrequired the finer control of 3D parallelism (Narayanan et al., 2021b) to achieve optimal performance.\\nBecause of limitations in open-source frameworks at the time, we elected to build our own proprietary\\ndistributed training framework. Gigatron is based on pytorch , and at its core implements a 3D\\ndistributed parallelism strategy (Shoeybi et al., 2019; Narayanan et al., 2021b) combined with ZeRO\\noptimizer sharding (Rajbhandari et al., 2020) to reduce memory consumption and improve scalability.\\n5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability\\nData Parallelism (DP). Data parallelism (Fig. 8) is by far the most commonly used form of paral-\\nlelism. All machine learning frameworks now provide ways to easily parallelize model training across\\ndata samples: pytorch with Distributed Data Parallel ( DDP),jax withpmap , andtensorflow\\nwithMirroredStrategy (Paszke et al., 2017; Bradbury et al., 2021; Abadi et al., 2015). Data\\nparallelism is appealing thanks to its simplicity: the distributed machinery can be hidden away inside\\nthe framework easily, without any interactions with the user-defined architecture. In its simplest\\nimplementation, data parallelism only requires two modifications to the training procedure: (1) each\\ndevice needs to operate on unique data samples; (2) the gradients needs to be reduced across devices\\nto keep the weights in sync. We note that this all_reduce will grow with the batch size, eventually\\nmaking data parallelism bandwidth-bound. Furthermore, data parallelism is however no cure-all:\\nsince the model is not sharded, memory footprint per device is constant, even as we add more devices.\\nAccordingly, data parallelism alone is constrained to models which fit on a single device.\\nTensor Parallelism (TP). To share the model across devices, we need to turn to model parallelism.\\nIntroduced by Shoeybi et al. (2019) in its most popular form, tensor parallelism splits linear layers\\nin the attention block and MLP in a principled way to reduce communication volume. This can\\nbe viewed as an instance of width-wise model parallelism. Specifically, for the simple case of a\\ntwo-layer MLP, by making the first layer column parallel and the second row parallel, only a single\\nall reduce is necessary to produce the final result–no communication of the intermediary result is\\nrequired. To propagate the gradient backward, the matrices are transposed, and hence the row parallel\\nlayer turn into a column parallel layer and vice-versa: this maintains the the e fficient communication\\npattern. We illustrate column and row parallelism in Fig. 9. A similar approach can be taken to\\nsplit attention blocks, splitting the heads across GPUs–wherein the K,Q,V calculations are column\\nparallel and the final projection row parallel, with all computations in between independent of one\\nanother between GPUs. Accordingly, entire blocks in Transformers can be e fficiently parallelized\\nthis way. Tensor parallel, however, requires both high-bandwith and low-latency interconnect to\\nbe effective: accordingly, on current GPU infrastructure, it is constrained within a single node, and\\ncannot e fficiently be used across nodes. Models will thus typically be trained with a tensor parallel\\ndegree up to 8; this may still be insu fficient to create small enough shards of the model.\\nData Parallel\\nModel\\nReplica #0Model\\nReplica #1Reduce Gradients\\nsample\\n#0sample\\n#1sample\\n#2sample\\n#3...\\nFigure 8: Data parallelism creates model replicas on each device and process di fferent samples\\nin parallel. Model replicas are placed on di fferent devices and compute gradients on di fferent data\\nsamples in parallel. The gradients are then reduced before the optimization step is carried out.\\n25', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 24}), Document(page_content='Input Weight ResultColumn Parallel\\n=GPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nAll GPUs\\nInput Weight ResultRow Parallel\\n+\\n+\\n+=GPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nFigure 9: By alternating column and row parallelism, no communications are required between\\ntwo subsequent matrix multiplications, enabling tensor parallelism to e fficiently split attention\\nand MLP blocks across GPUs. For column parallel matrix multiplication, the input is replicated\\n(all_reduce in the backward) across all GPUs, which can then independently perform their op-\\neration. The result is already split across GPUs to execute the next matrix multiplication in a row\\nparallel fashion. Finally, the output of each GPU is summed through an all_reduce . Intermediary\\nresults between the two matrix multiplications are never communicated.\\nPipeline parallelism. Model parallelism can also be performed depth-wise, by grouping layers\\ninto subsequent stages to be executed on di fferent accelerators. However, naive pipeline parallelism\\nwould either be ine fficient or result in an immediate roadblock: indeed, stages have to split batches to\\nconcurrently process multiple forwards and backwards. We adopt the PipeDream-Flush schedule of\\nNarayanan et al. (2021a), now commonly referred to as 1F1B. We found more involved schedules,\\nsuch as Interleaved-1F1B (Narayanan et al., 2021b), to be beneficial when the number of microbatches\\nper model replica is small, typically below 64. After the batch size rampup is completed, we train\\nwith a larger number of microbatches; as such we do not observe any speedup from interleaving. In\\norder to reduce the communication volumes, we consistently make use of so called scatter-gather\\noptimizations, where the activations sent to the next stage are first sharded over the tensor parallel\\ndegree and then gathered again once received on the next stage (see Fig. 10 for an illustration). This\\noptimizes for underlying interconnect topology (intranode communications over NVLink where the\\ngather is executed are significantly faster), even more so if internode links have been rail optimized.\\nThis optimization reduces communication volume for pipeline parallelism by a factor of the tensor\\nparallel world size, which in our case is 8.\\nOptimizer sharding. Model weights and gradients are materialized in a contiguous bfloat16\\nmemory bu ffer. Similar to (Rae et al., 2021), the optimizer maintains a fp32 version of those\\nweights /gradients with which to calculate updates. When using the Adam optimizer in this setting, as\\nwe also need to keep track of exponential averages, we require 20 bytes per model parameters:\\n2model param +2model grad|                       {z                       }\\nbfloat16+4opt param +4opt grad +4exp avgs +4exp avg sqs|                                                {z                                                }\\nfloat32=20bytes/param (1)\\nA full state of Falcon-7 /40/180B will occupy 140GB, 800GB, and 3,600GB of memory per replica,\\nrequiring at least 4, 20, or 90 A100 40GB per replica just in weights +gradients +states of memory.\\n26', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 25}), Document(page_content='ScatterP2P-CommAll Gatther\\nGPU 0\\nGPU 1\\nGPU 2\\nGPU 3\\nAll GPUs\\nFigure 10: Instead of each rank redundantly sending the full tensor, scatter /gather optimization\\nhas each rank sends a shard of the tensor, and then leverages the fast intranode interconnect\\ntogather it back. This patterns is more e fficient as internode communications are typically much\\nslower than intranode ones; note that the scatter is e ffectively a no-op, as each rank already has the\\nfull tensor. Internode rail optimization typically enables GPUs of the same rank in di fferent nodes to\\ncommunicate peer-to-peer with minimal intermediaries, further accelerating this pattern.\\nThe optimizer state alone accounts for 80% of the memory footprint, and eventually bottlenecks\\nour memory use, preventing us from, for instance, increasing the batch size to better saturate GPUs\\ninstead. Accordingly, we choose to shard the optimizer state across data parallel degrees–a practice\\ncommonly referred to as ZeRO-1 in the literature (Rajbhandari et al., 2020), and illustrated in Fig. 11.\\nRather than storing a full redundant copy of the optimizer on each data parallel degree, the optimizer\\nis independently sharded DP times. With optimizer state sharding the number of bytes per parameter\\nis further reduced with increased data parallelism, improving scalability:\\nBytes per parameter =4+16\\nDP(2)\\nFor Falcon-7 /40/180B, we now only require 30, 215, and 765GB of memory, or 1, 6, 20 A100\\n40GB to hold a model replica. Note that these do not account for activation memory. Now that the\\noptimizer is split, we however have to face increased communication burden during the backward.\\nFirst, areduce_scatter is applied on the bfloat16 model gradients: for each optimizer shard,\\nthis derives the relevant gradient shard reduced across all DP degrees. The resulting synchronized\\ngradient chunk is then copied to the fp32 optimizer gradient bu ffer for use by the optimizer. The\\noptimizer step is performed, resulting in an updated copy of the relevant fp32 sharded optimizer\\nweights. Finally, an all_gather is used to consolidate the bfloat16 model weights for all workers\\nfrom the aforementioned fp32 sharded optimizer weights. Pseudo-code for this ZeRO-1 workflow is\\nprovided in Appendix F.3. Interestingly, in terms of total communication volume, this workflow is\\nequivalent to theall_reduce usually used in the data parallel setting Rajbhandari et al. (2020).\\nWe found that across all scales (from 1 billion parameters to 180 billion parameters), optimizer\\nsharding had no performance overhead compared to traditional data parallelism. In fact, since\\noptimizer sharding frees-up memory, it enables us to use a larger microbatch size, hence resulting in\\nbetter ressource saturation and higher throughput systematically.\\nNo sequence parallelism. Li et al. (2021) proposed a novel strategy to reduce the activation memory\\nduring training: it’s possible to shard the activations on the residual stream across the tensor parallel\\nworkers where these activations are otherwise replicated. This sharding is relatively cheap: instead\\nof using an all_reduce after the MLP one can replace it with a reduce_scatter followed by an\\nall_gather right before the next decoder block. However, after implementing the measures to save\\nmemory discussed in 5.3.3, we observe that sequence parallelism is not necessary. We also saw a\\nslight decrease in throughput which made us decide against employing it during the final training.\\n27', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 26}), Document(page_content='parameters bf16\\ngradientsbf16\\nexp avg gradients fp32\\nexp avg gradients2fp32parameters fp32\\ngradientsfp32\\n... ...GPU 0 GPU i GPU n−1GPU i\\nSharded Optim StateFull Optim State\\nFigure 11: Optimizer sharding splits the large optimizer state across data parallel degrees,\\nreducing memory footprint and improving scalability. The freed memory can be traded for an\\nincreased microbatch size, improving throughput. Figure inspired from Rajbhandari et al. (2020).\\n5.3.2 State-of-the-art throughput with dedicated Triton kernels\\nFlash Attention. Memory e fficient attention alternatives have long garnered the attention of the\\ncommunity–however, these came with significant changes to the attention scheme, often resulting in\\ndegraded downstream performance, especially when scaling-up. It’s only (relatively) recently that\\nexact algorithms have been developed to compute attention without requiring materialization of the\\nfull attention matrix in memory (Rabe and Staats, 2021). Dao et al. (2022) subsequently showed that\\nunified kernels leveraging the same techniques can significantly speed-up model training. We use a\\ncustom implementation in Triton (Tillet et al., 2019), which incorporates some of the improvements\\nconcurrently developed for Dao (2023). We note that the use of FlashAttention is the main driver of\\nimproved throughput during training. Notably, the memory savings allow us to not rely on activation\\ncheckpointing during training, instead focusing our FLOPS on contributing to model training directly.\\nDedicated FlashAttention kernels are also faster in absolute terms than their naive counterparts.\\nInterestingly, we note that FlashAttention is significantly more numerically accurate than traditional\\nattention when both are performed in bfloat16 and compared to fp32 as the ground truth.\\nOther Triton kernels. We also employ specialised kernels for the rotary positional embeddings as\\nwell as the layer norms in Triton. For rotary embeddings we see a particularly large improvement\\ncompared to their unfused PyTorch counterpart. We note that since the transformation is the same for\\nall heads at the same position it’s possible to reuse the expensive trigonometric functions, additionally\\nsimplifying its use as nothing needs to be cached in RAM, as is common in other implementations.\\n5.3.3 E fficient memory use via selective recomputation implemented as a monolayer\\nAs we target A100 40GB for cost-e fficiency, memory footprint is a significant concern for us.\\nBoth ZeRO and FlashAttention already significantly improve memory availability for us, but we\\nfree up additionnal memory via selective recomputation. Li et al. (2021), on top of sequence\\nparallelism, proposed to recompute some activations rather than storing their output in memory for\\nbackpropagation–as activations are typically cheap to evaluate. We take this idea a step further, and\\nrecompute not only all the activation functions but also the layer norms. We save to memory the\\nstatistics of the layer norm only, which makes recomputation trivial. Overall, this implementation of\\nselective recomputation reduces the memory consumption of the decoder block by a factor 2x, while\\ncausing no degradation in throughput from the additional computations.\\nDue to limitations in the autograd engine of PyTorch, it’s rather di fficult to only recompute e.g. the\\nlayer norms without also recomputing the subsequent linear layer. In order to achieve this, we create\\na single custom autograd function for the entire decoder block–we dub this idea the monolayer .\\n28', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 27}), Document(page_content='5.3.4 Numerical precision: all you need is bfloat16 ?\\nBrown et al. (2020); Zhang et al. (2022); Scao et al. (2022a) all reported stability issues training\\nmodels in the hundred billion parameters range when relying on fp16 ; we instead adopt bfloat16 ,\\nwhich results in stable training more or less out-of-the-box. Rae et al. (2021) reported significant\\nimprovements from using stochastic rounding when quantizing the float32 parameters to bfloat16\\nafter the optimization step. In our case however, where the entire optimizer state is stored in float32 ,\\nwe do not observe a similar improvement. Instead, we observe that stochastic rounding helps during\\nthe initial steps, before the optimizer state has equilibriated to the training dynamics, but that training\\nwithout stochastic rounding approaches the same training trajectory shortly thereafter.\\n5.3.5 Quality-of-life features for improved flexibility and reliability\\nTopology-agnostic checkpoints. Surprisingly, when we developed the Falcon series, most distributed\\ntraining libraries strongly tied their checkpoint format and distribution topology: a change to the\\ntopology would require manually converting the checkpoint to a new format. Since we were\\nplanning to grow our clusters during training, we ensure that the model and optimizer checkpoints\\nare readable /writable between any topology configurations–similar to t5x(Roberts et al., 2022).\\nLow-discrepancy data loading. When aggregating di fferent sources into a single mixed dataset it\\nis common to randomly sample each source according to a target probability /weighing per dataset.\\nHowever, this only guarantees sampling from the sources with the target probability in expectation.\\nInstead, it is desirable that the e ffective weight during each subset of the training is constant. We use\\na predefined sampling pattern between di fferent data-sources with a relative short length of 10,000\\nsequences which is guaranteed to contain the exact weights for each data source.\\nTopology discovery and optimization. In order to optimize throughput, it’s important to place ranks\\nwith high communication volume on instances close to one another. At variance with traditional\\nHPC platforms, AWS does not expose topology information for the instances allocated to a training.\\nAccordingly, to optimize placement, we first discover the topology by measuring the bandwidth\\nbetween all pairs of nodes, and we subsequently optimize rank placement against the measured\\ntopology. Regarding the topology discovery phase, a naive solution would be for the first node to first\\ncheck the bandwidth to all other nodes, then the second node and so on. That would require O(n2)\\nsequential measurements. Instead, it is possible to achieve this in only O(n)steps, by doing many\\ncomparisons in parallel–see pseudocode provided in F.1. Once the topology has been (e fficiently)\\ndiscovered, we leverage Gromov-Wasserstein optimal transport (Mémoli, 2011) for placement. We\\nmeasure the source distance matrix based on the measured bandwidths, and define the target distance\\nmatrix from the distances in the pipeline /data parallel grid. We then use the Python Optimal Transport\\nToolbox (Flamary et al., 2021) to solve the resulting problem e fficiently.\\n5.4 Run management: keeping large-scale infrastructure running\\nHardware failures. When scaling-up to hundreds of nodes, hardware failures become increasingly\\ncommon–for Falcon-180B, every day on 4,096 A100 is equivalent to over 11 years of cumulative\\nuse! Furthermore, in our cloud setup, whenever the run is restarted, we sample a new anonymised\\nselection of nodes–we are unable to keep a history of which nodes from previous allocations were\\nsuspect. Accordingly, it is critical to be able to rapidly identify and exclude faulty nodes. We found a\\nlarge majority of hardware failures to be linked to faulty A100s, specifically to corrupted memory\\nrows. These failures do not always manifest with an Xidcode, and require manual tests to catch\\nthem–they typically result in computations returning NaNs. On start-up, we run a series of large matrix\\nmultiplications to catch these failures; during training, we also keep track of NaNs in communications\\nto rapidly identify culprit nodes. Additionally, we run some simple communication tests on start-up,\\nto ensure that the communication primitives work as expected.\\nMonitoring. We find that many web-based monitoring tools sample metrics, even when no smoothing\\nis applied: this can hide critical events like spikes. Accordingly, we deploy our own local viewers.\\nSpikes. We encountered few spikes during training. Similar to Chowdhery et al. (2022), when a spike\\noccurred, we resumed from the latest pre-spike checkpoint and skip 1 billion tokens. Nine spikes\\nwere encountered during the training of both the 40B model and the 180B model.\\n29', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 28}), Document(page_content='6 Results\\nBackground. The natural language processing community has developed a plethora of benchmarks to\\nassess the capabilities of models: from tasks inspired by reading comprehension tests (e.g., RACE Lai\\net al. (2017)), to world-knowledge evaluations (e.g., OpenBookQA Mihaylov et al. (2018)), or so-\\ncalled commonsense tasks (e.g., HellaSwag (Zellers et al., 2019)). Historical benchmarks like GLUE\\nand SuperGLUE also aggregate many linguistically-motivated tasks, measuring abilities such as word\\ndisambiguation or recognizing entailment (Wang et al., 2018, 2019). However, as large language\\nmodels have gained in capabilities and broadened in their applications, the landscape of evaluations\\nhas shifted away from this last genre of tasks; instead, recent benchmarks such as MMLU (Hendrycks\\net al., 2020) or BigBench (Srivastava et al., 2023) attempt to capture generic knowledge and abilities,\\nrather than linguistic behavior–perhaps in an attempt to stay closer to common use cases of large\\nlanguage models. Code evaluations have also grown increasingly common (Chen et al., 2021), along\\nwith mathematics tasks (Cobbe et al., 2021). Recently, technical reports on latest models have also\\nincluded numerous academic and professionnal exams (OpenAI, 2023a; Anil et al., 2023).\\nAlong with this shift in subjects, practices around how models are evaluated have also changed.\\nFor zero /few-shot evaluations, the canonical setup was popularized by Brown et al. (2020): as\\nmost classic NLP tasks have a limited number of choices, these can be evaluated by calculating the\\nlog-probabilities of the choices given the question, instead of generating freeform answers. This puts\\nstrict guardrails on the model, simplifying implementation and reproduction of evaluations as they\\ndo not depend on autoregressive inference with sampling in this framing. However, this inflexibility\\nis not always desired: for instance, chain-of-thought prompting (Wei et al., 2022b) let the model\\nwrite down intermediate steps of its reasoning before giving a final answer. This setup may also not\\nbest illustrate how models are used downstream; common freeform tasks, such as summarization,\\nrequire autoregressive generation. However, for these tasks, a separate challenge of evaluating the\\nmodel’s answer rapidly arise. For instance, Stiennon et al. (2020) found that ROUGE scores for\\nmodel-generated summaries are not necessarily aligned with human preferences.\\nTypical large language models will be deployed as a chatbot /virtual assistant. This is an extremely\\nchallenging use case to quantify: it requires appropriate style and chattiness, wide world knowledge,\\nand often reasoning /code abilities. Such deployments should also ideally be harmless and honest\\non top of being helpful (Bai et al., 2022a), further complexifying evaluation and tradeo ffs. Notably,\\nclassical NLP recipes translate poorly into better chatbots: we illustrate this in Fig. 12, where we\\nevaluate variants of Falcon-40B on the popular SuperGLUE benchmark (Wang et al., 2019), and on a\\nset of 250 prompts with completions rated by human annotators. The popular FLAN recipe (Longpre\\net al., 2023) results in the model with the best zero-shot performance on SuperGLUE; however, such\\na model is also the one least preferred by human annotators. Instead, models trained on so-called\\nself-instruct (Wang et al., 2022b; Taori et al., 2023) datasets improve SuperGLUE performance in a\\nsmaller way, but results in significant improvements to annotator preference–unfortunately, human\\nratings are more expensive to collect than evaluating on SuperGLUE!\\nFigure 12: Human ratings can be at odds with NLP task performance. Variants of Falcon-40B\\nfinetuned on di fferent datasets, horizontal lines for baselines from the OpenAI API and from human\\nannotators. Star ratings collected blind, from a pool of 15 annotators across 250 unique prompts.\\n30', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 29}), Document(page_content='As a middle ground between expensive human preference data and cheap NLP evaluations, it has\\nbeen recently proposed to use a strong external model (e.g., GPT-4) as a judge (Chiang and Lee,\\n2023; Chiang et al., 2023; Zheng et al., 2023)–broadly inspired by RLAIF, which seeks to substitute\\nhuman annotators in RLHF with models themselves (Bai et al., 2022b; Dubois et al., 2023). However,\\nit is unclear yet how reliable these practices are (Wang et al., 2023)–they are also predominantly\\nfocused on evaluating models finetuned for downstream usecases, not pretrained models.\\nFinally, fair comparisons across models are di fficult. First, task selections diverge widely: the PaLM\\npapers (Chowdhery et al., 2022; Anil et al., 2023) typically reproduce the setup of Brown et al. (2020);\\nrecent technical reports (OpenAI, 2023a; Inflection, 2023) report arbitrary selection of tasks in varying\\nsettings; and state-of-the-art models like Gopher (Rae et al., 2021), Chinchilla (Ho ffmann et al.,\\n2022), or LLaMA (Touvron et al., 2023a,b) have all elected to report varying selections of NLP tasks.\\nNotably, there is a lack of standardization across these setups; while standardized benchmarks exist,\\nsuch as the Eleuther AI Evaluation Harness (Gao et al., 2021), or HELM (Liang et al., 2022), they\\nhave only seen limited adoption among open-access models–and most papers do not report details of\\ntheir evaluation setup, including prompts used. Di ffering practices on data formatting or tokenization\\nmay result in widely di fferent task scores (Fourrier et al., 2023); in a way, one-size-fits-all evaluation\\ndoes not quite exist yet. We further discuss this issue in Section 6.1 with concrete examples.\\nOur evaluation setup. Since this paper focuses on the pretrained models in the Falcon series, we\\nchoose to center our evaluations on the more classic logprobs-based setup of Brown et al. (2020). This\\nsimplifies comparisons, although fairness concerns remain across models. To address these, we split\\nour evaluations across four comparisons: (1) in 1-shot against the PaLM models (Chowdhery et al.,\\n2022; Anil et al., 2023), with the tasks of (Brown et al., 2020); (2) on a small set of few-shot tasks\\nreported by the GPT-4 paper (OpenAI, 2023a); (3) against state-of-the-art models across common\\nsense, question answering, and code tasks; (4) against models which also report results from the EAI\\nHarness (Gao et al., 2021), for which we are able to compare with identical prompts and metrics. We\\nnote that this evaluation setup is only intended to provide a broad, first look at the performance of\\nthe model; adequate in-depth domain specific evaluations after dedicated specialization of the model\\nwould be required to inform about the use of the Falcon series in downstream use cases.\\n6.1 To prompt or not to prompt: comparing evaluations across codebases\\nWhen evaluating models in few-shot, two main discrepancies across setups can arise: (1) variations\\nin the style and writing of the prompt; (2) di fferences in the metrics used to measure accuracy.\\nPrompting. Specifically regarding (1), not only can small wording tweaks drive changes in per-\\nformance (use of plurals, yes /no instead of true /false, typos, etc.), but tasks can also be framed in\\ndifferent ways. For instance Rae et al. (2021) found that for multiple-choice question answering\\ntasks, writing down the options in the prompt was beneficial for larger models, but detrimental for\\nsmaller models on RACE (Lai et al., 2017)–regardless of whether the model was asked to predict\\nthe answer itself or a letter key for it. In the case of RACE, this can account for an absolute change\\nof 10-20%. As most commonly used tasks were created before zero /few-shot evaluations became\\npopular, there is rarely such a thing as a canonical prompt, or even framing, for a task. Furthermore,\\nthe vast majority of papers do not report the prompts they used, which hinders reproducibility.\\nFigure 13: In all our evaluations, we rank possible answers by their logprobabilities according\\nto the model given a prompt. In grey in the prompt, we highlight the alternative possibility of\\noutlining each candidate explicitely in the prompt ( CH)–in line with Rae et al. (2021), we find this\\nbeneficial for larger and more capable models, but detrimental for smaller ones.\\n31', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 30}), Document(page_content='Table 17: At different model sizes, di fferent prompt formulations and metrics calculations deliver\\nthe best zero-shot performance. Notably, we finding that outlining multiple-choice questions\\noptions in the prompt explicitly ( CH) nearly always improve performance for larger models, but\\ndegrades it for smaller, less capable models. Unconditional normalization ( UN) has a smaller e ffect,\\nwith improvements on some tasks but not others. Using both tricks together ( BT) never improves\\nperformance over using the best one individually. Degradation, improvement over baseline: Gao et al.\\n(2021) default prompt and length-normalized logprob calculation for CHandUN, best ofCH/UNforBT.\\nARC-Easy ARC-Challenge OpenBookQA\\nUN CH BT UN CH BT UN CH BT\\nFalcon-7B 73,7 68,1 33,3 24,7 44,5 49,5 27,4 24,8 44,6 39,2 24,4 24,0\\nFalcon-40B 81,2 76,6 80,5 78,8 56,7 59,0 62,1 61,3 48,0 43,2 61,2 56,4\\nFalcon-180B 84,7 79,5 94,4 84,6 63,7 63,9 83,5 81,6 47,6 48,8 76,4 71,8\\nMetrics. On most tasks, accuracy is typically reported; however, practices on the calculation of\\nthe logprobabilities used to evaluate candidate answers di ffer. For tasks with multiple choices,\\nP(candidate|context )is evaluated with each choice used as a candidate successively, and the choice\\nwith the higehst probability is taken as the model’s answer. For tasks with long answer (i.e., which\\ndon’t fit in a single token) the logprobabilities of each token in the candidate are summed to es-\\ntimate the logprobabilities of the overall completion. When comparing choices /completions with\\nwidely di ffering lengths, this can be unfair to the longer ones: they end-up being more unlikely. To\\ncompensate for this, the logprobabilities are sometimes normalized by the length of the candidate\\nanswer–a first potential divergence. Brown et al. (2020) also found that some answers were heavily\\nskewed by being more likely to follow the final sequence \"Answer:\" in the prompt. Accordingly, for\\nARC, OpenBookQA, and RACE they choose to normalize the probabilities by the so-called uncondi-\\ntional probability of each completion: P(candidate|context )/P(candidate|\"Answer:\" ). Touvron et al.\\n(2023a) also adopts this practice for OpenBookQA and BoolQ, but it’s often unclear whether other\\npapers have adopted it as well, as details on the calculations of logprobabilities are rarely provided.\\nExperiments. We explore the impact of outlining multiple-choice options in the prompt (see Fig. 13\\nfor an illustration), and of normalizing by the unconditional probability, in zero-shot on ARC\\nand OpenBookQA, for Falcon-7B /40B/180B in Table 17. We find widely di fferent e ffects for the\\ntwo practices across both model size and tasks. On ARC-Easy, unconditional normalization ( UN)\\nalways degrade performance, while it improves it for all models on ARC-Challenge, and improves\\nperformance only for Falcon-180B on OpenBookQA. Outlining the choices in the prompt ( CH) always\\ndegrade performance for Falcon-7B, but always improves it for Falcon-180B; on Falcon-40B, it\\nslightly degrades performance on ARC-Easy, and improves it significantly on ARC-Challenge and\\nOpenBookQA. These findings are in line with the observations of Rae et al. (2021) on RACE, and we\\nfind that the e ffect is indeed very significant ( +10-20% absolute gains on Falcon-180B). Combining\\nboth practices never improves performance over using one individually.\\nFor fairness, we seek to match the evaluation setting of other papers and models when comparing to\\ntheir results–so that using one of the trick above doesn’t put us at an unfair advantage for example.\\n•Comparisons with PaLM. Like PaLM (Chowdhery et al., 2022; Anil et al., 2023) we\\nreproduce the evaluation setup of Brown et al. (2020): we do not outline candidates in\\nthe prompt for multiple-choice questions (e.g., ARC, RACE), and use unconditionnal\\nnormalization for OpenBookQA. When candidates are longer than one token, we normalize\\nlogprobabilities by length. We do not significantly tweak prompts, staying close to the ones\\ndetailed by Brown et al. (2020). These results are presented in detail in Section 6.2.\\n•Comparisons with GPT-3.5 /GPT-4. For MMLU, we use the format proposed by the\\nauthors originally (Hendrycks et al., 2020); for ARC, we explicitly outline choices in the\\nprompt, as reported in the GPT-4 paper (OpenAI, 2023a). For HellaSwag and Winogrande,\\nwe do not change the prompt used by Gao et al. (2021). Results are reported in Section 6.3.\\n•State-of-the-art comparisons. Since these are the widest comparisons, they are likely\\nthe ones with the largest variation in practices. We use unconditional normalization on\\nOpenBookQA, and outline answers explicitely on ARC, OpenBookQA, and RACE. We\\nlightly tweak the wording of prompts, but stay close to the ones of Brown et al. (2020), as\\nimplemented in Gao et al. (2021). These results are reported in Section 6.4.\\n32', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 31}), Document(page_content='•Comparisons with the EleutherAI Evaluation Harness. We strictly use Gao et al. (2021),\\nand do not make any change to the prompt formulation or logprobability calculations for\\nthese. We only compare against other papers that adopt the same practice. This is our fairest\\nset-up, for which direct one-to-one comparisons are possible and for which we didn’t had to\\nguess the set-up used by other models. These results are reported in Section 6.5.\\nFinally, for all set-ups, we provide the prompts used in Appendix G for reproducibility.\\n6.2 Comparisons with PaLM on a natural language tasks aggregate\\nSet-up. We compare with PaLM (Chowdhery et al., 2022) and PaLM-2 (Anil et al., 2023) in the\\n1-shot NLP task benchmark reported in the PaLM-2 paper. This benchmark broadly reproduces the\\nset of tasks used for GPT-3 (Brown et al., 2020). We note that we are a missing a few of these tasks\\n(e.g., StoryCloze) as we did not implement and validate them in our evaluation framework.\\nResults. We report detailed results in Table 18. We find that when averaging performance across\\ntasks, Falcon-180B recovers 99.5% of the performance of PaLM-2 Large; significantly above the\\n94.8% and 94.4% of PaLM-2 Medium and PaLM. Notably, Falcon-180B even outperforms PaLM-2\\non some classic benchmarks such as HellaSwag, Winogrande, and PIQA. However, Falcon-180B lags\\nbehind on two particular benchmarks: RACE and ANLI. We found RACE to be very sensitive to the\\nformatting of the prompt and samples, which may explain some of the di fference; furthermore, we\\nobserved that ANLI typically exhibits staggered progression throughout training, rather than a smooth\\nincrease (e.g., like HellaSwag)–it’s possible Falcon-180B is just under the next phase transition that\\nwould allow it to catch-up to the range of performance of PaLM-2 Large.\\nTable 18: Falcon-180B matches the performance of PaLM-2 Large on most tasks, with the\\nnotable exception of ANLI and RACE for which it performs closer to PaLM-2 Medium. We\\nfound these tasks to be very sensitive to the prompt format used, in particular RACE, and suspect that\\nfurther improvements could be unlocked with additional tweaking–however, for fairness, we stay\\nclose to the prompts proposed by Brown et al. (2020). Overall, we find that Falcon-180B recovers\\n99.5% of the performance of PaLM-2 Large, significantly above the 94.8% of PaLM-2 Medium.\\nTask Subtask PaLM PaLM-2 Falcon\\nS M L 180B\\nWebQuestions (EM) 22,6 21,8 26,9 28,2 31,9\\nHellaSwag 83,6 82,0 84,0 86,8 87,5\\nLAMBADA 81,8 80,7 83,7 86,9 84,4\\nWSC 86,3 84,6 88,1 86,9 87,5\\nWinogrande 83,7 77,9 79,2 83,0 85,1\\nRACE Hard 52,1 53,3 57,2 62,3 56,7\\nPIQA 83,9 82,2 83,2 85,0 86,1\\nARC Challenge 60,1 59,6 64,9 69,2 67,8\\nEasy 85,0 85,6 88,0 89,7 88,8\\nOverall 72,6 72,6 76,5 79,5 78,3\\nOpenBookQA 53,6 57,4 56,2 58,5 64,2\\nBoolQ 88,7 88,1 88,6 90,9 89,0\\nCB 83,9 82,1 80,4 87,5 89,3\\nCOPA 91,0 89,0 90,0 96,0 96,0\\nRTE 78,7 78,7 81,8 79,3 80,1\\nWiC 63,2 50,6 52,0 66,8 66,1\\nReCORD 92,8 92,1 92,4 93,8 93,2\\nANLI R1 52,6 53,1 58,1 73,1 60,5\\nR2 48,7 48,8 49,5 63,4 55,5\\nR3 52,3 53,2 54,5 67,1 56,8\\nTask average 73,1 71,6 73,4 77,5 77,1\\nFraction of PaLM-2 L 94,4 92,4 94,8 99,5\\n33', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 32}), Document(page_content='Table 19: Falcon-180B delivers downstream performance between GPT-3.5 and GPT-4. Falcon-\\n180B performs well on commonsense tasks (HellaSwag and Winogrande), where it is well ahead of\\nGPT-3.5. For multiple choice question answering (ARC and MMLU), Falcon-180B performs above\\nGPT-3.5 but not as significantly so.∗we report 2-shot, not 25-shot performance on ARC Challenge.\\nGPT-3.5 GPT-4 Falcon-180B\\nHellaSwag (10-shot) 85.5 95.3 89.0\\nWinogrande (5-shot) 81.6 87.5 87.1\\nARC Challenge (25-shot) 85.2 96.3 87.8∗\\nMMLU (5-shot) 70.0 86.5 70.6\\nOverall, these are strong scores, exhibiting the robustness of the Falcon recipe. Beside dataset\\ncomposition and architectural tweaks, we note two di fferences between PaLM-2 Large and Falcon-\\n180B: (1) PaLM-2 L was supposedly trained with a larger compute budget, with twice the parameters\\nand a similar amount of tokens as Falcon-180B (see Appendix E); (2) PaLM-2 models used a mixture\\nof objectives (Tay et al., 2022a), which has been reported to enhance downstream task performance.\\nFurther a posteriori adaptation of Falcon-180B (e.g., with the PaLM-U recipe (Tay et al., 2022b))\\ncould help recover more of the performance of PaLM-2 L with Falcon-180B as a base model.\\n6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks\\nSet-up. We compare with GPT-3.5 and the GPT-4 as reported in GPT-4 paper (OpenAI, 2023a),\\nfocusing on natural language tasks: MMLU, HellaSwag, Winogrande, and ARC. We use the same\\nnumber of shots as proposed, except for ARC, for which we report 2-shot instead of 25-shot accuracy.\\nResults. Results are in Table 19. We find that Falcon-180B systematically performs above GPT-3.5,\\nbut below GPT-4 on all tasks. Notably, Falcon-180B achieves strong performance on commonsense\\ntasks: on HellaSwag, it is close to midway between GPT-3.5 and GPT-4, while on Winogrande, it\\nnearly matches the performance of GPT-4. We find the performance of Falcon-180B on multiple\\nchoice question answering tasks to be closer to GPT-3.5, albeit always higher. We note that GPT-4\\nwas allegedly trained with 4-5x more pretraining compute than Falcon-180B (see Appendix E), which\\nlikely contributes to most of the di fference in performance between the two models.\\n6.4 State-of-the-art comparisons on common sense, question answering, and code tasks\\nSet-up. In this section, we compare the Falcon series with other models on commonsense, question\\nanswering, and code tasks. We compare with models that have successively defined the state-of-\\nthe-art \"before\" PaLM-2 Large and GPT-4: GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021),\\nChinchilla (Ho ffmann et al., 2022), MT-NLG (Smith et al., 2022), PaLM (Chowdhery et al., 2022),\\nLLaMA-2 (Touvron et al., 2023b), and Inflection-1 (Inflection, 2023). For commonsense tasks, we\\ninclude PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al.,\\n2019), BoolQ (Clark et al., 2019), and LAMBADA (Paperno et al., 2016)–only for BoolQ does our\\nprompt di ffer slightly from the default one of the EleutherAI Evaluation Harness (Gao et al., 2021).\\nFor question answering datasets, we include ARC (Clark et al., 2018), OpenBookQA (Mihaylov et al.,\\n2018), and MMLU (Hendrycks et al., 2020). To enable fair comparisons, we evaluate ARC without\\noutlining choices (see Section 6.1). For OpenBookQA, we outline choices, as we find performance\\nto be flat otherwise; finally, for MMLU, we use the canonical setup from Hendrycks et al. (2020).\\nFor all these tasks we take results for other models from the papers outlined above. For code, we\\nreproduce the set-up of the BigCode Models Leaderboard for HumanEval in Python, and include\\nfurther code-specialized models such as Codex (Chen et al., 2021), StarCoder (Li et al., 2023a),\\nand Code LLaMA (Rozière et al., 2023). As we focus on comparisons with the Falcon models\\nimmediately after pretraining, we do not consider instruct variants of the models above.\\nCommonsense. We report results in Table 20. With the exception of BoolQ, Falcon-180B improves\\nsignificantly over state-of-the-art models across all tasks. Broadly speaking, we find that Falcon-40B\\nis slightly under LLaMA-2 34B, which we attribute to smaller pretraining compute: 2,800PF-days\\nfor Falcon-40B against 4,700PF-days for LLaMA-2 34B (nearly 70% more). Falcon-7B also slightly\\nunderperforms LLaMA-2 7B; this time the di fference in compute is smaller (730PF-days against\\n34', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 33}), Document(page_content='Table 20: Outside of PaLM-2 Large and GPT-4, Falcon-180B significantly improves other state-\\nof-the-art models such as LLaMA-2 or Inflection-1 on commonsense tasks . Falcon-40B performs\\nslightly under LLaMA-2 34B, because of a significantly smaller compute budget (2,800PF-days\\nagainst 4,700PF-days, 70% more for LLaMA-2). We note the exceptional performance of Inflection-1\\non BoolQ; conversely, we note that, despite our best prompt engineering e fforts, we were unable\\nto reproduce the performance reported by the LLaMA-2 paper on BoolQ for the Falcon series: the\\nresults we report can likely be improved. Bold for best, underline for second-best.\\nPIQA HellaSwag Winogrande BoolQ LAMBADA\\n(10-shot) (5-shot)\\nGPT-3 81,0 78,9 70,2 60,5 76,2\\nGopher 81,8 79,2 70,1 79,4 74,5\\nChinchilla 81,8 80,8 74,9 83,7 77,4\\nMT-NLG 82,0 80,2 73 78,2 76,6\\nPaLM 82,3 83,4 81,1 88,0 77,9\\nLLaMA-2 7B 78,8 77,2 78,6 69,2 77,4\\n13B 80,5 80,7 82,1 72,8 81,7\\n34B 81,9 83,3 76,7 83,7\\n70B 82,8 85,3 87,3 80,2 85,0\\nInflection-1 84,2 84,3 85,8 83,3 89,7 78,5\\nFalcon 7B 80,3 76,3 78,1 67,2 72,6 73,8 74,9\\n40B 83,0 82,7 85,3 76,0 81,8 81,9 77,3\\n180B 84,9 85,9 89,0 80,3 87,1 87,8 79,8\\n970PF-days, 30% more), but we suspect that multiquery with a single head of dimension 64 is a very\\naggressive configuration for Falcon-7B. We find that LLaMA-2 and Inflection-1 achieve relatively\\nsimilar performance, with Inflection-1 perhaps ahead thanks to exceptional performance on BoolQ.\\nQuestion answering. In Table 21, we find again that Falcon-180B strongly outperforms other\\nmodels from the state-of-art on question answering tasks. We do note that the Falcon series seems to\\nunderperform slightly (comparatively to other tasks) on MMLU: we believe this may be attributed\\nto the large prevalence of web data in our pretraining dataset, compared to more technical sources\\nwhich may be more immediately relevant to the style and content of questions found in MMLU.\\nCode. We report results on HumanEval in Table 22. We find that Falcon-180B performs best amongst\\nmodels focusing on natural language, with performance only matched by Inflection-1. In fact, despite\\nbeing trained on only 3% code, Falcon-180B nearly matches the performance of both PaLM-Coder\\nand PaLM-2 S∗, two models which have undergone dedicated code specialization following their\\npretraining. This is an encouraging result for the development of a Falcon-Coder specialization.\\nTable 21: Falcon-180B improves significantly over GPT-3, PaLM, and LLaMA-2 on question\\nanswering datasets, while Falcon-40B performs in-line with LLaMA-2 34B.∗: for Falcon-7B on\\nOpenBookQA, we report accuracy without outlining candidates, as performance is otherwise close to\\nrandom (see Table 17 for details). Bold for best, underline for second-best.\\nARC-Challenge ARC-Easy OpenBookQA MMLU\\nGPT-3 51,4 68,8 57,6\\nPaLM 53,0 76,6 53,4 69,3\\nLLaMA-2 7B 45,9 75,2 58,6 45,3\\n13B 49,4 77,3 57,0 54,8\\n34B 54,5 79,4 58,2 62,6\\n70B 57,4 80,2 60,2 68,9\\nFalcon 7B 44,5 73,6 44,6∗28,0\\n40B 56,7 81,2 61,2 57,0\\n180B 63,7 84,7 76,4 70,6\\n35', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 34}), Document(page_content='Table 22: Falcon-180B outperforms all other predominantly English language models on\\nHumanEval. Morever, despite being trained on only 3% code, it nearly matches the performance\\nof PaLM-Coder and PaLM-2 S∗, both having undergone code specialization.†: the GPT-3.5 and\\nGPT-4 models are somewhat unique in being pretrained on a large fraction of code on top of natural\\nlanguage (OpenAI, 2023b). Bold for best, underline for second-best (per specialization), pass@1.\\nSpecialized for code? HumanEval\\nPaLM 26,2\\nLLaMA-2 7B 12,2\\n13B 20,1\\n34B 22,6\\n70B 30,5\\nInflection-1 35,4\\nFalcon 180B 35,4\\nCodex cushman-001 ✓ 33,5\\ndavinci-002 ✓ 45,9\\nStarCoder ✓ 30,4\\nPaLM-Coder ✓ 35,9\\nPaLM-2 S∗✓ 37,6\\nGPT-3.5 ✓†48,1\\nGPT-4 ✓†67.0\\nCode LLaMA 7B ✓ 33,5\\n13B ✓ 36,0\\n34B ✓ 48,8\\n6.5 Comparison with other models using the EleutherAI Evaluation Harness\\nSet-up. For this final comparison, we only consider models which have had reports of evaluations\\nperformed with the EleutherAI Evaluation Harness (Gao et al., 2021). These results are the most\\ndirectly comparable, as the preprocessing of samples, the prompt, and the calculations of the metrics\\nare identical. We report results for GPT-3 evaluated through the API by the BigScience group,\\nFairSeq (Artetxe et al., 2021), GPT-Neo-1.3B (Black et al., 2021), GPT-J (Wang and Komatsuzaki,\\n2021), GPT-NeoX-20B (Black et al., 2022), OPT (Zhang et al., 2022), Pythia (Biderman et al.,\\n2023), CerebrasGPT (Dey et al., 2023), Aleph Alpha (Aleph Alpha, 2023), and BLOOM (Scao et al.,\\n2022a). We average performance on HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al.,\\n2016), Winogrande (Sakaguchi et al., 2019), PIQA (Bisk et al., 2020), ARC (Clark et al., 2018), and\\nOpenBookQA (Mihaylov et al., 2018)–this was the widest set of tasks we could assemble based\\non di fferent reporting practices across papers. We note that these models mostly span a smaller\\ncompute range (up to a few thousands PF-days at most), and with performance starkly below other\\nstate-of-the-art models on with which we have compared before. We report results both for the Falcon\\nseries presented in this paper, and for the smaller-scale Falcon-RefinedWeb models trained in Penedo\\net al. (2023), which used only the RefinedWeb dataset for performance validation.\\nResults. We present results in Fig. 14. We find that, across scales, the Falcon series significantly\\nimproves against other models in this set of comparisons. Notably, Falcon-40B outperforms GPT-3\\n175B, despite being trained with a smaller compute budget. In fact, even Falcon-7B approaches the\\nperformance of GPT-3 175B: we believe that with longer training and a less aggressive multiquery\\nsetup, it should be possible to match the performance of the original GPT-3 model with 7B parameters\\n(or less). Finally, we also note that the smaller validation models trained on RefinedWeb alone also\\ncompare favorably, reproducing the performance of the GPT-3 models of the same size.\\nTaking a step back, it’s interesting to note some broader trends in this plot. Most older series of\\nmodels achieve very similar performance, with the GPT-3 series as a roofline. The OPT models,\\ndespite underperforming at the smaller sizes, eventually match the performance of GPT-3 175B. Two\\noutsiders stand out: GPT-Neo-1.3B, the first open-source large language model, likely because of\\nissues in its pretraining, and the BLOOM series, likely because of its heavily multingual pretraining\\ndata and conservative use of an additionnal layer norm after the embeddings (Scao et al., 2022b).\\n36', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 35}), Document(page_content='101102103104105\\nCompute [PF-days]455055606570Aggregated zero-shot performance [ eai-agg %]\\nGPT-3∗\\nReﬁnedWebFalcon (ours)\\nOPT\\nBLOOMAleph\\nAlphaAleph Alpha\\nCerebras-GPT\\nFairSeq\\nPythia-Dedup\\nOPT\\nBLOOM\\nGPT-NeoX/J\\nGPT-3 API\\nOurs (RW)\\nOurs (Falcon)Figure 14: The Falcon series strongly improves at all scales against other models which have\\nreported results with the EleutherAI Evaluation Harness. Models trained on RefinedWeb alone\\nalso reproduce the performance of the GPT-3 series, despite not using curated data. Aggregated\\nzero-shot accuracy on HellaSwag, LAMBADA, Winogrande, PIQA, ARC, and OpenBookQA.\\n7 Limitations\\nWe highlight in this section limitations of our findings and of the Falcon series of models itself, both\\nto nuance potential applications of our models, and to highlight further research directions. Broadly\\nspeaking, these limitations stem from two factors: (1) most of the research on the Falcon series\\nwas conducted prior to December 2022, when model training started–in the meanwhile significant\\ndevelopments have occurred in the world of large language models; (2) constraints in compute\\nresources preventing a more exhaustive exploration of some directions.\\n7.1 Limitations of our findings and ablations\\nScale. Our ablations are conducted with 1B and 3B models trained to optimality; although we\\nvalidated the final architecture and datasets with 1B and 7B models trained for 350B tokens, this\\nremains order of magnitude below the compute budget of the actual models in the Falcon series. On the\\nmodel side, as scale increases, other works have noted the emergence of outlier features, which may\\ndrive important dynamics which are not captured by our set-up (Dettmers et al., 2022). On the data\\nside, increasingly large models are known to become increasingly sensitive to memorization (Carlini\\net al., 2022), potentially leading to catastrophic quality degradation (Hernandez et al., 2022).\\nBenchmarks. In our ablations, we focused on measuring zero-shot performance on a limited set\\nof tasks, using only logprobs-based evaluations. Not only can downstream task performance be at\\nodds with actual human preference (see Fig. 12 for our own experience of that issue), but our set\\nof tasks does not capture code or multilingual performance. Evaluating general purpose language\\nmodels is di fficult, and single aggregated metrics often poorly translate the nuances brought forth\\nby modeling or data interventions. Broadly speaking, on our set of tasks, we never experienced a\\nconsistent trend with one intervention systematically improving a specific metrics but not the others\\n(e.g., adding technical data did not consistently uplift performance on SciQ or PubMedQA); however,\\nthis is likely to be a limitation of our small-scale setup, which end-up predominantly measuring\\ngeneral gains. It is likely the performance on popular benchmarks such as MMLU (Hendrycks et al.,\\n2020) or AGIEval (Zhong et al., 2023) could be improved with dedicated in-domain data. We notably\\nbelieve in the value of larger-scale ablations on optimal pretraining data composition, and on better\\nunderstanding quantitative versus qualitative aspects of deduplication (i.e., deduplication on web data\\nmay be unreasonably e ffective because it happens to filter out some of the worst samples).\\nOverall, we view Section 4 as a set of cursory experiments that helped us ground and broadly validate\\nsome decisions for the Falcon series. Future practitioners will likely want to revisit these experiments\\nand results at increased scale, enabling broader benchmarking to better capture performance.\\n37', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 36}), Document(page_content='7.2 Limitations of the Falcon models\\nDecoupling training and inference compute. In the past year, there has been a dramatic explosion\\nin the adoption of large language models, leading to significantly increased downstream use. Under\\nthat paradigm, inference costs can become predominant, shadowing pretraining costs; furthermore,\\nsmaller open-source models are easier for the community to build upon, enabling local deployments\\nand even finetuning. When scaling-up pretraining, additional compute may be either spent towards a\\nlarger model, or towards longer training. While increasing parameter count caries over to inference,\\nincreasing costs over the entire lifecycle of the model, this is not the case for longer pretraining,\\noffering an axis for decoupling training and inference compute. Recent open-source models, such as\\nthe LLaMA series (Touvron et al., 2023a,b), have adopted this practice by eventually training their\\n7-70B parameters models for a fixed 2,000B tokens. While our 7B follows a similar idea, our 40B\\nand 180B models are trained closer to the pretraining optimality suggested by Ho ffmann et al. (2022).\\nThis makes deployment of models in the Falcon series more challenging. At the time, this decision\\nwas partially motivated by constraints over data availability: we elected early on to strictly stick to\\na single epoch of training, and the total stock of RefinedWeb tokens was unknown to us when we\\nstarted training (data processing jobs only concluded in early January). Recent works have however\\nsuggested that up to 4 epochs may lead to minimal degradation in performance, at least for models in\\nthe 10B parameters range (Xue et al., 2023; Muennigho ffet al., 2023). Based on the final size of our\\navailable data (around 5,000B tokens of English and 1,000B tokens of code) we would recommend\\ntraining future models on at least 4,000-5,000B tokens, and skewing towards 10,000-15,000B tokens\\nfor the larger ones. Note that this does pose some challenges during pretraining, as larger models\\nare easier to distribute e fficiently. We also see promise in enabling training and inference compute\\ndecoupling through architecture interventions such as layerwise mixture of experts (MoE), also\\nknown as routed language models (Clark et al., 2022; Fedus et al., 2022). In MoE models, parameters\\nare only sparsely activated, allowing for another 8-16 reduction in inference costs.\\nCode performance and pretraining data. As we did not allow upsampling for pretraining subsets,\\nwe predominantly trained Falcon on web data from RefinedWeb. We also were conservative with the\\nfraction of code in the pretraining, whereas available data could have allowed us to reach 10-30% of\\ncode data in pretraining. For future models, we would recommend making code data significantly\\nmore prevalent, and potentially upsampling sources illustrative of common usecases or data domains\\nof interest for downstream use cases. Code data is particularly promising, because it is widely\\navailable from a single source (i.e., GitHub) and can undergo large-scale processing similar to web\\ndata–deduplication is also extremely e ffective for code data (Kocetkov et al., 2022; Allal et al., 2023;\\nLi et al., 2023a). Furthermore, the GPT-3.5 and GPT-4 models have been seemingly trained on a mix\\nof both natural language and code data (OpenAI, 2023b,a). Such hybrid language /code models are\\nalso likely to be easier to adapt for downstream use cases as chatbots which may make extensive\\nuse of code to answer user questions, or to interface with a variety of tools. We also see potential\\npromise in the use of synthetic data (Gunasekar et al., 2023; Li et al., 2023b); however, many of these\\nmethods are currently closer to distillation than true pretraining, and it is somewhat unclear how they\\nmay be used to bootstrap a new class of larger models beyond downstream adaptation.\\nLonger sequence lengths. All Falcon models are pretrained with a 2,048 sequence length, which\\ncan be limiting for multi-turn chat or code use cases. Thanks to the use of rotary embeddings (Su\\net al., 2021), a posteriori adaptation to longer sequences is possible: kaiokenmdenv (2023); Chen\\net al. (2023) concurrently found that extending the context length with rotary positionnal embeddings\\nwas possible by interpolation and lightweight finetuning. In our own experiments on long context\\nfinetuning, we found long context data to be plentiful in RefinedWeb. Upward of and more than 13%\\nof the tokens come from documents of over 8k tokens, with 1.5% of the tokens in documents of over\\n32k tokens. This approach was further refined for LLaMA-2 Long (Xiong et al., 2023), resulting in a\\nsignificant performance boost on tasks with many shots (e.g., MMLU and ARC in Section 6.3).\\nPretrained only. Although we make lightly tuned versions of the Falcon models available to\\ndemonstrate possibilities as instruct or chat models, our work predominantly concerns the pretrained\\nversions. Prior to deployment of the Falcon models, we strongly recommend for adequate guardrails\\nto be deployed and for bias /harm evaluations specific to the target use case to be systematically\\nundertaken. In particular, we note that reinforcement learning from human feedback may be relevant\\nfor not only making the models more helpful, but also more robust to adversarial queries (Ganguli\\net al., 2022). We also see promise in code adaptation of the Falcon models, similar to PaLM-Coder\\n(Chowdhery et al., 2022) and PaLM-2 S∗(Anil et al., 2023) or Code LLaMA (Rozière et al., 2023).\\n38', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 37}), Document(page_content='8 Conclusion\\nIn this paper, we have introduced and extensively described the Falcon series of pretrained models,\\nwith Falcon-7 /40/180B, trained on up to 3,500B tokens.\\nFirst, we described some of the ablations and experiments we performed to prepare the training\\ndataset and architecture of the Falcon models. We found adequately filtered and deduplicated web\\ndata to be a surprisingly strong baseline; concerns around memorization for larger models (Carlini\\net al., 2022; Hernandez et al., 2022) led us to elect not to upsample any sources, and to predominantly\\ntrain on this web data. On the architecture-side, we found most interventions to have a limited e ffect,\\nand adopted multigroup attention (an extension of multiquery (Shazeer, 2019)) as a way to improve\\ninference scalability by significantly reducing the size of the K,V-cache. For future generations of the\\nFalcon series, we see promise in significantly increasing the fraction of code in the pretraining data,\\nand in training with longer sequence lengths in a staged process (e.g., half of the training up to 8k,\\nand second half up to 16k, with downstream adaptation to 32-256k).\\nThen, we described our implementation of our final strategy for the pretraining of the Falcon series.\\nWe report extensively on our data pipeline in Penedo et al. (2023). We described our approach to\\nconversational masking, and our distributed training strategy for running on cost-e fficient cloud\\ninfrastructure, relying notably on 3D parallelism combined with ZeRO. We also discussed some of\\nour interventions for fast memory e fficient training, such as dedicated FlashAttention (Dao et al.,\\n2022) kernels in Triton (Tillet et al., 2019) and our monolayer strategy. We also discussed some of\\nthe details around setting hyper-parameters and managing runs over multiple thousand GPUs.\\nFinally, we outlined some of the results obtained by the Falcon series on key benchmarks. We found\\nFalcon-180B to near the performance of PaLM-2 Large (Anil et al., 2023), and to end-up in-between\\nGPT-3.5 and GPT-4 (OpenAI, 2023a). Falcon-180B is the best open-source model currently available,\\nand likely one of the best models overall. We note our evaluation predominantly focuses on classic\\nnatural language tasks, and that further work will be required for evaluating human preferences on\\ndownstream versions of Falcon having undergone dedicated finetuning or reinforcement learning.\\nTo foster open research on large language models, and accelerate technological development in this\\nspace, we make the following artefacts public available under permissive open-source licenses:\\n•Falcon-7 /40/180B. We make all models in the Falcon series available, with Falcon-7 /40B\\nunder an Apache 2.0 license and Falcon-180B under a dedicated responsible AI use license.\\nAt time of release, Falcon-180B is the most powerful open large language model available.\\n•A 600B tokens extract of RefinedWeb. We make a 600B tokens extract of our web dataset\\navailable, for use by researchers to study large-scale filtered and deduplicated web data, and\\nfor other practioners to adopt as a standard for high-quality web data. We also open-source\\n1/7B models trained on 350B tokens from this extract.\\n•Detailed research. With this paper and the RefinedWeb paper (Penedo et al., 2023), we\\nhave detailed numerous of our decisions and experiments concerning the Falcon series.\\nWe believe large language models to be a foundational technology for the future of our civilization,\\nand in turn we believe they should be shared responsibly. Widespread exchange of ideas is a staple of\\naccelerated technological and economical progress in our history; in turn, this acceleration uplifts\\nall. By open-sourcing artificial intelligence research and models, we can foster a broader and more\\ndiverse community, and benefit from vibrant collaborative e fforts to improve the safety and reliability\\nof large language models. We hope the Falcon series can be a small step towards this vision.\\n39', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 38}), Document(page_content='References\\nAbadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean,\\nJ., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y ., Jozefowicz,\\nR., Kaiser, L., Kudlur, M., Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C.,\\nSchuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan,\\nV ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y ., and Zheng, X. (2015).\\nTensorFlow: Large-scale machine learning on heterogeneous systems. Software available from\\ntensorflow.org.\\nAbadji, J., Ortiz Suarez, P., Romary, L., and Sagot, B. (2022). Towards a Cleaner Document-Oriented\\nMultilingual Crawled Corpus. arXiv e-prints , page arXiv:2201.06642.\\nAdiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R., Yang, Z., Kulshreshtha,\\nA., Nemade, G., Lu, Y ., et al. (2020). Towards a human-like open-domain chatbot. arXiv preprint\\narXiv:2001.09977 .\\nAghajanyan, A., Yu, L., Conneau, A., Hsu, W.-N., Hambardzumyan, K., Zhang, S., Roller, S., Goyal,\\nN., Levy, O., and Zettlemoyer, L. (2023). Scaling laws for generative mixed-modal language\\nmodels. arXiv preprint arXiv:2301.03728 .\\nAinslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y ., Lebrón, F., and Sanghai, S. (2023). Gqa:\\nTraining generalized multi-query transformer models from multi-head checkpoints. arXiv preprint\\narXiv:2305.13245 .\\nAleph Alpha (2023). Luminous: performance benchmarks. arXiv preprint arXiv:1810.12885 .\\nAllal, L. B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C. M., Muennigho ff, N., Mishra, M.,\\nGu, A., Dey, M., et al. (2023). Santacoder: don’t reach for the stars! In Deep Learning for Code\\n(DL4C) Workshop .\\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey,\\nP., Chen, Z., et al. (2023). Palm 2 technical report. arXiv preprint arXiv:2305.10403 .\\nArtetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M., Shleifer, S., Lin, X. V ., Du, J., Iyer, S.,\\nPasunuru, R., et al. (2021). E fficient large scale language modeling with mixtures of experts. arXiv\\npreprint arXiv:2112.10684 .\\nBahdanau, D., Cho, K., and Bengio, Y . (2014). Neural machine translation by jointly learning to\\nalign and translate. arXiv preprint arXiv:1409.0473 .\\nBai, Y ., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D.,\\nHenighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning\\nfrom human feedback. arXiv preprint arXiv:2204.05862 .\\nBai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini,\\nA., McKinnon, C., et al. (2022b). Constitutional ai: Harmlessness from ai feedback. arXiv preprint\\narXiv:2212.08073 .\\nBaumgartner, J., Zannettou, S., Keegan, B., Squire, M., and Blackburn, J. (2020). The pushshift\\nreddit dataset.\\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M. (2022).\\nEfficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255 .\\nBeltagy, I., Lo, K., and Cohan, A. (2019). Scibert: A pretrained language model for scientific text. In\\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages\\n3615–3620.\\nBiderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M. A.,\\nPurohit, S., Prashanth, U. S., Ra ff, E., et al. (2023). Pythia: A suite for analyzing large language\\nmodels across training and scaling. In International Conference on Machine Learning , pages\\n2397–2430. PMLR.\\n40', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 39}), Document(page_content='Bisk, Y ., Zellers, R., Bras, R. L., Gao, J., and Choi, Y . (2020). Piqa: Reasoning about physical\\ncommonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence .\\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell,\\nK., Phang, J., et al. (2022). Gpt-neox-20b: An open-source autoregressive language model. In\\nProceedings of BigScience Episode# 5–Workshop on Challenges &Perspectives in Creating Large\\nLanguage Models , pages 95–136.\\nBlack, S., Gao, L., Wang, P., Leahy, C., and Biderman, S. (2021). GPT-Neo: Large Scale Autoregres-\\nsive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these\\nmetadata.\\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke,\\nA., VanderPlas, J., Wanderman-Milne, S., et al. (2021). Jax: Autograd and xla. Astrophysics\\nSource Code Library , pages ascl–2111.\\nBroder, A. Z. (1997). On the resemblance and containment of documents. In Proceedings. Compres-\\nsion and Complexity of Sequences 1997 , pages 21–29. IEEE.\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\\nP., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child, R.,\\nRamesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,\\nChess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020).\\nLanguage models are few-shot learners. In Advances in Neural Information Processing Systems ,\\nvolume 33, pages 1877–1901.\\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F., and Zhang, C. (2022). Quantifying\\nmemorization across neural language models. In The Eleventh International Conference on\\nLearning Representations .\\nChelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. (2013). One\\nbillion word benchmark for measuring progress in statistical language modeling. arXiv preprint\\narXiv:1312.3005 .\\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y ., Joseph,\\nN., Brockman, G., et al. (2021). Evaluating large language models trained on code. arXiv preprint\\narXiv:2107.03374 .\\nChen, S., Wong, S., Chen, L., and Tian, Y . (2023). Extending context window of large language\\nmodels via positional interpolation. arXiv preprint arXiv:2306.15595 .\\nChiang, C.-H. and Lee, H.-y. (2023). Can large language models be an alternative to human\\nevaluations? arXiv preprint arXiv:2305.01937 .\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y ., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y .,\\nGonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot impressing\\ngpt-4 with 90%* chatgpt quality.\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\\nH. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways.\\narXiv preprint arXiv:2204.02311 .\\nClark, A., De Las Casas, D., Guy, A., Mensch, A., Paganini, M., Ho ffmann, J., Damoc, B., Hechtman,\\nB., Cai, T., Borgeaud, S., et al. (2022). Unified scaling laws for routed language models. In\\nInternational Conference on Machine Learning , pages 4057–4086. PMLR.\\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. (2019). Boolq:\\nExploring the surprising di fficulty of natural yes /no questions. In NAACL .\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. (2018).\\nThink you have solved question answering? try arc, the AI2 reasoning challenge. CoRR ,\\nabs/1803.05457.\\n41', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 40}), Document(page_content='Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,\\nJ., Nakano, R., et al. (2021). Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 .\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V ., Wenzek, G., Guzmán, F., Grave, É., Ott, M.,\\nZettlemoyer, L., and Stoyanov, V . (2020). Unsupervised cross-lingual representation learning at\\nscale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics ,\\npages 8440–8451.\\nCosta-jussà, M. R., Cross, J., Çelebi, O., Elbayad, M., Heafield, K., He ffernan, K., Kalbassi, E., Lam,\\nJ., Licht, D., Maillard, J., et al. (2022). No language left behind: Scaling human-centered machine\\ntranslation. arXiv preprint arXiv:2207.04672 .\\nDao, T. (2023). Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\\npreprint arXiv:2307.08691 .\\nDao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. (2022). Flashattention: Fast and memory-\\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,\\n35:16344–16359.\\nDettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer, L. (2022). Llm.int8(): 8-bit matrix multipli-\\ncation for transformers at scale. Advances in Neural Information Processing Systems , 35:30318–\\n30332.\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805 .\\nDey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness, J., et al. (2023).\\nCerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster.\\narXiv preprint arXiv:2304.03208 .\\nDinan, E., Yaida, S., and Zhang, S. (2023). E ffective theory of transformers at initialization. arXiv\\npreprint arXiv:2304.02034 .\\nDodge, J., Sap, M., Marasovi ´c, A., Agnew, W., Ilharco, G., Groeneveld, D., Mitchell, M., and Gardner,\\nM. (2021). Documenting large webtext corpora: A case study on the colossal clean crawled corpus.\\nInProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing ,\\npages 1286–1305.\\nDubois, Y ., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P., and Hashimoto,\\nT. B. (2023). Alpacafarm: A simulation framework for methods that learn from human feedback.\\narXiv preprint arXiv:2305.14387 .\\nEberhard, D. M., Simons, G. F., and Fennig, C. D. (2023). Ethnologue: Languages of the World . SIL\\nInternational, Dallas, TX, USA, twenty-sixth edition.\\nFedus, W., Zoph, B., and Shazeer, N. (2022). Switch transformers: Scaling to trillion parameter\\nmodels with simple and e fficient sparsity. The Journal of Machine Learning Research , 23(1):5232–\\n5270.\\nFlamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L.,\\nCorenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T., Janati, H., Rakotomamonjy,\\nA., Redko, I., Rolet, A., Schutz, A., Seguy, V ., Sutherland, D. J., Tavenard, R., Tong, A., and Vayer,\\nT. (2021). Pot: Python optimal transport. Journal of Machine Learning Research , 22(78):1–8.\\nFourrier, C., Habib, N., Launay, J., and Wolf, T. (2023). What’s going on with the open llm\\nleaderboard? \"https: //huggingface.co /blog/evaluating-mmlu-leaderboard\".\\nFu, D. Y ., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A., and Re, C. (2022). Hungry hungry hippos:\\nTowards language modeling with state space models. In The Eleventh International Conference on\\nLearning Representations .\\nGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y ., Kadavath, S., Mann, B., Perez, E., Schiefer,\\nN., Ndousse, K., et al. (2022). Red teaming language models to reduce harms: Methods, scaling\\nbehaviors, and lessons learned. arXiv preprint arXiv:2209.07858 .\\n42', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 41}), Document(page_content='Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A.,\\nNabeshima, N., Presser, S., and Leahy, C. (2020). The Pile: an 800GB dataset of diverse text for\\nlanguage modeling. arXiv preprint arXiv:2101.00027 .\\nGao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K.,\\nMuennigho ff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.\\n(2021). A framework for few-shot language model evaluation.\\nGordon, A., Kozareva, Z., and Roemmele, M. (2012). SemEval-2012 task 7: Choice of plausible\\nalternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First Joint\\nConference on Lexical and Computational Semantics – Volume 1: Proceedings of the main\\nconference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop\\non Semantic Evaluation (SemEval 2012) , pages 394–398, Montréal, Canada. Association for\\nComputational Linguistics.\\nGunasekar, S., Zhang, Y ., Aneja, J., Mendes, C. C. T., Del Giorno, A., Gopi, S., Javaheripi, M.,\\nKauffmann, P., de Rosa, G., Saarikivi, O., et al. (2023). Textbooks are all you need. arXiv preprint\\narXiv:2306.11644 .\\nHamborg, F., Meuschke, N., Breitinger, C., and Gipp, B. (2017). news-please: A generic news\\ncrawler and extractor. In Proceedings of the 15th International Symposium of Information Science ,\\npages 218–223.\\nHaviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. (2022). Transformer language models\\nwithout positional encodings still learn positional information. In Findings of the Association for\\nComputational Linguistics: EMNLP 2022 , pages 1382–1390.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. (2020).\\nMeasuring massive multitask language understanding. In International Conference on Learning\\nRepresentations .\\nHernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-\\nDodds, Z., Henighan, T., Hume, T., et al. (2022). Scaling laws and interpretability of learning from\\nrepeated data. arXiv preprint arXiv:2205.10487 .\\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A.,\\nYang, Y ., and Zhou, Y . (2017). Deep learning scaling is predictable, empirically. arXiv preprint\\narXiv:1712.00409 .\\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L.,\\nHendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556 .\\nHooker, S. (2021). The hardware lottery. Communications of the ACM , 64(12):58–65.\\nHoward, J. and Ruder, S. (2018). Universal language model fine-tuning for text classification. In\\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\\n1: Long Papers) , pages 328–339.\\nInan, H., Khosravi, K., and Socher, R. (2016). Tying word vectors and word classifiers: A loss\\nframework for language modeling. arXiv preprint arXiv:1611.01462 .\\nInflection (2023). Inflection 1.\\nJohannes Welbl, Nelson F. Liu, M. G. (2017). Crowdsourcing multiple choice science questions.\\nJozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., and Wu, Y . (2016). Exploring the limits of\\nlanguage modeling. arXiv preprint arXiv:1602.02410 .\\nkaiokenmdenv (2023). Extending context is hard. . . but not impossible. Accessed: 2023-10-02.\\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,\\nA., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint\\narXiv:2001.08361 .\\n43', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 42}), Document(page_content='Kenton, J. D. M.-W. C. and Toutanova, L. K. (2019). Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. In Proceedings of NAACL-HLT , pages 4171–4186.\\nKocetkov, D., Li, R., Jia, L., Mou, C., Jernite, Y ., Mitchell, M., Ferrandis, C. M., Hughes, S., Wolf,\\nT., Bahdanau, D., et al. (2022). The stack: 3 tb of permissively licensed source code. Transactions\\non Machine Learning Research .\\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A. A.,\\nSubramani, N., Sokolov, A., Sikasote, C., et al. (2022). Quality at a glance: An audit of web-\\ncrawled multilingual datasets. Transactions of the Association for Computational Linguistics ,\\n10:50–72.\\nLai, G., Xie, Q., Liu, H., Yang, Y ., and Hovy, E. (2017). Race: Large-scale reading comprehension\\ndataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in\\nNatural Language Processing , pages 785–794.\\nLaurençon, H., Saulnier, L., Wang, T., Akiki, C., del Moral, A. V ., Scao, T. L., Werra, L. V ., Mou, C.,\\nPonferrada, E. G., Nguyen, H., Frohberg, J., Šaško, M., Lhoest, Q., McMillan-Major, A., Dupont,\\nG., Biderman, S., Rogers, A., allal, L. B., Toni, F. D., Pistilli, G., Nguyen, O., Nikpoor, S., Masoud,\\nM., Colombo, P., de la Rosa, J., Villegas, P., Thrush, T., Longpre, S., Nagel, S., Weber, L., Muñoz,\\nM. R., Zhu, J., Strien, D. V ., Alyafeai, Z., Almubarak, K., Chien, V . M., Gonzalez-Dios, I., Soroa,\\nA., Lo, K., Dey, M., Suarez, P. O., Gokaslan, A., Bose, S., Adelani, D. I., Phan, L., Tran, H., Yu,\\nI., Pai, S., Chim, J., Lepercq, V ., Ilic, S., Mitchell, M., Luccioni, S., and Jernite, Y . (2022). The\\nbigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In Thirty-sixth Conference\\non Neural Information Processing Systems Datasets and Benchmarks Track .\\nLe Scao, T., Wang, T., Hesslow, D., Bekman, S., Bari, M. S., Biderman, S., Elsahar, H., Muennigho ff,\\nN., Phang, J., Press, O., et al. (2022). What language model to train if you have one million\\ngpu hours? In Findings of the Association for Computational Linguistics: EMNLP 2022 , pages\\n765–782.\\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N. (2022).\\nDeduplicating training data makes language models better. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 8424–\\n8445.\\nLevine, Y ., Wies, N., Sharir, O., Bata, H., and Shashua, A. (2020). Limits to depth e fficiencies of\\nself-attention. Advances in Neural Information Processing Systems , 33:22640–22651.\\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V ., Slone, A.,\\nAnil, C., Schlag, I., Gutman-Solo, T., Wu, Y ., Neyshabur, B., Gur-Ari, G., and Misra, V . (2022).\\nSolving quantitative reasoning problems with language models.\\nLi, R., Allal, L. B., Zi, Y ., Muennigho ff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J.,\\nChim, J., et al. (2023a). Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161 .\\nLi, S., Xue, F., Li, Y ., and You, Y . (2021). Sequence parallelism: Making 4d parallelism possible.\\narXiv preprint arXiv:2105.13120 .\\nLi, Y ., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., and Lee, Y . T. (2023b). Textbooks are\\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 .\\nLiang, D., Gonen, H., Mao, Y ., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa,\\nM. (2023). Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models.\\narXiv preprint arXiv:2301.10472 .\\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y ., Narayanan,\\nD., Wu, Y ., Kumar, A., et al. (2022). Holistic evaluation of language models. arXiv preprint\\narXiv:2211.09110 .\\nLieber, O., Sharir, O., Lenz, B., and Shoham, Y . (2021). Jurassic-1: Technical details and evaluation.\\nTechnical report, AI21 Labs.\\n44', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 43}), Document(page_content='Lin, X. V ., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S.,\\nDu, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V ., O’Horo, B., Wang, J., Zettlemoyer,\\nL., Kozareva, Z., Diab, M., Stoyanov, V ., and Li, X. (2021). Few-shot learning with multilingual\\nlanguage models. ArXiv , abs/2112.10668.\\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y ., Zhou, D., Le, Q. V ., Zoph, B., Wei,\\nJ., et al. (2023). The flan collection: Designing data and methods for e ffective instruction tuning.\\narXiv preprint arXiv:2301.13688 .\\nLoshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint\\narXiv:1711.05101 .\\nLoshchilov, I. and Hutter, F. (2018). Decoupled weight decay regularization. In International\\nConference on Learning Representations .\\nLuo, S., Li, S., Zheng, S., Liu, T.-Y ., Wang, L., and He, D. (2022). Your transformer may not be as\\npowerful as you expect. arXiv preprint arXiv:2205.13401 .\\nLuo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D.\\n(2023). Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint\\narXiv:2306.08568 .\\nMadaan, A., Zhou, S., Alon, U., Yang, Y ., and Neubig, G. (2022). Language models of code are\\nfew-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing , pages 1384–1403.\\nManber, U. and Myers, G. (1993). Su ffix arrays: a new method for on-line string searches. Journal\\non Computing , 22(5):935–948.\\nMémoli, F. (2011). Gromov–wasserstein distances and the metric approach to object matching.\\nFoundations of computational mathematics , 11:417–487.\\nMihaylov, T., Clark, P., Khot, T., and Sabharwal, A. (2018). Can a suit of armor conduct electricity?\\na new dataset for open book question answering. In EMNLP .\\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). E fficient estimation of word representations\\nin vector space. arXiv preprint arXiv:1301.3781 .\\nMitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E., Raji, I. D.,\\nand Gebru, T. (2019). Model cards for model reporting. In Proceedings of the conference on\\nfairness, accountability, and transparency , pages 220–229.\\nMosaicML (2023). Introducing mpt-30b: Raising the bar for open-source foundation models.\\nAccessed: 2023-06-22.\\nMuennigho ff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., and\\nRaffel, C. (2023). Scaling data-constrained language models. arXiv preprint arXiv:2305.16264 .\\nNarayanan, D., Phanishayee, A., Shi, K., Chen, X., and Zaharia, M. (2021a). Memory-e fficient\\npipeline-parallel dnn training. In International Conference on Machine Learning , pages 7937–7947.\\nPMLR.\\nNarayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V ., Vainbrand, D.,\\nKashinkunti, P., Bernauer, J., Catanzaro, B., et al. (2021b). E fficient large-scale language model\\ntraining on gpu clusters using megatron-lm. In Proceedings of the International Conference for\\nHigh Performance Computing, Networking, Storage and Analysis , pages 1–15.\\nOpenAI (2023a). Gpt-4 technical report. arXiv , pages 2303–08774.\\nOpenAI (2023b). Model index for researchers. Accessed: 2023-09-26.\\nOrtiz Suárez, P. J., Sagot, B., and Romary, L. (2019). Asynchronous pipelines for processing huge\\ncorpora on medium to low resource infrastructures. Proceedings of the Workshop on Challenges\\nin the Management of Large Corpora (CMLC-7) 2019. Cardi ff, 22nd July 2019, pages 9 – 16,\\nMannheim. Leibniz-Institut für Deutsche Sprache.\\n45', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 44}), Document(page_content='Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,\\nSlama, K., Ray, A., et al. (2022). Training language models to follow instructions with human\\nfeedback. Advances in Neural Information Processing Systems , 35:27730–27744.\\nPaperno, D., Kruszewski, G., Lazaridou, A., Pham, N. Q., Bernardi, R., Pezzelle, S., Baroni, M.,\\nBoleda, G., and Fernández, R. (2016). The LAMBADA dataset: Word prediction requiring a broad\\ndiscourse context. In Proceedings of the 54th Annual Meeting of the Association for Computa-\\ntional Linguistics (Volume 1: Long Papers) , pages 1525–1534, Berlin, Germany. Association for\\nComputational Linguistics.\\nParesh, D. (2023). Stack overflow will charge ai giants for training data.\\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga,\\nL., and Lerer, A. (2017). Automatic di fferentiation in pytorch. In NIPS-W .\\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B.,\\nAlmazrouei, E., and Launay, J. (2023). The RefinedWeb dataset for Falcon LLM: outperforming\\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 .\\nPennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global vectors for word representation.\\nInProceedings of the 2014 conference on empirical methods in natural language processing\\n(EMNLP) , pages 1532–1543.\\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L.\\n(2018). Deep contextualized word representations. In Proceedings of the 2018 Conference of the\\nNorth American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, Volume 1 (Long Papers) , pages 2227–2237, New Orleans, Louisiana. Association\\nfor Computational Linguistics.\\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and\\nDean, J. (2023). E fficiently scaling transformer inference. Proceedings of Machine Learning and\\nSystems , 5.\\nPress, O., Smith, N., and Lewis, M. (2022). Train short, test long: Attention with linear biases enables\\ninput length extrapolation. In International Conference on Learning Representations .\\nPress, O. and Wolf, L. (2016). Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 .\\nRabe, M. N. and Staats, C. (2021). Self-attention does not need o(n2)memory. arXiv preprint\\narXiv:2112.05682 .\\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language understand-\\ning by generative pre-training. OpenAI Blog .\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models\\nare unsupervised multitask learners. OpenAI blog , 1(8):9.\\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Ho ffmann, J., Song, F., Aslanides, J., Henderson, S.,\\nRing, R., Young, S., et al. (2021). Scaling language models: Methods, analysis & insights from\\ntraining gopher. arXiv preprint arXiv:2112.11446 .\\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. (2019). Compressive transformers\\nfor long-range sequence modelling.\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu, P. J.\\n(2019). Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR ,\\nabs/1910.10683.\\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . (2020). Zero: Memory optimizations toward\\ntraining trillion parameter models. In SC20: International Conference for High Performance\\nComputing, Networking, Storage and Analysis , pages 1–16. IEEE.\\n46', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 45}), Document(page_content='Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B.,\\nGaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J.,\\nGoodman, S., Soares, L. B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J.,\\nGarcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J. H., Lee, S., Garrette, D., Lee-Thorp, J., Ra ffel,\\nC., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M.,\\nSaeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gesmundo, A. (2022). Scaling up models\\nand data with t5xandseqio .arXiv preprint arXiv:2203.17189 .\\nRozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y ., Liu, J., Remez, T., Rapin,\\nJ., et al. (2023). Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 .\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . (2019). Winogrande: An adversarial\\nwinograd schema challenge at scale. arXiv preprint arXiv:1907.10641 .\\nSanh, V ., Webson, A., Ra ffel, C., Bach, S. H., Sutawika, L. A., Alyafeai, Z., Cha ffin, A., Stiegler,\\nA., Scao, T. L., Raja, A., Dey, M., BARI, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla,\\nE., Kim, T., Chhablani, G., Nayak, N. V ., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica,\\nM., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A.,\\nSantilli, A., Févry, T., Fries, J. A., Teehan, R., Biderman, S. R., Gao, L., Bers, T. G. O., Wolf, T.,\\nand Rush, A. M. (2021). Multitask prompted training enables zero-shot task generalization. ArXiv ,\\nabs/2110.08207.\\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili ´c, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon,\\nF., Gallé, M., et al. (2022a). Bloom: A 176b-parameter open-access multilingual language model.\\narXiv preprint arXiv:2211.05100 .\\nScao, T. L., Wang, T., Hesslow, D., Saulnier, L., Bekman, S., Bari, M. S., Bideman, S., Elsahar, H.,\\nMuennigho ff, N., Phang, J., et al. (2022b). What language model to train if you have one million\\ngpu hours? arXiv preprint arXiv:2210.15424 .\\nShaham, U., Elbayad, M., Goswami, V ., Levy, O., and Bhosale, S. (2022). Causes and cures for\\ninterference in multilingual translation. arXiv preprint arXiv:2212.07530 .\\nShannon, C. E. (1951). Prediction and entropy of printed english. Bell system technical journal ,\\n30(1):50–64.\\nShaw, P., Uszkoreit, J., and Vaswani, A. (2018). Self-attention with relative position representations.\\narXiv preprint arXiv:1803.02155 .\\nShazeer, N. (2019). Fast transformer decoding: One write-head is all you need. arXiv preprint\\narXiv:1911.02150 .\\nShazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202 .\\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P., Hawkins, P., Lee, H.,\\nHong, M., Young, C., Sepassi, R., and Hechtman, B. (2018). Mesh-TensorFlow: Deep learning for\\nsupercomputers. In Neural Information Processing Systems .\\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. (2019). Megatron-\\nlm: Training multi-billion parameter language models using model parallelism. arXiv preprint\\narXiv:1909.08053 .\\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., Liu, Z., Prabhumoye, S.,\\nZerveas, G., Korthikanti, V ., et al. (2022). Using deepspeed and megatron to train megatron-turing\\nnlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990 .\\nSoldaini, L., Lo, K., Kinney, R., Naik, A., Ravichander, A., Bhagia, A., Groeneveld, D., Schwenk, D.,\\nMagnusson, I., and Chandu, K. (2023). Dolma.\\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro,\\nA., Gupta, A., Garriga-Alonso, A., et al. (2023). Beyond the imitation game: Quantifying and\\nextrapolating the capabilities of language models. Transactions on Machine Learning Research .\\n47', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 46}), Document(page_content='Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A\\nsimple way to prevent neural networks from overfitting. Journal of Machine Learning Research ,\\n15(56):1929–1958.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., V oss, C., Radford, A., Amodei, D., and\\nChristiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural\\nInformation Processing Systems , 33:3008–3021.\\nSu, J., Lu, Y ., Pan, S., Wen, B., and Liu, Y . (2021). Roformer: Enhanced transformer with rotary\\nposition embedding. arXiv preprint arXiv:2104.09864 .\\nSutton, R. (2019). The bitter lesson. Incomplete Ideas (blog) , 13(1).\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y ., Li, X., Guestrin, C., Liang, P., and Hashimoto,\\nT. B. (2023). Stanford alpaca: An instruction-following llama model. https://github.com/\\ntatsu-lab/stanford_alpaca .\\nTay, Y ., Dehghani, M., Rao, J., Fedus, W., Abnar, S., Chung, H. W., Narang, S., Yogatama, D.,\\nVaswani, A., and Metzler, D. (2021). Scale e fficiently: Insights from pre-training and fine-tuning\\ntransformers. ArXiv , abs/2109.10686.\\nTay, Y ., Dehghani, M., Tran, V . Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster,\\nT., Zheng, S., et al. (2022a). Ul2: Unifying language learning paradigms. In The Eleventh\\nInternational Conference on Learning Representations .\\nTay, Y ., Wei, J., Chung, H. W., Tran, V . Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao, J.,\\nChowdhery, A., et al. (2022b). Transcending scaling laws with 0.1% extra compute. arXiv preprint\\narXiv:2210.11399 .\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T.,\\nBaker, L., Du, Y ., et al. (2022). Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239 .\\nTian, R. and Parikh, A. P. (2022). Amos: An adam-style optimizer with adaptive weight decay\\ntowards model-oriented scale. arXiv preprint arXiv:2210.11693 .\\nTiedemann, J. (2016). Finding alternative translations in a large corpus of movie subtitle. In Proceed-\\nings of the Tenth International Conference on Language Resources and Evaluation (LREC’16) ,\\npages 3518–3522, Portorož, Slovenia. European Language Resources Association (ELRA).\\nTillet, P., Kung, H.-T., and Cox, D. (2019). Triton: an intermediate language and compiler for tiled\\nneural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop\\non Machine Learning and Programming Languages , pages 10–19.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal,\\nN., Hambro, E., Azhar, F., et al. (2023a). Llama: Open and e fficient foundation language models.\\narXiv preprint arXiv:2302.13971 .\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y ., Bashlykov, N., Batra, S.,\\nBhargava, P., Bhosale, S., et al. (2023b). Llama 2: Open foundation and fine-tuned chat models.\\narXiv preprint arXiv:2307.09288 .\\nTrinh, T. H. and Le, Q. V . (2018). A simple method for commonsense reasoning. arXiv preprint\\narXiv:1806.02847 .\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\\nPolosukhin, I. (2017). Attention is all you need. In Advances in neural information processing\\nsystems , pages 5998–6008.\\nVillalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn, M., and Ho, A. (2022). Will we run\\nout of data? an analysis of the limits of scaling datasets in machine learning. arXiv preprint\\narXiv:2211.04325 .\\n48', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 47}), Document(page_content='Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.\\n(2019). Superglue: A stickier benchmark for general-purpose language understanding systems.\\nAdvances in neural information processing systems , 32.\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. (2018). Glue: A multi-task\\nbenchmark and analysis platform for natural language understanding. In International Conference\\non Learning Representations .\\nWang, B. and Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language\\nModel.https://github.com/kingoflolz/mesh-transformer-jax .\\nWang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y ., Liu, Q., Liu, T., and Sui, Z. (2023). Large\\nlanguage models are not fair evaluators. arXiv preprint arXiv:2305.17926 .\\nWang, T., Roberts, A., Hesslow, D., Scao, T. L., Chung, H. W., Beltagy, I., Launay, J., and Ra ffel,\\nC. (2022a). What language model architecture and pretraining objective work best for zero-shot\\ngeneralization? arXiv preprint arXiv:2204.05832 .\\nWang, Y ., Kordi, Y ., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H.\\n(2022b). Self-instruct: Aligning language model with self generated instructions. arXiv preprint\\narXiv:2212.10560 .\\nWei, J., Tay, Y ., Bommasani, R., Ra ffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou,\\nD., Metzler, D., et al. (2022a). Emergent abilities of large language models. Transactions on\\nMachine Learning Research .\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V ., Zhou, D., et al. (2022b).\\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural\\nInformation Processing Systems , 35:24824–24837.\\nWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L. A., Anderson, K., Kohli, P.,\\nCoppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. In Findings of\\nthe Association for Computational Linguistics: EMNLP 2021 , pages 2447–2469.\\nWenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V ., Guzmán, F., Joulin, A., and Grave, É.\\n(2020). Ccnet: Extracting high quality monolingual datasets from web crawl data. In Proceedings\\nof the 12th Language Resources and Evaluation Conference , pages 4003–4012.\\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M., Macherey, W., Krikun, M., Cao, Y ., Gao,\\nQ., Macherey, K., et al. (2016). Google’s neural machine translation system: Bridging the gap\\nbetween human and machine translation. arXiv preprint arXiv:1609.08144 .\\nXiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankarara-\\nman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y ., Narang, S., Malik, K., Fan, A., Bhosale,\\nS., Edunov, S., Lewis, M., Wang, S., and Ma, H. (2023). E ffective long-context scaling of\\nfoundation models.\\nXue, F., Fu, Y ., Zhou, W., Zheng, Z., and You, Y . (2023). To repeat or not to repeat: Insights from\\nscaling llm under token-crisis. arXiv preprint arXiv:2305.13230 .\\nXue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Ra ffel, C.\\n(2022). Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of\\nthe Association for Computational Linguistics , 10:291–306.\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Ra ffel, C.\\n(2021). mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the\\n2021 Conference of the North American Chapter of the Association for Computational Linguistics:\\nHuman Language Technologies , pages 483–498.\\nYang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N., Pachocki, J., Chen, W.,\\nand Gao, J. (2022). Tensor programs v: Tuning large neural networks via zero-shot hyperparameter\\ntransfer. arXiv preprint arXiv:2203.03466 .\\n49', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 48}), Document(page_content='Zellers, R., Holtzman, A., Bisk, Y ., Farhadi, A., and Choi, Y . (2019). HellaSwag: Can a machine\\nreally finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics , pages 4791–4800, Florence, Italy. Association for Computational\\nLinguistics.\\nZeng, W., Ren, X., Su, T., Wang, H., Liao, Y ., Wang, Z., Jiang, X., Yang, Z., Wang, K., Zhang,\\nX., et al. (2021). Pangu- α: Large-scale autoregressive pretrained chinese language models with\\nauto-parallel computation. arXiv preprint arXiv:2104.12369 .\\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X.,\\nLin, X. V ., et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint\\narXiv:2205.01068 .\\nZheng, L., Chiang, W.-L., Sheng, Y ., Zhuang, S., Wu, Z., Zhuang, Y ., Lin, Z., Li, Z., Li, D.,\\nXing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint\\narXiv:2306.05685 .\\nZhong, W., Cui, R., Guo, Y ., Liang, Y ., Lu, S., Wang, Y ., Saied, A., Chen, W., and Duan, N.\\n(2023). Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint\\narXiv:2304.06364 .\\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., and Fidler, S. (2015).\\nAligning books and movies: Towards story-like visual explanations by watching movies and\\nreading books. In Proceedings of the IEEE international conference on computer vision , pages\\n19–27.\\n50', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 49}), Document(page_content='A Contributions\\n•Engineering & Tooling .\\n– Distributed training codebase. Baptiste Pannier, Daniel Hesslow.\\n–Web data. Guilherme Penedo, Ruxandra Cojocaru (multilingual data) , Quentin Malar-\\ntic(data quality) , Alessandro Cappelli, Hamza Alobeidli.\\n–Curated data. Alessandro Cappelli, Etienne Go ffinet(code data) , Quentin Malartic,\\nRuxandra Cojocaru, Abdulaziz Alshamsi (books data) .\\n– Inference. Daniel Hesslow, Baptiste Pannier, Guilherme Penedo (deployment) .\\n– Infrastructure. Daniele Mazzotta, Etienne Go ffinet, Guilherme Penedo.\\n– Hardware correctness. Baptiste Pannier, Julien Launay, Daniel Hesslow.\\n•Pretraining .Baptiste Pannier, Julien Launay, Daniel Hesslow.\\n•Research .\\n–Data ablations. Julien Launay, Ruxandra Cojocaru (curriculum learning) , Alessandro\\nCappelli, Quentin Malartic (fine-grained filters) , Daniel Hesslow.\\n–Architecture ablations. Julien Launay, Daniel Hesslow, Etienne Go ffinet, Quentin\\nMalartic (training objectives) , Baptiste Pannier, Badreddine Noune (optimizers) .\\n–Model finetuning. Quentin Malartic, Alessandro Capelli, Etienne Go ffinet(code\\nspecialization) , Guilherme Penedo (human evaluation) , Baptiste Pannier (long-context) ,\\nJulien Launay.\\n– Evaluation. Julien Launay, Daniel Hesslow, Quentin Malartic.\\n–Paper writing. Julien Launay, Daniel Hesslow, Alessandro Cappelli, Baptiste Pannier,\\nEbtesam Almazrouei.\\n•Leadership .Julien Launay, Ebtesam Almazrouei, Mérouane Debbah.\\nB Acknowledgements\\nWe would like to thank the AWS team, in particular Olivier Cruchant, for their support throughout\\nthe project, enabling us to eventually scale to up to 4,096 A100s the training of Falcon-180B. We\\nwould also like to thank Axel Marmet, Tri Dao, Dan Fu, Colin Ra ffel, Katherine Lee, Thomas Wolf,\\nIz Beltagy, and Dirk Groeneveld for insightful discussions throughout the project.\\n51', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 50}), Document(page_content='C Model card\\nModel details\\nOrganization The models were created by the Technology Innovation Institute.\\nModel date Training of the Falcon models started in December and completed\\nin the first half of 2023.\\nModel type and informa-\\ntion about trainingFalcon are autoregressive Transformer models trained with a\\ncausal language modeling objective. Architecture based on\\nPaLM Chowdhery et al. (2022), with an extension of multiquery\\nattention for tensor parallelism (multigroup) and minor tweaks\\n(no SwiGLU, etc.). See Section 4 and Section 5 for details.\\nLicence Falcon-7B and Falcon-40B are made available under the Apache\\n2.0 license; Falcon-180B is made available under the Falcon-\\n180B TII license, with restrictions related to responsible use.\\nPoint of contact falconllm@tii.ae\\nIntended use\\nPrimary intended uses Research on large language models; as a foundation for further\\nspecialization for specific use cases (e.g., chatbot, etc.)\\nPrimary intended users NLP researchers and engineers.\\nOut-of-scope use cases Production use without adequate assessment of risks and mitiga-\\ntion; use cases which may be considered irresponsible or harmful.\\nFactors\\nRelevant factors The Falcon models are predominantly trained on English data\\nfrom a large-scale web corpora representative of the web. Ac-\\ncordingly, they will carry the stereotypes and biases commonly\\nencountered online, and are unlikely to generalize appropriately\\nbeyond English or European latin languages.\\nEvaluation factors We evaluated the toxicity of the underlying pretraining dataset and\\nfound it to be in line with common curated pretraining datasets\\nsuch as The Pile, see Penedo et al. (2023). Note that this only\\naccounts for toxicity under the definition of Perspective API:\\n\"content that is rude or disrespectful\". Notably, this fails to\\ninclude concerns about social biases or harmfulness.\\nMetrics\\nModel performance mea-\\nsuresWe focus our evaluation on the zero-shot generalization capabil-\\nities of our models across a wide range of tasks, leveraging the\\nEleuther AI language model evaluation harness Gao et al. (2021).\\nVariation approaches Due to the costs associated with training Falcon we cannot train\\nthe models multiple times and measure variability across runs.\\nEvaluation data\\nDatasets We evaluate zero-shot accuracy on 18 natural language tasks and\\none Python programming task, detailed in Section 6.\\nMotivation We selected and aggregated tasks to build comparisons with other\\nmodels in the literature (see Section 6.1).\\nPreprocessing We mostly use the default setup of Gao et al. (2021), see Ap-\\npendix G for the custom prompts we used for some evaluations\\nTraining data\\nSee the dedicated datasheet in Penedo et al. (2023).\\nTable 23: Model card for Falcon , following the framework introduced by Mitchell et al. (2019).\\n52', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 51}), Document(page_content='D Datasheet\\nSee the dedicated RefinedWeb paper (Penedo et al., 2023).\\nE Comparisons with undocumented models\\nA number of recent models have elected to release scant promotional technical reports instead of\\nadequately documented research papers; they notably only provide sparse evaluations and modeling\\ndetails are often absent. This makes comparisons challenging, even more so as many of the details\\nend-up being leaked or known through back-channels–a format which lends itself poorly to citation\\nand attribution. For Anil et al. (2023), parameter count and pretraining length were reported by\\nCNBC3for the largest of the three models. For OpenAI (2023a), a number of leaks have occurred,\\nmost of which have been summarized in a piece by SemiAnalysis4.\\nF Pseudocode samples\\nF.1 Measurement plan to measure all to all bandwidths /latencies e fficiently\\ndefget_all_comps(n: int):\\n# n: power of two\\ndefop(l, d=4, r=1):\\nl = l.reshape(-1, d)\\nl[1::2] = np.roll(l[1::2], r, axis=1)\\nreturn l.T.reshape(-1)\\nx = np.array(list( range(n)))\\ncomps = []\\nd = 1\\nwhiled < n:\\nforrin range (d):\\ncomps.append(op(x, d=d, r=r).copy())\\nd *= 2\\nret = np.stack(comps)\\nreturn ret.reshape(ret.shape[0], -1, 2)\\nF.2 Converting tree token depth into an attention mask:\\ndefattn_mask_pos_(x, attention_mask):\\nforiin range (len(x)):\\nattn = False\\nforjin range (0, len(x)):\\nattn |= (i + 1) == j\\nattn &= x[i] < x[j]\\nattention_mask[j, i] = ~attn &(i != j)\\nF.3 Zero-1 pseudo-code\\ndefreduce_scatter_grads( buffer):\\nchunk_size = buffer.model_grads.numel() // dp_world_size\\nt = empty_tensor(chunk_size, dtype=bfloat16)\\nreduce_scatter_tensor(t, buffer.model_grad, group=data_parallel_group\\n)\\nbuffer.optimizer_grads.copy(t)\\n3https://www.cnbc.com/2023/05/16/googles-palm-2-uses-nearly-five-times-more-text-data-than-predecessor.\\nhtml\\n4https://www.semianalysis.com/p/gpt-4-architecture-infrastructure\\n53', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 52}), Document(page_content='defall_gather_opt_params_to_model_params( buffer):\\nbfloat_sharded_params = buffer.optimizer_params.to(bfloat16)\\nall_gather_into_tensor( buffer.model_params, bfloat_sharded_params,\\ngroup=data_parallel_group)\\n...\\nreduce_scatter_grads( buffer)\\noptimizer.step()\\nall_gather_opt_params_to_model_params( buffer)\\nG Prompts\\nFor Section 4 and Section 6.5, we use the default prompts of the Eleuther AI Harness (Gao et al.,\\n2021)–these prompts aim to reproduce the setup of the GPT-3 evaluations (Brown et al., 2020). For\\nresults in Section 6.2, Section 6.3, Section 6.4 we use the prompts outlined thereafter; if no prompt\\nis specified, then the default one from the Eleuther AI Harness is used. Specifically for Section 6.2\\nwe never outline candidates in multiple-choice questions (e.g., ARC, RACE, etc.) to allow for fair\\ncomparisons with PaLM-2: this left-out content is colored in gray in the following prompts.\\nTheContext is appended once at the beginning of the prompt; Sample(s) are repeated for each shot,\\nwith the answer provided for all but the last sample; Candidates have their logprobability evaluated\\ngiven the previous context and sample(s). Content in italics is fetched from the task data (from the\\ntrain set for illustrative purpose thereafter, but the test or validation sets are used whenever available\\nfor actual evaluation). Weay write down the candidates in a comma-separated list [candidate1,\\ncandidate2, ...]. Candidates are always preceded by a space for tokenization.\\nTable 24: ANLI.\\nContext Determine whether the statement made about the extract is true, false, or unsure.\\nSample(s) Extract: Ernest Jones is a British jeweller and watchmaker. Established in 1949,\\nits first store was opened in Oxford Street, London. Ernest Jones specialises in\\ndiamonds and watches, stocking brands such as Gucci and Emporio Armani. Ernest\\nJones is part of the Signet Jewelers group.\\nStatement: The first Ernest Jones store was opened on the continent of Europe.\\nQuestion: true, false, or unsure?\\nAnswer:\\nCandidates [true, false, unsure]\\nTable 25: ARC. When options are not provided in the prompt, the candidates are directly the possible\\nanswers (e.g., dry palms, wet palms, etc.) instead of the letter keys.\\nContext Answer the following multiple-choice questions.\\nSample(s) Question: George wants to warm his hands quickly by rubbing them. Which skin\\nsurface will produce the most heat?\\nA.dry palms\\nB.wet palms\\nC.palms covered with oil\\nD.palms covered with lotion\\nAnswer:\\nCandidates [A., B., C., D.]\\n54', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 53}), Document(page_content='Table 26: RTE.\\nContext Determine whether the statement made about an extract is True, or False.\\nSample(s) Extract: Herceptin was already approved to treat the sickest breast cancer patients,\\nand the company said, Monday, it will discuss with federal regulators the possibility\\nof prescribing the drug for more breast cancer patients.\\nStatement: Herceptin can be used to treat breast cancer. True or False?\\nAnswer:\\nCandidates [True, False]\\nTable 27: LAMBADA. We evaluate whether the correct answer is the most likely one according to\\nthe model overall, without constraint to a set of predetermined candidates.\\nContext Complete the ____ in the following extracts.\\nSample(s) Extract: My wife refused to allow me to come to Hong Kong when the plague was at\\nits height and –\" \"Your wife, Johanne? You are married at last ?\" Johanne grinned.\\n\"Well, when a man gets to my age, he starts to need a few home comforts. After my\\ndear mother passed away ten years ago now, I became ____\\nCompletion:\\nTable 28: OpenBookQA. When options are not provided in the prompt, the candidates are directly\\nthe possible answers (e.g., puppies learning new tricks, etc.) instead of the letter keys.\\nContext Answer the following multiple-choice questions.\\nSample(s) Question: The sun is responsible for\\nA.puppies learning new tricks\\nB.children growing up and getting old\\nC.flowers wilting in a vase\\nD.plants sprouting, blooming and wilting\\nAnswer:\\nCandidates [A., B., C., D.]\\n55', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 54}), Document(page_content='Table 29: RACE. Note that the way Brown et al. (2020) and Gao et al. (2021) implement RACE\\nmeans that the number of shots is at the article level; 3 questions regarding each article are always\\nprovided, even in 0-shot.\\nSample(s) Article: Last week I talked with some of my students about what they wanted to\\ndo after they graduated, and what kind of job prospects they thought they had.\\nGiven that I teach students who are training to be doctors, I was surprised do find\\nthat most thought that they would not be able to get the jobs they wanted without\\n\"outside help\". \"What kind of help is that?\" I asked, expecting them to tell me\\nthat they would need a or family friend to help them out. \"Surgery ,\" one replied.\\nI was pretty alarmed by that response. It seems that the graduates of today are\\nincreasingly willing to go under the knife to get ahead of others when it comes to\\ngetting a job . One girl told me that she was considering surgery to increase her\\nheight. \"They break your legs, put in special extending screws, and slowly expand\\nthe gap between the two ends of the bone as it re-grows, you can get at least 5\\ncm taller!\" At that point, I was shocked. I am short, I can’t deny that, but I don’t\\nthink I would put myself through months of agony just to be a few centimetres taller.\\nI don’t even bother to wear shoes with thick soles, as I’m not trying to hide the\\nfact that I am just not tall! It seems to me that there is a trend towards wanting\\n\"perfection\" , and that is an ideal that just does not exist in reality. No one is born\\nperfect, yet magazines, TV shows and movies present images of thin, tall, beautiful\\npeople as being the norm. Advertisements for slimming aids, beauty treatments\\nand cosmetic surgery clinics fill the pages of newspapers, further creating an idea\\nthat \"perfection\" is a requirement, and that it must be purchased, no matter what\\nthe cost. In my opinion, skills, rather than appearance, should determine how\\nsuccessful a person is in his /her chosen career\\nQuestion: We can know from the passage that the author works as a_.\\nA.doctor\\nB.model\\nC.teacher\\nD.reporter\\nAnswer: C. teacher\\nQuestion: Many graduates today turn to cosmetic surgery to_.\\nA.marry a better man /woman\\nB.become a model\\nC.get an advantage over others in job-hunting\\nD.attract more admirers\\nAnswer: C. get an advantage over others in job-hunting\\nQuestion: According to the passage, the author believes that_.\\nA.everyone should purchase perfection, whatever the cost\\nB.it’s right for graduates to ask for others to help them out in hunting for jobs\\nC.it is one’s appearance instead of skills that really matters in one’s career\\nD.media are to blame for misleading young people in their seeking for surgery\\nAnswer: D. media are to blame for misleading young people in their seeking for\\nsurgery\\nQuestion: Which’ s the best title for the passage?\\nA.Young Graduates Have Higher Expectations\\nB.Young Graduates Look to Surgery for Better Jobs\\nC.Young Graduates’ Opinion About Cosmetic Surgery\\nD.Young Graduates Face a Di fferent Situation in Job-hunting\\nAnswer:\\nCandidates [A., B., C., D.]\\n56', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 55}), Document(page_content='Table 30: BoolQ.\\nContext Answer the following questions about an extract by yes, or no.\\nSample(s) Extract: Powdered sugar, also called confectioners’ sugar, icing sugar, and icing\\ncake, is a finely ground sugar produced by milling granulated sugar into a powdered\\nstate. It usually contains a small amount of anti-caking agent to prevent clumping\\nand improve flow. Although most often produced in a factory, powdered sugar can\\nalso be made by processing ordinary granulated sugar in a co ffee grinder, or by\\ncrushing it by hand in a mortar and pestle.\\nQuestion: is confectionary sugar the same as powdered suga , yes or no?\\nAnswer:\\nCandidates [yes, no]\\nTable 31: CB.\\nContext Determine whether the statement made about by the extract is True, False, or\\nUnsure.\\nSample(s) Extract: It was a complex language. Not written down but handed down. One might\\nsay it was peeled down.\\nStatement: the language was peeled down .\\nQuestion: True, False, or Unsure?\\nAnswer:\\nCandidates [True, False, Unsure]\\nTable 32: COPA. We keep the formatting of Gao et al. (2021) unchanged, but add an instruction.\\nContext Complete the following sentences.\\nTable 33: WiC.\\nSample(s) Extract: place has a similar meaning in the following two sentences. Yes or no?\\nSentence 1: Do you want to come over to my place later?\\nSentence 2: A political system with no place for the less prominent groups.\\nAnswer:\\nCandidates [yes, no]\\nTable 34: Winograd. We keep the formatting of Gao et al. (2021) unchanged, but switch yes /no in\\nthe candidates for true /false.\\nCandidates [true, false]\\n57', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 56})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 05: Split the Extracted Data into Text Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "\n",
        "text_chunks = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "0U5JXPVI8qPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "id": "OfKgzA6f8qNB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cecf402-f83f-479b-c7cf-0710c8d5ff41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the third chunk\n",
        "text_chunks[2]"
      ],
      "metadata": {
        "id": "bdvDab_W8qKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "388b1dea-9f8c-4f25-c378-6bf5ee459c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Contents\\n1 Introduction 2\\n2 State-of-the-art: from language modeling to frontier models 5\\n3 Design philosophy 6\\n4 Experiments and motivations for data, architecture, and hyperparameters 7\\n4.1 Setup for small-scale experiments . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n4.2 Data: web vs curated, code and multilinguality impact on English performance . . 8\\n4.2.1 Web data alone can outperform curated corpora . . . . . . . . . . . . . . . 8\\n4.2.2 Against a strong web baseline, curated data can even be detrimental . . . . 10\\n4.2.3 Limited code and multilingual data do not strongly degrade English performance 11\\n4.3 Architecture and pretraining: validating popular recipes, and inference optimizations 12\\n4.3.1 Extending multiquery into multigroup for tensor parallel training and inference 12\\n4.3.2 Rotary positionnal embeddings may only o ffer a limited edge over ALiBi . 14\\n4.3.3 The extra memory cost of GLU may not be worth it for cost-e fficient training 14\\n4.3.4 Small tweaks help scalability: parallel layers and no biases in linear layers 15\\n4.3.5 Validating best practices for hyperparameters: z-loss, weight decay, LR search 15\\n4.4 Further experimentation required: ideas that did not make the cut . . . . . . . . . . 17\\n4.5 Wrapping-it up: validating overall dataset and architecture recipes . . . . . . . . . 18\\n5 Implementation 19\\n5.1 The Falcon dataset: predominantly web, with added curated and conversational data 19\\n5.1.1 The Macrodata Refinement pipeline and the RefinedWeb dataset . . . . . . 20\\n5.1.2 The Microdata curated corpora and conversational masking . . . . . . . . . 21\\n5.2 The Falcon architecture and recipe for e fficient inference and (stable) training . . . 22\\n5.2.1 Architectural nitpicks: separate layer norms, tied embeddings, and scaling-up 22\\n5.2.2 Large language model alchemy: hyperparameters for pretraining . . . . . . 24\\n5.3 Large-scale distributed training on cloud infrastructure with Gigatron . . . . . . . 24\\n5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability 25\\n5.3.2 State-of-the-art throughput with dedicated Triton kernels . . . . . . . . . . 28\\n5.3.3 Efficient memory use via selective recomputation implemented as a monolayer 28\\n5.3.4 Numerical precision: all you need is bfloat16 ? . . . . . . . . . . . . . . 29\\n5.3.5 Quality-of-life features for improved flexibility and reliability . . . . . . . 29\\n5.4 Run management: keeping large-scale infrastructure running . . . . . . . . . . . . 29\\n6 Results 30\\n6.1 To prompt or not to prompt: comparing evaluations across codebases . . . . . . . . 31\\n6.2 Comparisons with PaLM on a natural language tasks aggregate . . . . . . . . . . . 33\\n6.3 Comparisons with GPT-3.5 and GPT-4 on a limited set of tasks . . . . . . . . . . . 34\\n3', metadata={'source': '/content/sample_data/Data/falcon reasearch paper.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 06:Downlaod the Embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "d-4KJQPu9B4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "aea924fbf6434b54916188d4f436e9ce",
            "0b9d7fbbcdec48efb7724f9b4a54136e",
            "5c596f70a8284b56bb0d6963f8b88dbf",
            "8260e5fe9be24387ac79d04a351b4557",
            "1cc1c5df47624a4fba553ee64afd6a51",
            "b0f4e8e29cf04fd1aa1d7153a4884d84",
            "a4cff59bb91f47559808242048ff22a7",
            "337b7cde41494bbc9ca64491a98bf19d",
            "7aed852e7c514a37825adee20a847817",
            "b9ada69891ee49bdb93a984102e4c966",
            "05b53a8f5f6e452592a1b6c199a33b90",
            "88b1bb8bfacd410e966ec661509ff265",
            "49e5521c64834132910998470880bf6b",
            "4ce9843ebfcf41298b9577bbda98caf9",
            "f71592df56b340f195ee5b30d3880487",
            "724cc39d099144e79816c6b46375caa3",
            "d1c6ac82c4ee46b1a5d1dc435e72b5a3",
            "26e7b92c2cd34136a381b7cbb6ef631d",
            "a70b6d32c4ea4acbbcf788dac120ce27",
            "0a8ce55de76d41aa83bb09d48fe5d4e7",
            "32b3d278ab1c40b5b68a0006f570cfa6",
            "ac1223bfd2d74f1b9009e362cdf89988",
            "e638e36b2d454c17be01accdef691061",
            "f79b883294da4f07a682c61d10fa1d01",
            "5c47acc687bd4c848f6460ec0d6a144b",
            "ffaae84b5f1e4a3d8f282ca78e5c7277",
            "9c9c7873693248fe8ad3351f7e41587f",
            "b2f2fe7f5c7c4b3d85bc0c5e41222726",
            "5f6e2c9759c647a2b0eb9b9ee93adf62",
            "dfd7c1e87b6e4aefa82261c7ca5a5594",
            "7a29de968f514c93a12e3c9edb36437b",
            "c1470812be2f467fb47ee38c7d3cb346",
            "1406ff5091d94514b5343542012b9796",
            "c160eac44ac94d8ba96e106915709ad3",
            "393cd6f77ddf46f4a65f34beaf20e073",
            "8e6dfb1ca38849dfaad59abd4befe95b",
            "a97137fa10f4444192f8066a0613639f",
            "9677234cdd294fc0ba34babf7cd99dc9",
            "f36cad760a5e40a8a9e55b5ebb80984e",
            "181162ddd0ae40fbb31add09f0bd0ebb",
            "1f8b5d38623d48c789d8f03f2b587def",
            "8a468cf68ffd4fc9b29fb9b17aa09b64",
            "b09ea6e78d3547b3a39783dab2382f8b",
            "685405a0f74f44d3b2b2776a96ace33f",
            "4c85eb691af341c69006f5ab29ecc720",
            "c78f1e08b31845048f28186b66b5ea0f",
            "ce6e04fd75d6487582676a91ce43afe3",
            "ce72ae662cba4196893855335657f5a5",
            "64df1f92c4df4234b26d2e5bba8408a2",
            "465d6fa2852b439f88f4e92df30e09f9",
            "f6de3c571b5f4f79a7da28d0bda7fdaf",
            "43666a38defe45f4be91d332a82a5884",
            "ade71f17c11f4e47b47c3ce0c70e9e98",
            "b9f7c19b102044ef881ad14003ec2bd8",
            "137a183f65574731a1fbd83047065364",
            "b8b550650ab34694bf84e754cedad975",
            "6e82f22de91b404cb483891107e31717",
            "638bfaa88a5440b1a4d2a1f018430942",
            "dbc5b626a9e648e1a74c6167ae86d257",
            "80061d9270da4e429acb1afc7e3e3036",
            "4694e0f3e85640ffa5d5e74c9f91efd3",
            "b8ddc81f8dfa41ca87b687e7eec11ba1",
            "e363b3ce6ae947639992024c79f00bb9",
            "1b0665d96c2244a48071aeaf801a4600",
            "e38a976bf01e45d2aac837b140fc9a59",
            "b413ecdf199a4af6b61f4b80a86b39f8",
            "2b736e9d43d0480cbf97fdf5ee2351bb",
            "2ef44b4182b141c68e01868837d87245",
            "60ecdb6154f142df873a3f4fb5eea456",
            "1de84ffd043c408aae675f0e0e7cdc7f",
            "e321e912cb5c48bcaebcdbf101a58b73",
            "7ea19ac9241743aaad401ce031ecbb81",
            "0b6da214052c46248abd92157289b48e",
            "a99c31f7064141caaf567e0274dd69e6",
            "fdd686cc77ff4d40814497b4eb965fb5",
            "142a617f1db043c79d08780d07b1ddd5",
            "66f014111e314932ab48f840fc1f3c3f",
            "0a43ad7ebbe94250b40aa0021eaae96e",
            "98255f279521431a8e053c9e4ef37bfd",
            "db30f2b106344be69093c496e2df1dda",
            "fc4d56d7ca0b4d0285337f7eb4854dee",
            "dc3de42613674acfa3fcd0c6391c2bdc",
            "cdf698b7de814d1784a63e6dd45301c2",
            "b23aa2e6c81448f0b9737e622a9edc45",
            "91c2e6a4d07a4cb480cd95e99e3bd535",
            "2175f1736d3e4baba7f4eb2caeb494f6",
            "8207616a2a034e3796c946d5703d1372",
            "325c621b9f604b6e8b1688c0f7bd1e27",
            "c63c48661992455ba278355d5b39fcee",
            "bc77744ccdc84cb9b7a779b5f8d47d45",
            "9df28132ca084f0c8b39609d0e49be61",
            "ac2d08d5690141a99b2e71fffda3ec2e",
            "6025cff1bf6e4311a1e401f232bf14f6",
            "17a45604062e4363b454e43ac4f069fa",
            "81475e26459c45508fecaebc24c487b5",
            "8eab7fafa564491cabb16e310b3195f6",
            "6d5b85047853417b88502c16c40881b7",
            "64d3a894954e4d5282247d515e0081c5",
            "0a054397d5dd499bae581ef7a147a82b",
            "5902e1345c974c96944eb8a89754462e",
            "7e66ef21324e404c9dbd2bbcecf5de31",
            "9c00b6ab658f49bfa49a5f76e664ab65",
            "86552490ff9e4b1482cb0ce9f845130c",
            "0fa1a2d83c4648f08646db93465d44a1",
            "0fdd6e64ce974f22b865f5a8100048d5",
            "58babb1b9ae64dafb276fbbdf8fb80f2",
            "7994d02d09624a20bfed588f6fca2263",
            "0b7617bee5b846ada71fc403c0147bca",
            "f6d59af107fe4582a7aaf9a3809c4b35",
            "f93104b503134cf79d57bf2fa02b9288",
            "f9929f0a15844a938a7cd414f2ba2b50",
            "137362a1b0e24e0fb3f0fa89de6de5d9",
            "a4c91310b68441faae8f6dbf4ed4c0e4",
            "ba5aa0194c2f4c869b547bed095ce9a0",
            "02f59cb878a7417cb77949c5895e3964",
            "0cc60df6711344d2ad22b80718c8ac48",
            "f1e14d196fcf462ba35921db32ac7420",
            "9139521fe0914103b37a78ce105add60",
            "ea5764654b0f4257af468262b33a4f3a",
            "aa85216e4dc043c4a988157539243a2b",
            "f0e830b04d4242c582b3a463f4b293af"
          ]
        },
        "outputId": "cef0561b-7d26-4b82-e780-2eb3027e180d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aea924fbf6434b54916188d4f436e9ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88b1bb8bfacd410e966ec661509ff265"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e638e36b2d454c17be01accdef691061"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c160eac44ac94d8ba96e106915709ad3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c85eb691af341c69006f5ab29ecc720"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8b550650ab34694bf84e754cedad975"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b736e9d43d0480cbf97fdf5ee2351bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a43ad7ebbe94250b40aa0021eaae96e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c63c48661992455ba278355d5b39fcee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5902e1345c974c96944eb8a89754462e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9929f0a15844a938a7cd414f2ba2b50"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 08: Create Embeddings for each of the Text Chunk\n",
        "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
      ],
      "metadata": {
        "id": "K1SMCcbS9B1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token =\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSatadKtttZN",
        "outputId": "9f6e8864-f1e0-4c8d-8b1e-dfb36318045d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "kbkIyO2TwwSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export HUGGINGFACEHUB_API_TOKEN=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"
      ],
      "metadata": {
        "id": "_NHfA424w1WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('secret')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7oJq_tary9YP",
        "outputId": "6e5725d8-9b8e-465f-f272-445d482ca1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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\\r\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "llm=HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.5 ,\"max_length\":1000},huggingfacehub_api_token=\"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\")"
      ],
      "metadata": {
        "id": "yk7TWa5u9Byk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vpb_w-i94Th"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DWiBe3cENZB"
      },
      "outputs": [],
      "source": [
        "query = \"What is linear regression model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdFYMQfR_gzI"
      },
      "outputs": [],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl_4JdUatnTQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d606ab1-3870-4f64-9f7f-eda60ee57c80"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "C Model card\n",
            "Model details\n",
            "Organization The models were created by the Technology Innovation Institute.\n",
            "Model date Training of the Falcon models started in December and completed\n",
            "in the first half of 2023.\n",
            "Model type and informa-\n",
            "tion about trainingFalcon are autoregressive Transformer models trained with a\n",
            "causal language modeling objective. Architecture based on\n",
            "PaLM Chowdhery et al. (2022), with an extension of multiquery\n",
            "attention for tensor parallelism (multigroup) and minor tweaks\n",
            "(no SwiGLU, etc.). See Section 4 and Section 5 for details.\n",
            "Licence Falcon-7B and Falcon-40B are made available under the Apache\n",
            "2.0 license; Falcon-180B is made available under the Falcon-\n",
            "180B TII license, with restrictions related to responsible use.\n",
            "Point of contact falconllm@tii.ae\n",
            "Intended use\n",
            "Primary intended uses Research on large language models; as a foundation for further\n",
            "specialization for specific use cases (e.g., chatbot, etc.)\n",
            "Primary intended users NLP researchers and engineers.\n",
            "Out-of-scope use cases Production use without adequate assessment of risks and mitiga-\n",
            "tion; use cases which may be considered irresponsible or harmful.\n",
            "Factors\n",
            "Relevant factors The Falcon models are predominantly trained on English data\n",
            "from a large-scale web corpora representative of the web. Ac-\n",
            "cordingly, they will carry the stereotypes and biases commonly\n",
            "encountered online, and are unlikely to generalize appropriately\n",
            "beyond English or European latin languages.\n",
            "Evaluation factors We evaluated the toxicity of the underlying pretraining dataset and\n",
            "found it to be in line with common curated pretraining datasets\n",
            "such as The Pile, see Penedo et al. (2023). Note that this only\n",
            "accounts for toxicity under the definition of Perspective API:\n",
            "\"content that is rude or disrespectful\". Notably, this fails to\n",
            "include concerns about social biases or harmfulness.\n",
            "Metrics\n",
            "Model performance mea-\n",
            "suresWe focus our evaluation on the zero-shot generalization capabil-\n",
            "ities of our models across a wide range of tasks, leveraging the\n",
            "Eleuther AI language model evaluation harness Gao et al. (2021).\n",
            "Variation approaches Due to the costs associated with training Falcon we cannot train\n",
            "the models multiple times and measure variability across runs.\n",
            "Evaluation data\n",
            "Datasets We evaluate zero-shot accuracy on 18 natural language tasks and\n",
            "one Python programming task, detailed in Section 6.\n",
            "Motivation We selected and aggregated tasks to build comparisons with other\n",
            "models in the literature (see Section 6.1).\n",
            "Preprocessing We mostly use the default setup of Gao et al. (2021), see Ap-\n",
            "pendix G for the custom prompts we used for some evaluations\n",
            "Training data\n",
            "See the dedicated datasheet in Penedo et al. (2023).\n",
            "Table 23: Model card for Falcon , following the framework introduced by Mitchell et al. (2019).\n",
            "52\n",
            "\n",
            "8 Conclusion\n",
            "In this paper, we have introduced and extensively described the Falcon series of pretrained models,\n",
            "with Falcon-7 /40/180B, trained on up to 3,500B tokens.\n",
            "First, we described some of the ablations and experiments we performed to prepare the training\n",
            "dataset and architecture of the Falcon models. We found adequately filtered and deduplicated web\n",
            "data to be a surprisingly strong baseline; concerns around memorization for larger models (Carlini\n",
            "et al., 2022; Hernandez et al., 2022) led us to elect not to upsample any sources, and to predominantly\n",
            "train on this web data. On the architecture-side, we found most interventions to have a limited e ffect,\n",
            "and adopted multigroup attention (an extension of multiquery (Shazeer, 2019)) as a way to improve\n",
            "inference scalability by significantly reducing the size of the K,V-cache. For future generations of the\n",
            "Falcon series, we see promise in significantly increasing the fraction of code in the pretraining data,\n",
            "and in training with longer sequence lengths in a staged process (e.g., half of the training up to 8k,\n",
            "and second half up to 16k, with downstream adaptation to 32-256k).\n",
            "Then, we described our implementation of our final strategy for the pretraining of the Falcon series.\n",
            "We report extensively on our data pipeline in Penedo et al. (2023). We described our approach to\n",
            "conversational masking, and our distributed training strategy for running on cost-e fficient cloud\n",
            "infrastructure, relying notably on 3D parallelism combined with ZeRO. We also discussed some of\n",
            "our interventions for fast memory e fficient training, such as dedicated FlashAttention (Dao et al.,\n",
            "2022) kernels in Triton (Tillet et al., 2019) and our monolayer strategy. We also discussed some of\n",
            "the details around setting hyper-parameters and managing runs over multiple thousand GPUs.\n",
            "Finally, we outlined some of the results obtained by the Falcon series on key benchmarks. We found\n",
            "Falcon-180B to near the performance of PaLM-2 Large (Anil et al., 2023), and to end-up in-between\n",
            "GPT-3.5 and GPT-4 (OpenAI, 2023a). Falcon-180B is the best open-source model currently available,\n",
            "and likely one of the best models overall. We note our evaluation predominantly focuses on classic\n",
            "natural language tasks, and that further work will be required for evaluating human preferences on\n",
            "downstream versions of Falcon having undergone dedicated finetuning or reinforcement learning.\n",
            "To foster open research on large language models, and accelerate technological development in this\n",
            "space, we make the following artefacts public available under permissive open-source licenses:\n",
            "•Falcon-7 /40/180B. We make all models in the Falcon series available, with Falcon-7 /40B\n",
            "under an Apache 2.0 license and Falcon-180B under a dedicated responsible AI use license.\n",
            "At time of release, Falcon-180B is the most powerful open large language model available.\n",
            "•A 600B tokens extract of RefinedWeb. We make a 600B tokens extract of our web dataset\n",
            "available, for use by researchers to study large-scale filtered and deduplicated web data, and\n",
            "for other practioners to adopt as a standard for high-quality web data. We also open-source\n",
            "1/7B models trained on 350B tokens from this extract.\n",
            "•Detailed research. With this paper and the RefinedWeb paper (Penedo et al., 2023), we\n",
            "have detailed numerous of our decisions and experiments concerning the Falcon series.\n",
            "We believe large language models to be a foundational technology for the future of our civilization,\n",
            "and in turn we believe they should be shared responsibly. Widespread exchange of ideas is a staple of\n",
            "accelerated technological and economical progress in our history; in turn, this acceleration uplifts\n",
            "all. By open-sourcing artificial intelligence research and models, we can foster a broader and more\n",
            "diverse community, and benefit from vibrant collaborative e fforts to improve the safety and reliability\n",
            "of large language models. We hope the Falcon series can be a small step towards this vision.\n",
            "39\n",
            "\n",
            "Question: How did the use of AWS affect the overall development timeline of the Falcon series?\n",
            "Helpful Answer: The use of AWS affected the overall development timeline of the Falcon series by providing cost-efficient cloud infrastructure for distributed training, allowing for the training of larger models and faster memory-efficient training. AWS also allowed for the use of 3D parallelism combined with ZeRO, which improved inference scalability. Additionally, AWS provided the necessary computing resources for running on multiple thousand GPUs, allowing for faster hyperparameter management and setting up runs over multiple GPUs.\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "5 Implementation\n",
            "Based on our findings from the previous ablations Section 4, and further tests and best practices from\n",
            "the literature, we now describe the codebases and methods used to train the Falcon series of models.\n",
            "5.1 The Falcon dataset: predominantly web, with added curated and conversational data\n",
            "Based on estimates of our compute budget of 30,000-50,000 PF-days, we target a pretraining dataset\n",
            "size in the range of 3,000-5,000 billion tokens –we use Ho ffmann et al. (2022) as an upper boundary\n",
            "for model size and lower boundary for pretraining length. This is more than 2x the size of the dataset\n",
            "for Chinchilla, and 10x the one for GPT-3; although this range of size is recently becoming more\n",
            "common in concurrent models like LLaMA-2 or OLMo (Touvron et al., 2023b; Soldaini et al., 2023).\n",
            "Out of concerns for memorization and degradation caused by repeating data (Carlini et al., 2022;\n",
            "Hernandez et al., 2022) we choose to not upsample any sources .\n",
            "High-level overview. In Section 4.2.1, we have shown that su fficiently filtered and deduplicated web\n",
            "data can deliver performant models: this leads to focus on scaling-up web data to achieve the scale\n",
            "necessary. Because improvements to data quality translate to significant improvements to downstream\n",
            "performance, and because data processing is tremendously cheaper than model training, we do not\n",
            "concern ourselves too much with optimizing for costs with data processing–it is likely that a 90%\n",
            "cheaper recipe with less than a 10% relative performance degradation could be found.\n",
            "We still include a small amount of curated data, inspired by The Pile (Gao et al., 2020) with the\n",
            "addition of conversations from Reddit (Baumgartner et al., 2020), as it is unlikely to degrade perfor-\n",
            "mance if adequately processed (Section 4.2.2) and as it may broaden the downstream applicability of\n",
            "the model. However, these sources are bound to remain a minority, given that we do not allow any\n",
            "upsampling–they end-up accounting for 13% of our final dataset. Regarding code and multilinguality,\n",
            "we take a conservative approach: based on our results in Section 4.2.3, we include 8% multilingual\n",
            "data and 3% code. These lower fractions than the ones experimented with are due to stock con-\n",
            "straint; specifically for code, further improvements to our pipeline enabled us to significantly scale\n",
            "availability, but this was after the models had started training, so we did not revise the mix.\n",
            "The final Falcon mixture is presented in Table 15. We designed the mixture based on a 3,500B tokens\n",
            "pretraining dataset, not allowing any upsampling of the curated sources. Despite di ffering training\n",
            "lengths, the same mixture (in %) is used for Falcon-7B, 40B, and 180B.\n",
            "Table 15: The final Falcon mixture is predominantly web-based (nearly 85%), but includes\n",
            "other curated corpora (without any upsampling) to broaden the expressiveness of the model.\n",
            "Individual curated corpora are inspired from The Pile (Gao et al., 2020), but rebuilt from scratch to\n",
            "ensure high-quality and compatibility with our data pipeline. Code stock is a rough estimate based\n",
            "on an updated pipeline; code data used in Falcon was sourced from permissively licensed GitHub\n",
            "repositories. Mixture was designed to avoid upsampling; note that total stocks for RefinedWeb were\n",
            "not known at the beginning of training, as processing was still in-progress. Quantities in tokens.\n",
            "Corpora Pretraining\n",
            "Name Source Stock Fraction Used\n",
            "RefinedWeb-English Filtered and deduplicated Common-\n",
            "Crawl, see Penedo et al. (2023)∼5,000B 76% 2,700B\n",
            "RefinedWeb-Euro Filtered and deduplicated multi-\n",
            "lingual (Europe-focused) Common-\n",
            "Crawl, see Penedo et al. (2023)∼2,000B 8% 400B\n",
            "Books Project Gutenberg 215B 6% 214B\n",
            "Conversations Reddit, StackOverflow, Hack-\n",
            "erNews, IRC, YouTube Subtitles170B 5% 168B\n",
            "Code GitHub ∼1,000B 3% 115B\n",
            "Technical arXiv, PubMed, USPTO, Wikipedia 60B 2% 57B\n",
            "19\n",
            "\n",
            "Table 20: Outside of PaLM-2 Large and GPT-4, Falcon-180B significantly improves other state-\n",
            "of-the-art models such as LLaMA-2 or Inflection-1 on commonsense tasks . Falcon-40B performs\n",
            "slightly under LLaMA-2 34B, because of a significantly smaller compute budget (2,800PF-days\n",
            "against 4,700PF-days, 70% more for LLaMA-2). We note the exceptional performance of Inflection-1\n",
            "on BoolQ; conversely, we note that, despite our best prompt engineering e fforts, we were unable\n",
            "to reproduce the performance reported by the LLaMA-2 paper on BoolQ for the Falcon series: the\n",
            "results we report can likely be improved. Bold for best, underline for second-best.\n",
            "PIQA HellaSwag Winogrande BoolQ LAMBADA\n",
            "(10-shot) (5-shot)\n",
            "GPT-3 81,0 78,9 70,2 60,5 76,2\n",
            "Gopher 81,8 79,2 70,1 79,4 74,5\n",
            "Chinchilla 81,8 80,8 74,9 83,7 77,4\n",
            "MT-NLG 82,0 80,2 73 78,2 76,6\n",
            "PaLM 82,3 83,4 81,1 88,0 77,9\n",
            "LLaMA-2 7B 78,8 77,2 78,6 69,2 77,4\n",
            "13B 80,5 80,7 82,1 72,8 81,7\n",
            "34B 81,9 83,3 76,7 83,7\n",
            "70B 82,8 85,3 87,3 80,2 85,0\n",
            "Inflection-1 84,2 84,3 85,8 83,3 89,7 78,5\n",
            "Falcon 7B 80,3 76,3 78,1 67,2 72,6 73,8 74,9\n",
            "40B 83,0 82,7 85,3 76,0 81,8 81,9 77,3\n",
            "180B 84,9 85,9 89,0 80,3 87,1 87,8 79,8\n",
            "970PF-days, 30% more), but we suspect that multiquery with a single head of dimension 64 is a very\n",
            "aggressive configuration for Falcon-7B. We find that LLaMA-2 and Inflection-1 achieve relatively\n",
            "similar performance, with Inflection-1 perhaps ahead thanks to exceptional performance on BoolQ.\n",
            "Question answering. In Table 21, we find again that Falcon-180B strongly outperforms other\n",
            "models from the state-of-art on question answering tasks. We do note that the Falcon series seems to\n",
            "underperform slightly (comparatively to other tasks) on MMLU: we believe this may be attributed\n",
            "to the large prevalence of web data in our pretraining dataset, compared to more technical sources\n",
            "which may be more immediately relevant to the style and content of questions found in MMLU.\n",
            "Code. We report results on HumanEval in Table 22. We find that Falcon-180B performs best amongst\n",
            "models focusing on natural language, with performance only matched by Inflection-1. In fact, despite\n",
            "being trained on only 3% code, Falcon-180B nearly matches the performance of both PaLM-Coder\n",
            "and PaLM-2 S∗, two models which have undergone dedicated code specialization following their\n",
            "pretraining. This is an encouraging result for the development of a Falcon-Coder specialization.\n",
            "Table 21: Falcon-180B improves significantly over GPT-3, PaLM, and LLaMA-2 on question\n",
            "answering datasets, while Falcon-40B performs in-line with LLaMA-2 34B.∗: for Falcon-7B on\n",
            "OpenBookQA, we report accuracy without outlining candidates, as performance is otherwise close to\n",
            "random (see Table 17 for details). Bold for best, underline for second-best.\n",
            "ARC-Challenge ARC-Easy OpenBookQA MMLU\n",
            "GPT-3 51,4 68,8 57,6\n",
            "PaLM 53,0 76,6 53,4 69,3\n",
            "LLaMA-2 7B 45,9 75,2 58,6 45,3\n",
            "13B 49,4 77,3 57,0 54,8\n",
            "34B 54,5 79,4 58,2 62,6\n",
            "70B 57,4 80,2 60,2 68,9\n",
            "Falcon 7B 44,5 73,6 44,6∗28,0\n",
            "40B 56,7 81,2 61,2 57,0\n",
            "180B 63,7 84,7 76,4 70,6\n",
            "35\n",
            "\n",
            "Question: Could you explain how the use of 4,096 A100s contributed to the tr?aining of Falcon-180B\n",
            "Helpful Answer: The use of 4,096 A100s contributed to the training of Falcon-180B by providing a large amount of computational resources. The A100 is a powerful deep learning processor that is capable of performing large-scale matrix operations, which are essential for training large language models. With 4,096 A100s, Falcon-180B was able to process a large amount of data quickly and efficiently,\n",
            "Input Prompt: tell me about Large-scale distributed training on cloud infrastructure with Gigatron?\n",
            "Answer: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Our infrastructure accordingly exists as an in-between between a true HPC system and a flexible\n",
            "cloud environment–notably, we found that the GPUs still had to share a single spine in the datacenter,\n",
            "as multispine configurations were unreliable. Altough it is more cost-e fficient, it also requires us to\n",
            "be mindful of its limitations. We found that the popular (and simple) recipe of training with fully\n",
            "sharded data parallelism (Rajbhandari et al., 2020) did not scale well to this infrastructure. Instead, we\n",
            "required the finer control of 3D parallelism (Narayanan et al., 2021b) to achieve optimal performance.\n",
            "Because of limitations in open-source frameworks at the time, we elected to build our own proprietary\n",
            "distributed training framework. Gigatron is based on pytorch , and at its core implements a 3D\n",
            "distributed parallelism strategy (Shoeybi et al., 2019; Narayanan et al., 2021b) combined with ZeRO\n",
            "optimizer sharding (Rajbhandari et al., 2020) to reduce memory consumption and improve scalability.\n",
            "5.3.1 Combining 3D parallelism for fine-grained control, and ZeRO for scalability\n",
            "Data Parallelism (DP). Data parallelism (Fig. 8) is by far the most commonly used form of paral-\n",
            "lelism. All machine learning frameworks now provide ways to easily parallelize model training across\n",
            "data samples: pytorch with Distributed Data Parallel ( DDP),jax withpmap , andtensorflow\n",
            "withMirroredStrategy (Paszke et al., 2017; Bradbury et al., 2021; Abadi et al., 2015). Data\n",
            "parallelism is appealing thanks to its simplicity: the distributed machinery can be hidden away inside\n",
            "the framework easily, without any interactions with the user-defined architecture. In its simplest\n",
            "implementation, data parallelism only requires two modifications to the training procedure: (1) each\n",
            "device needs to operate on unique data samples; (2) the gradients needs to be reduced across devices\n",
            "to keep the weights in sync. We note that this all_reduce will grow with the batch size, eventually\n",
            "making data parallelism bandwidth-bound. Furthermore, data parallelism is however no cure-all:\n",
            "since the model is not sharded, memory footprint per device is constant, even as we add more devices.\n",
            "Accordingly, data parallelism alone is constrained to models which fit on a single device.\n",
            "Tensor Parallelism (TP). To share the model across devices, we need to turn to model parallelism.\n",
            "Introduced by Shoeybi et al. (2019) in its most popular form, tensor parallelism splits linear layers\n",
            "in the attention block and MLP in a principled way to reduce communication volume. This can\n",
            "be viewed as an instance of width-wise model parallelism. Specifically, for the simple case of a\n",
            "two-layer MLP, by making the first layer column parallel and the second row parallel, only a single\n",
            "all reduce is necessary to produce the final result–no communication of the intermediary result is\n",
            "required. To propagate the gradient backward, the matrices are transposed, and hence the row parallel\n",
            "layer turn into a column parallel layer and vice-versa: this maintains the the e fficient communication\n",
            "pattern. We illustrate column and row parallelism in Fig. 9. A similar approach can be taken to\n",
            "split attention blocks, splitting the heads across GPUs–wherein the K,Q,V calculations are column\n",
            "parallel and the final projection row parallel, with all computations in between independent of one\n",
            "another between GPUs. Accordingly, entire blocks in Transformers can be e fficiently parallelized\n",
            "this way. Tensor parallel, however, requires both high-bandwith and low-latency interconnect to\n",
            "be effective: accordingly, on current GPU infrastructure, it is constrained within a single node, and\n",
            "cannot e fficiently be used across nodes. Models will thus typically be trained with a tensor parallel\n",
            "degree up to 8; this may still be insu fficient to create small enough shards of the model.\n",
            "Data Parallel\n",
            "Model\n",
            "Replica #0Model\n",
            "Replica #1Reduce Gradients\n",
            "sample\n",
            "#0sample\n",
            "#1sample\n",
            "#2sample\n",
            "#3...\n",
            "Figure 8: Data parallelism creates model replicas on each device and process di fferent samples\n",
            "in parallel. Model replicas are placed on di fferent devices and compute gradients on di fferent data\n",
            "samples in parallel. The gradients are then reduced before the optimization step is carried out.\n",
            "25\n",
            "\n",
            "A Contributions\n",
            "•Engineering & Tooling .\n",
            "– Distributed training codebase. Baptiste Pannier, Daniel Hesslow.\n",
            "–Web data. Guilherme Penedo, Ruxandra Cojocaru (multilingual data) , Quentin Malar-\n",
            "tic(data quality) , Alessandro Cappelli, Hamza Alobeidli.\n",
            "–Curated data. Alessandro Cappelli, Etienne Go ffinet(code data) , Quentin Malartic,\n",
            "Ruxandra Cojocaru, Abdulaziz Alshamsi (books data) .\n",
            "– Inference. Daniel Hesslow, Baptiste Pannier, Guilherme Penedo (deployment) .\n",
            "– Infrastructure. Daniele Mazzotta, Etienne Go ffinet, Guilherme Penedo.\n",
            "– Hardware correctness. Baptiste Pannier, Julien Launay, Daniel Hesslow.\n",
            "•Pretraining .Baptiste Pannier, Julien Launay, Daniel Hesslow.\n",
            "•Research .\n",
            "–Data ablations. Julien Launay, Ruxandra Cojocaru (curriculum learning) , Alessandro\n",
            "Cappelli, Quentin Malartic (fine-grained filters) , Daniel Hesslow.\n",
            "–Architecture ablations. Julien Launay, Daniel Hesslow, Etienne Go ffinet, Quentin\n",
            "Malartic (training objectives) , Baptiste Pannier, Badreddine Noune (optimizers) .\n",
            "–Model finetuning. Quentin Malartic, Alessandro Capelli, Etienne Go ffinet(code\n",
            "specialization) , Guilherme Penedo (human evaluation) , Baptiste Pannier (long-context) ,\n",
            "Julien Launay.\n",
            "– Evaluation. Julien Launay, Daniel Hesslow, Quentin Malartic.\n",
            "–Paper writing. Julien Launay, Daniel Hesslow, Alessandro Cappelli, Baptiste Pannier,\n",
            "Ebtesam Almazrouei.\n",
            "•Leadership .Julien Launay, Ebtesam Almazrouei, Mérouane Debbah.\n",
            "B Acknowledgements\n",
            "We would like to thank the AWS team, in particular Olivier Cruchant, for their support throughout\n",
            "the project, enabling us to eventually scale to up to 4,096 A100s the training of Falcon-180B. We\n",
            "would also like to thank Axel Marmet, Tri Dao, Dan Fu, Colin Ra ffel, Katherine Lee, Thomas Wolf,\n",
            "Iz Beltagy, and Dirk Groeneveld for insightful discussions throughout the project.\n",
            "51\n",
            "\n",
            "Question: tell me about Large-scale distributed training on cloud infrastructure with Gigatron?\n",
            "Helpful Answer:\n",
            "\n",
            "Large-scale distributed training on cloud infrastructure with Gigatron is a technique used to train large language models on massive amounts of data. Gigatron is a proprietary distributed training framework that is based on PyTorch and implements a 3D distributed parallelism strategy combined with ZeRO optimizer sharding. The 3D distributed parallelism strategy allows for fine-grained control over the training process, while ZeRO optimizer sharding helps to reduce memory consumption and\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdvyGq6yIJ_w"
      },
      "source": [
        "Prompt engineering\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyVOON6stMWl"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import PromptTemplate, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_eMYSeUtMUL"
      },
      "outputs": [],
      "source": [
        "_template = \"\"\"\n",
        "this the mistral ai prompot template {question}.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2QOb_4ytMRR"
      },
      "outputs": [],
      "source": [
        "prompt=PromptTemplate(\n",
        "    input_variables=['question','chat_history'],\n",
        "    template=_template\n",
        ")\n",
        "prompt.format(question='llm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWUIy9T-tMNL"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "llmchain=LLMChain(llm=llm,prompt=prompt)\n",
        "llmchain.invoke('Economics of Information Systems')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dmBv7tb2JoT"
      },
      "outputs": [],
      "source": [
        "llmchain.invoke('how can Generative AI increase the efficiency and productivity')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9lKOC_d2Jlw"
      },
      "outputs": [],
      "source": [
        "llmchain.invoke('what is the example for system level for code generation')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LSSW0wa2JjV"
      },
      "outputs": [],
      "source": [
        "llmchain.invoke('what is VAE')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cf038807301e4f14b9c0cccd39c6be7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab24449d7e124da082de460b6a76c71a",
              "IPY_MODEL_4e7e238907494cb4a5da0d84a1f271c7",
              "IPY_MODEL_6ed1149021da451a931edb5e78cb564e"
            ],
            "layout": "IPY_MODEL_e9cc1a19264946a9a10e1e13231c2050"
          }
        },
        "ab24449d7e124da082de460b6a76c71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19340b5991ef40ebb4141ea2e6328ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_c80ff6d9933946408e2e1dba4e69f0ed",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "4e7e238907494cb4a5da0d84a1f271c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5ee8fa99ee4edca3a2f4dfa0091807",
            "max": 967,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03eec5b42b244ccf84790289a5bc541f",
            "value": 967
          }
        },
        "6ed1149021da451a931edb5e78cb564e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed2e3159964d4ed3a322b399ef6aed55",
            "placeholder": "​",
            "style": "IPY_MODEL_5eea716f26b241b88727b4235c58610c",
            "value": " 967/967 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "e9cc1a19264946a9a10e1e13231c2050": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19340b5991ef40ebb4141ea2e6328ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80ff6d9933946408e2e1dba4e69f0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d5ee8fa99ee4edca3a2f4dfa0091807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03eec5b42b244ccf84790289a5bc541f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed2e3159964d4ed3a322b399ef6aed55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eea716f26b241b88727b4235c58610c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ca9d02b04174fd6b85d3c4787c4c18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_128edeafaa3543528f6ae389bc41283a",
              "IPY_MODEL_73618b8d71044e818180c66cfed1edba",
              "IPY_MODEL_8b2f856d92ff42779dc84ceb3c53d792"
            ],
            "layout": "IPY_MODEL_229eaf19a86c420eafb6a461f203de92"
          }
        },
        "128edeafaa3543528f6ae389bc41283a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37ee6356493d4e7cb33c8c743cb6bd15",
            "placeholder": "​",
            "style": "IPY_MODEL_c842e4b235544cfb9d231e0d6a144ec1",
            "value": "tokenizer.model: 100%"
          }
        },
        "73618b8d71044e818180c66cfed1edba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2658a7ff03478394bba977a1c385ca",
            "max": 493443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3f4513a5afba44838fc0e63e7fb30da0",
            "value": 493443
          }
        },
        "8b2f856d92ff42779dc84ceb3c53d792": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c3a530ed3cf4eff8a608ca808a458a1",
            "placeholder": "​",
            "style": "IPY_MODEL_98ba4f441924490fbd2b7a91262f3779",
            "value": " 493k/493k [00:00&lt;00:00, 4.55MB/s]"
          }
        },
        "229eaf19a86c420eafb6a461f203de92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37ee6356493d4e7cb33c8c743cb6bd15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c842e4b235544cfb9d231e0d6a144ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d2658a7ff03478394bba977a1c385ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f4513a5afba44838fc0e63e7fb30da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c3a530ed3cf4eff8a608ca808a458a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ba4f441924490fbd2b7a91262f3779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7608deba9bd449a38f54fa5ed811e76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da990c20d4b44596bc9d2621d97e38ba",
              "IPY_MODEL_82245351de52418ebd069006406ac330",
              "IPY_MODEL_77c90d4e101a4983a79ca0036bca51de"
            ],
            "layout": "IPY_MODEL_182f22fdacbf41fdac3109fa8eec80df"
          }
        },
        "da990c20d4b44596bc9d2621d97e38ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_670cbbcaad1c4f2bb8bc889ac6278288",
            "placeholder": "​",
            "style": "IPY_MODEL_4457737d60d0403eb96c3875a136fc0f",
            "value": "tokenizer.json: 100%"
          }
        },
        "82245351de52418ebd069006406ac330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b30bdecd721d4c6e9bbef5b562db8f20",
            "max": 1795303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_31180285472e4a51aad9025822ab72f7",
            "value": 1795303
          }
        },
        "77c90d4e101a4983a79ca0036bca51de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46662585eab04a8f9a03dfe1ff6772cb",
            "placeholder": "​",
            "style": "IPY_MODEL_b42a642f2780409395f579cc73ba28a7",
            "value": " 1.80M/1.80M [00:00&lt;00:00, 17.6MB/s]"
          }
        },
        "182f22fdacbf41fdac3109fa8eec80df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670cbbcaad1c4f2bb8bc889ac6278288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4457737d60d0403eb96c3875a136fc0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b30bdecd721d4c6e9bbef5b562db8f20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31180285472e4a51aad9025822ab72f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46662585eab04a8f9a03dfe1ff6772cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b42a642f2780409395f579cc73ba28a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "521641c2e79a4e01bceffacbbed95408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46bf0b908c034e218e4b9084466ded98",
              "IPY_MODEL_20ba35bf276e440192e4b6756059db90",
              "IPY_MODEL_f4aa340925c344709214a821b77b70bd"
            ],
            "layout": "IPY_MODEL_cf8b6267b3594beaba2c30b4497b9cac"
          }
        },
        "46bf0b908c034e218e4b9084466ded98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bab7a82c47e4a2b91cc6fd6acbb574d",
            "placeholder": "​",
            "style": "IPY_MODEL_0e97bf36b54243f8bd02323015ffe6fd",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "20ba35bf276e440192e4b6756059db90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d13a003082d4cd6a5b26844b4602e69",
            "max": 72,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_131bf91b48624691bbdc94f12017fc49",
            "value": 72
          }
        },
        "f4aa340925c344709214a821b77b70bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0791297bf2824ff99e94ee7f2613cfff",
            "placeholder": "​",
            "style": "IPY_MODEL_5cb898688b3b4b71ac60b8171eb4ae0f",
            "value": " 72.0/72.0 [00:00&lt;00:00, 950B/s]"
          }
        },
        "cf8b6267b3594beaba2c30b4497b9cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bab7a82c47e4a2b91cc6fd6acbb574d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e97bf36b54243f8bd02323015ffe6fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d13a003082d4cd6a5b26844b4602e69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "131bf91b48624691bbdc94f12017fc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0791297bf2824ff99e94ee7f2613cfff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cb898688b3b4b71ac60b8171eb4ae0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4670f6f23d1a46d08824c107ddf82210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4301e8c6a3f424a9da2d3840ca38651",
              "IPY_MODEL_c128c15e0b264d06acefa68c592c2c95",
              "IPY_MODEL_9a6fb76b58434ce1ba104d6b019ee114"
            ],
            "layout": "IPY_MODEL_9b9e619fe2a84472ae8e7e692223002f"
          }
        },
        "f4301e8c6a3f424a9da2d3840ca38651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9812121fdbe4b8a9527c93f0271f25a",
            "placeholder": "​",
            "style": "IPY_MODEL_f74638f3b6af4d49a62d467935a56b90",
            "value": "config.json: 100%"
          }
        },
        "c128c15e0b264d06acefa68c592c2c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b1d0578e3ea48cc871459aab048100a",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_322f18acb92e44b4a98dc5351cc37709",
            "value": 571
          }
        },
        "9a6fb76b58434ce1ba104d6b019ee114": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2788790db94242b58237f152f622e0",
            "placeholder": "​",
            "style": "IPY_MODEL_1abda04b469b4586831b052e7c672de4",
            "value": " 571/571 [00:00&lt;00:00, 21.9kB/s]"
          }
        },
        "9b9e619fe2a84472ae8e7e692223002f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9812121fdbe4b8a9527c93f0271f25a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74638f3b6af4d49a62d467935a56b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b1d0578e3ea48cc871459aab048100a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "322f18acb92e44b4a98dc5351cc37709": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef2788790db94242b58237f152f622e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1abda04b469b4586831b052e7c672de4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ef5ce43e7fa44718b2cf9ee4898a1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2481a0ec58814004aadd4613adcd7822",
              "IPY_MODEL_d27648abb27d4bf0b4ef7c01f2edb48b",
              "IPY_MODEL_fbbee23c09f54ef5b8fb9a921fc83fbc"
            ],
            "layout": "IPY_MODEL_4f04cad715cd481d8fc473300447fd28"
          }
        },
        "2481a0ec58814004aadd4613adcd7822": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_336db333b45543819eee69a68c417933",
            "placeholder": "​",
            "style": "IPY_MODEL_b9a4a13c67194bd3a5ae015fa72d7932",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "d27648abb27d4bf0b4ef7c01f2edb48b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4b230996794fbebea74816d372c6c4",
            "max": 25125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e570ec5210414550a790d415c475a411",
            "value": 25125
          }
        },
        "fbbee23c09f54ef5b8fb9a921fc83fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_910cb9d9d09d49558515f7c39b8a9a00",
            "placeholder": "​",
            "style": "IPY_MODEL_a990f9a6e4c54b62a55851c93bd66079",
            "value": " 25.1k/25.1k [00:00&lt;00:00, 770kB/s]"
          }
        },
        "4f04cad715cd481d8fc473300447fd28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "336db333b45543819eee69a68c417933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a4a13c67194bd3a5ae015fa72d7932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db4b230996794fbebea74816d372c6c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e570ec5210414550a790d415c475a411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "910cb9d9d09d49558515f7c39b8a9a00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a990f9a6e4c54b62a55851c93bd66079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ed190d0dfc447a5a385e3ea8745691f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d10b12fda31a4b0fac3002fea35b419f",
              "IPY_MODEL_059e6fc879d54c9bacc2f49b28136d94",
              "IPY_MODEL_e6fddf78977d4dfe9b583bbe6895a004"
            ],
            "layout": "IPY_MODEL_a420ce9642b64bd581d473c6b6a0c0b9"
          }
        },
        "d10b12fda31a4b0fac3002fea35b419f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7796597221e24a1ebacac6c7b4725bb2",
            "placeholder": "​",
            "style": "IPY_MODEL_db3171252414451a9a37162e1a4537b5",
            "value": "Downloading shards: 100%"
          }
        },
        "059e6fc879d54c9bacc2f49b28136d94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ba042a699f64602af45fa83207f8943",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ead64e77c8c34b62b319ebb043f23231",
            "value": 2
          }
        },
        "e6fddf78977d4dfe9b583bbe6895a004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99e9506038994010a432ae057339733c",
            "placeholder": "​",
            "style": "IPY_MODEL_85398197a1fc44abb6e86e1340598719",
            "value": " 2/2 [02:09&lt;00:00, 61.79s/it]"
          }
        },
        "a420ce9642b64bd581d473c6b6a0c0b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7796597221e24a1ebacac6c7b4725bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db3171252414451a9a37162e1a4537b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ba042a699f64602af45fa83207f8943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ead64e77c8c34b62b319ebb043f23231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99e9506038994010a432ae057339733c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85398197a1fc44abb6e86e1340598719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a4d2fc4191c4f2699c968668dda27ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4b7a36ad4a254f9586f5a77ff7e2d522",
              "IPY_MODEL_1eb3d67e7de04d94a961e6596d546abc",
              "IPY_MODEL_78dec8af0b894a8881f180c8ac5c96a1"
            ],
            "layout": "IPY_MODEL_c57c6b3202974bce956216e3fc67b017"
          }
        },
        "4b7a36ad4a254f9586f5a77ff7e2d522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96e1b0f72a2345ca9c646408c3e1f2a6",
            "placeholder": "​",
            "style": "IPY_MODEL_b971f441f202452c856637f9c2e9b054",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "1eb3d67e7de04d94a961e6596d546abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83329472e0054cd4ba1f1040001bd382",
            "max": 9942981696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a1097da66154198b7032849fe6c056b",
            "value": 9942981696
          }
        },
        "78dec8af0b894a8881f180c8ac5c96a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1be905ee5f34c02b4881dbc08371730",
            "placeholder": "​",
            "style": "IPY_MODEL_59581290642e40cfa979ad61a52ce226",
            "value": " 9.94G/9.94G [01:22&lt;00:00, 163MB/s]"
          }
        },
        "c57c6b3202974bce956216e3fc67b017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e1b0f72a2345ca9c646408c3e1f2a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b971f441f202452c856637f9c2e9b054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83329472e0054cd4ba1f1040001bd382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a1097da66154198b7032849fe6c056b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1be905ee5f34c02b4881dbc08371730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59581290642e40cfa979ad61a52ce226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd83353a13694f8a91a3b21ba71e8b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6e90dd9e8bc4406be7af3c471dd90d0",
              "IPY_MODEL_132c74b460fa4e3c9b27b50a803c42d7",
              "IPY_MODEL_f7dfbf6fd1c4476b8f2289607f1bbc6d"
            ],
            "layout": "IPY_MODEL_8a6525bcae714c58a2fdc38cecb959c1"
          }
        },
        "e6e90dd9e8bc4406be7af3c471dd90d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d45766943a1042a0aab04c6deeb5ea3b",
            "placeholder": "​",
            "style": "IPY_MODEL_d59d78cb48db4e3e89eccd70166249c0",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "132c74b460fa4e3c9b27b50a803c42d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2e570b127d441c480473f0c164aea6b",
            "max": 4540516344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4fbc505590934d9f9377ba9518a7138c",
            "value": 4540516344
          }
        },
        "f7dfbf6fd1c4476b8f2289607f1bbc6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_813e705a20b6466181e52f815d058039",
            "placeholder": "​",
            "style": "IPY_MODEL_1a694b86cb6344beb6e2fac50f3a1df2",
            "value": " 4.54G/4.54G [00:46&lt;00:00, 107MB/s]"
          }
        },
        "8a6525bcae714c58a2fdc38cecb959c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45766943a1042a0aab04c6deeb5ea3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d59d78cb48db4e3e89eccd70166249c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2e570b127d441c480473f0c164aea6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fbc505590934d9f9377ba9518a7138c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "813e705a20b6466181e52f815d058039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a694b86cb6344beb6e2fac50f3a1df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf3307018179426190b64139d4121b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8101634aef0d40f5ae7c12bc2ef3ab68",
              "IPY_MODEL_9b14627b6c0147f78f6b1e0cabbbe134",
              "IPY_MODEL_1181bbde004a40d1b5dd4d1059495f17"
            ],
            "layout": "IPY_MODEL_34ef3a623efa4277afaec8ed06091b0f"
          }
        },
        "8101634aef0d40f5ae7c12bc2ef3ab68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d8a97cf8713481d98e162c11a071eb1",
            "placeholder": "​",
            "style": "IPY_MODEL_ee4a13ff895942f2a70414c7846e54bc",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9b14627b6c0147f78f6b1e0cabbbe134": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e69c3c0486db41699700529ebc55ef07",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9aa6c8870e064c12a30a43eb96afcb8c",
            "value": 2
          }
        },
        "1181bbde004a40d1b5dd4d1059495f17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1194bb3dba814bfd989ec12d9ef18b8f",
            "placeholder": "​",
            "style": "IPY_MODEL_244fefbc6130404898c4052a64280d08",
            "value": " 2/2 [01:08&lt;00:00, 32.34s/it]"
          }
        },
        "34ef3a623efa4277afaec8ed06091b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8a97cf8713481d98e162c11a071eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4a13ff895942f2a70414c7846e54bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e69c3c0486db41699700529ebc55ef07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aa6c8870e064c12a30a43eb96afcb8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1194bb3dba814bfd989ec12d9ef18b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244fefbc6130404898c4052a64280d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fadd50330c84c9ea5b11a770313735c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cae21ce7492042bda33fb2977315b615",
              "IPY_MODEL_adf76ed984ee44ac90ba6261d9ebfd9e",
              "IPY_MODEL_3903687f16404d52ba6257c597cf20b3"
            ],
            "layout": "IPY_MODEL_c21f855ccb1e4789be6de07804e09fe0"
          }
        },
        "cae21ce7492042bda33fb2977315b615": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd9ff14d0af84ea0ade0c861ed5c82be",
            "placeholder": "​",
            "style": "IPY_MODEL_7ad9639ba00e401c917ea8bfc35ff15b",
            "value": "generation_config.json: 100%"
          }
        },
        "adf76ed984ee44ac90ba6261d9ebfd9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b582f00439e46ce90f1ffdf7c89b9c3",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b80eab38f95740a2bc501c4587b11353",
            "value": 116
          }
        },
        "3903687f16404d52ba6257c597cf20b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f158a077dd64ac1a70d0887cdea5458",
            "placeholder": "​",
            "style": "IPY_MODEL_c4b2e8f632e94e8f81bebd54dcb3f00c",
            "value": " 116/116 [00:00&lt;00:00, 8.04kB/s]"
          }
        },
        "c21f855ccb1e4789be6de07804e09fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd9ff14d0af84ea0ade0c861ed5c82be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ad9639ba00e401c917ea8bfc35ff15b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b582f00439e46ce90f1ffdf7c89b9c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80eab38f95740a2bc501c4587b11353": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f158a077dd64ac1a70d0887cdea5458": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b2e8f632e94e8f81bebd54dcb3f00c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aea924fbf6434b54916188d4f436e9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b9d7fbbcdec48efb7724f9b4a54136e",
              "IPY_MODEL_5c596f70a8284b56bb0d6963f8b88dbf",
              "IPY_MODEL_8260e5fe9be24387ac79d04a351b4557"
            ],
            "layout": "IPY_MODEL_1cc1c5df47624a4fba553ee64afd6a51"
          }
        },
        "0b9d7fbbcdec48efb7724f9b4a54136e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0f4e8e29cf04fd1aa1d7153a4884d84",
            "placeholder": "​",
            "style": "IPY_MODEL_a4cff59bb91f47559808242048ff22a7",
            "value": "modules.json: 100%"
          }
        },
        "5c596f70a8284b56bb0d6963f8b88dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_337b7cde41494bbc9ca64491a98bf19d",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7aed852e7c514a37825adee20a847817",
            "value": 349
          }
        },
        "8260e5fe9be24387ac79d04a351b4557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9ada69891ee49bdb93a984102e4c966",
            "placeholder": "​",
            "style": "IPY_MODEL_05b53a8f5f6e452592a1b6c199a33b90",
            "value": " 349/349 [00:00&lt;00:00, 22.6kB/s]"
          }
        },
        "1cc1c5df47624a4fba553ee64afd6a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f4e8e29cf04fd1aa1d7153a4884d84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4cff59bb91f47559808242048ff22a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "337b7cde41494bbc9ca64491a98bf19d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aed852e7c514a37825adee20a847817": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9ada69891ee49bdb93a984102e4c966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b53a8f5f6e452592a1b6c199a33b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88b1bb8bfacd410e966ec661509ff265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49e5521c64834132910998470880bf6b",
              "IPY_MODEL_4ce9843ebfcf41298b9577bbda98caf9",
              "IPY_MODEL_f71592df56b340f195ee5b30d3880487"
            ],
            "layout": "IPY_MODEL_724cc39d099144e79816c6b46375caa3"
          }
        },
        "49e5521c64834132910998470880bf6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1c6ac82c4ee46b1a5d1dc435e72b5a3",
            "placeholder": "​",
            "style": "IPY_MODEL_26e7b92c2cd34136a381b7cbb6ef631d",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "4ce9843ebfcf41298b9577bbda98caf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70b6d32c4ea4acbbcf788dac120ce27",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a8ce55de76d41aa83bb09d48fe5d4e7",
            "value": 116
          }
        },
        "f71592df56b340f195ee5b30d3880487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b3d278ab1c40b5b68a0006f570cfa6",
            "placeholder": "​",
            "style": "IPY_MODEL_ac1223bfd2d74f1b9009e362cdf89988",
            "value": " 116/116 [00:00&lt;00:00, 6.44kB/s]"
          }
        },
        "724cc39d099144e79816c6b46375caa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1c6ac82c4ee46b1a5d1dc435e72b5a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26e7b92c2cd34136a381b7cbb6ef631d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a70b6d32c4ea4acbbcf788dac120ce27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a8ce55de76d41aa83bb09d48fe5d4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32b3d278ab1c40b5b68a0006f570cfa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac1223bfd2d74f1b9009e362cdf89988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e638e36b2d454c17be01accdef691061": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f79b883294da4f07a682c61d10fa1d01",
              "IPY_MODEL_5c47acc687bd4c848f6460ec0d6a144b",
              "IPY_MODEL_ffaae84b5f1e4a3d8f282ca78e5c7277"
            ],
            "layout": "IPY_MODEL_9c9c7873693248fe8ad3351f7e41587f"
          }
        },
        "f79b883294da4f07a682c61d10fa1d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f2fe7f5c7c4b3d85bc0c5e41222726",
            "placeholder": "​",
            "style": "IPY_MODEL_5f6e2c9759c647a2b0eb9b9ee93adf62",
            "value": "README.md: 100%"
          }
        },
        "5c47acc687bd4c848f6460ec0d6a144b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfd7c1e87b6e4aefa82261c7ca5a5594",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a29de968f514c93a12e3c9edb36437b",
            "value": 10659
          }
        },
        "ffaae84b5f1e4a3d8f282ca78e5c7277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1470812be2f467fb47ee38c7d3cb346",
            "placeholder": "​",
            "style": "IPY_MODEL_1406ff5091d94514b5343542012b9796",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 772kB/s]"
          }
        },
        "9c9c7873693248fe8ad3351f7e41587f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f2fe7f5c7c4b3d85bc0c5e41222726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f6e2c9759c647a2b0eb9b9ee93adf62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfd7c1e87b6e4aefa82261c7ca5a5594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a29de968f514c93a12e3c9edb36437b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1470812be2f467fb47ee38c7d3cb346": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1406ff5091d94514b5343542012b9796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c160eac44ac94d8ba96e106915709ad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_393cd6f77ddf46f4a65f34beaf20e073",
              "IPY_MODEL_8e6dfb1ca38849dfaad59abd4befe95b",
              "IPY_MODEL_a97137fa10f4444192f8066a0613639f"
            ],
            "layout": "IPY_MODEL_9677234cdd294fc0ba34babf7cd99dc9"
          }
        },
        "393cd6f77ddf46f4a65f34beaf20e073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f36cad760a5e40a8a9e55b5ebb80984e",
            "placeholder": "​",
            "style": "IPY_MODEL_181162ddd0ae40fbb31add09f0bd0ebb",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "8e6dfb1ca38849dfaad59abd4befe95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f8b5d38623d48c789d8f03f2b587def",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a468cf68ffd4fc9b29fb9b17aa09b64",
            "value": 53
          }
        },
        "a97137fa10f4444192f8066a0613639f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09ea6e78d3547b3a39783dab2382f8b",
            "placeholder": "​",
            "style": "IPY_MODEL_685405a0f74f44d3b2b2776a96ace33f",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.51kB/s]"
          }
        },
        "9677234cdd294fc0ba34babf7cd99dc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f36cad760a5e40a8a9e55b5ebb80984e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181162ddd0ae40fbb31add09f0bd0ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f8b5d38623d48c789d8f03f2b587def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a468cf68ffd4fc9b29fb9b17aa09b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b09ea6e78d3547b3a39783dab2382f8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685405a0f74f44d3b2b2776a96ace33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c85eb691af341c69006f5ab29ecc720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c78f1e08b31845048f28186b66b5ea0f",
              "IPY_MODEL_ce6e04fd75d6487582676a91ce43afe3",
              "IPY_MODEL_ce72ae662cba4196893855335657f5a5"
            ],
            "layout": "IPY_MODEL_64df1f92c4df4234b26d2e5bba8408a2"
          }
        },
        "c78f1e08b31845048f28186b66b5ea0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_465d6fa2852b439f88f4e92df30e09f9",
            "placeholder": "​",
            "style": "IPY_MODEL_f6de3c571b5f4f79a7da28d0bda7fdaf",
            "value": "config.json: 100%"
          }
        },
        "ce6e04fd75d6487582676a91ce43afe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43666a38defe45f4be91d332a82a5884",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ade71f17c11f4e47b47c3ce0c70e9e98",
            "value": 612
          }
        },
        "ce72ae662cba4196893855335657f5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9f7c19b102044ef881ad14003ec2bd8",
            "placeholder": "​",
            "style": "IPY_MODEL_137a183f65574731a1fbd83047065364",
            "value": " 612/612 [00:00&lt;00:00, 29.3kB/s]"
          }
        },
        "64df1f92c4df4234b26d2e5bba8408a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "465d6fa2852b439f88f4e92df30e09f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6de3c571b5f4f79a7da28d0bda7fdaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43666a38defe45f4be91d332a82a5884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ade71f17c11f4e47b47c3ce0c70e9e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9f7c19b102044ef881ad14003ec2bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "137a183f65574731a1fbd83047065364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8b550650ab34694bf84e754cedad975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e82f22de91b404cb483891107e31717",
              "IPY_MODEL_638bfaa88a5440b1a4d2a1f018430942",
              "IPY_MODEL_dbc5b626a9e648e1a74c6167ae86d257"
            ],
            "layout": "IPY_MODEL_80061d9270da4e429acb1afc7e3e3036"
          }
        },
        "6e82f22de91b404cb483891107e31717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4694e0f3e85640ffa5d5e74c9f91efd3",
            "placeholder": "​",
            "style": "IPY_MODEL_b8ddc81f8dfa41ca87b687e7eec11ba1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "638bfaa88a5440b1a4d2a1f018430942": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e363b3ce6ae947639992024c79f00bb9",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b0665d96c2244a48071aeaf801a4600",
            "value": 90888945
          }
        },
        "dbc5b626a9e648e1a74c6167ae86d257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e38a976bf01e45d2aac837b140fc9a59",
            "placeholder": "​",
            "style": "IPY_MODEL_b413ecdf199a4af6b61f4b80a86b39f8",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 178MB/s]"
          }
        },
        "80061d9270da4e429acb1afc7e3e3036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4694e0f3e85640ffa5d5e74c9f91efd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ddc81f8dfa41ca87b687e7eec11ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e363b3ce6ae947639992024c79f00bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0665d96c2244a48071aeaf801a4600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e38a976bf01e45d2aac837b140fc9a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b413ecdf199a4af6b61f4b80a86b39f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b736e9d43d0480cbf97fdf5ee2351bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ef44b4182b141c68e01868837d87245",
              "IPY_MODEL_60ecdb6154f142df873a3f4fb5eea456",
              "IPY_MODEL_1de84ffd043c408aae675f0e0e7cdc7f"
            ],
            "layout": "IPY_MODEL_e321e912cb5c48bcaebcdbf101a58b73"
          }
        },
        "2ef44b4182b141c68e01868837d87245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ea19ac9241743aaad401ce031ecbb81",
            "placeholder": "​",
            "style": "IPY_MODEL_0b6da214052c46248abd92157289b48e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "60ecdb6154f142df873a3f4fb5eea456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a99c31f7064141caaf567e0274dd69e6",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdd686cc77ff4d40814497b4eb965fb5",
            "value": 350
          }
        },
        "1de84ffd043c408aae675f0e0e7cdc7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142a617f1db043c79d08780d07b1ddd5",
            "placeholder": "​",
            "style": "IPY_MODEL_66f014111e314932ab48f840fc1f3c3f",
            "value": " 350/350 [00:00&lt;00:00, 21.5kB/s]"
          }
        },
        "e321e912cb5c48bcaebcdbf101a58b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea19ac9241743aaad401ce031ecbb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b6da214052c46248abd92157289b48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a99c31f7064141caaf567e0274dd69e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdd686cc77ff4d40814497b4eb965fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "142a617f1db043c79d08780d07b1ddd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66f014111e314932ab48f840fc1f3c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a43ad7ebbe94250b40aa0021eaae96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98255f279521431a8e053c9e4ef37bfd",
              "IPY_MODEL_db30f2b106344be69093c496e2df1dda",
              "IPY_MODEL_fc4d56d7ca0b4d0285337f7eb4854dee"
            ],
            "layout": "IPY_MODEL_dc3de42613674acfa3fcd0c6391c2bdc"
          }
        },
        "98255f279521431a8e053c9e4ef37bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf698b7de814d1784a63e6dd45301c2",
            "placeholder": "​",
            "style": "IPY_MODEL_b23aa2e6c81448f0b9737e622a9edc45",
            "value": "vocab.txt: 100%"
          }
        },
        "db30f2b106344be69093c496e2df1dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91c2e6a4d07a4cb480cd95e99e3bd535",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2175f1736d3e4baba7f4eb2caeb494f6",
            "value": 231508
          }
        },
        "fc4d56d7ca0b4d0285337f7eb4854dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8207616a2a034e3796c946d5703d1372",
            "placeholder": "​",
            "style": "IPY_MODEL_325c621b9f604b6e8b1688c0f7bd1e27",
            "value": " 232k/232k [00:00&lt;00:00, 5.70MB/s]"
          }
        },
        "dc3de42613674acfa3fcd0c6391c2bdc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf698b7de814d1784a63e6dd45301c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23aa2e6c81448f0b9737e622a9edc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c2e6a4d07a4cb480cd95e99e3bd535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2175f1736d3e4baba7f4eb2caeb494f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8207616a2a034e3796c946d5703d1372": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325c621b9f604b6e8b1688c0f7bd1e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c63c48661992455ba278355d5b39fcee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc77744ccdc84cb9b7a779b5f8d47d45",
              "IPY_MODEL_9df28132ca084f0c8b39609d0e49be61",
              "IPY_MODEL_ac2d08d5690141a99b2e71fffda3ec2e"
            ],
            "layout": "IPY_MODEL_6025cff1bf6e4311a1e401f232bf14f6"
          }
        },
        "bc77744ccdc84cb9b7a779b5f8d47d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a45604062e4363b454e43ac4f069fa",
            "placeholder": "​",
            "style": "IPY_MODEL_81475e26459c45508fecaebc24c487b5",
            "value": "tokenizer.json: 100%"
          }
        },
        "9df28132ca084f0c8b39609d0e49be61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eab7fafa564491cabb16e310b3195f6",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d5b85047853417b88502c16c40881b7",
            "value": 466247
          }
        },
        "ac2d08d5690141a99b2e71fffda3ec2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64d3a894954e4d5282247d515e0081c5",
            "placeholder": "​",
            "style": "IPY_MODEL_0a054397d5dd499bae581ef7a147a82b",
            "value": " 466k/466k [00:00&lt;00:00, 21.8MB/s]"
          }
        },
        "6025cff1bf6e4311a1e401f232bf14f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a45604062e4363b454e43ac4f069fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81475e26459c45508fecaebc24c487b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eab7fafa564491cabb16e310b3195f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d5b85047853417b88502c16c40881b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64d3a894954e4d5282247d515e0081c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a054397d5dd499bae581ef7a147a82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5902e1345c974c96944eb8a89754462e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e66ef21324e404c9dbd2bbcecf5de31",
              "IPY_MODEL_9c00b6ab658f49bfa49a5f76e664ab65",
              "IPY_MODEL_86552490ff9e4b1482cb0ce9f845130c"
            ],
            "layout": "IPY_MODEL_0fa1a2d83c4648f08646db93465d44a1"
          }
        },
        "7e66ef21324e404c9dbd2bbcecf5de31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fdd6e64ce974f22b865f5a8100048d5",
            "placeholder": "​",
            "style": "IPY_MODEL_58babb1b9ae64dafb276fbbdf8fb80f2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9c00b6ab658f49bfa49a5f76e664ab65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7994d02d09624a20bfed588f6fca2263",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b7617bee5b846ada71fc403c0147bca",
            "value": 112
          }
        },
        "86552490ff9e4b1482cb0ce9f845130c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d59af107fe4582a7aaf9a3809c4b35",
            "placeholder": "​",
            "style": "IPY_MODEL_f93104b503134cf79d57bf2fa02b9288",
            "value": " 112/112 [00:00&lt;00:00, 6.28kB/s]"
          }
        },
        "0fa1a2d83c4648f08646db93465d44a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdd6e64ce974f22b865f5a8100048d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58babb1b9ae64dafb276fbbdf8fb80f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7994d02d09624a20bfed588f6fca2263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b7617bee5b846ada71fc403c0147bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6d59af107fe4582a7aaf9a3809c4b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f93104b503134cf79d57bf2fa02b9288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9929f0a15844a938a7cd414f2ba2b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_137362a1b0e24e0fb3f0fa89de6de5d9",
              "IPY_MODEL_a4c91310b68441faae8f6dbf4ed4c0e4",
              "IPY_MODEL_ba5aa0194c2f4c869b547bed095ce9a0"
            ],
            "layout": "IPY_MODEL_02f59cb878a7417cb77949c5895e3964"
          }
        },
        "137362a1b0e24e0fb3f0fa89de6de5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cc60df6711344d2ad22b80718c8ac48",
            "placeholder": "​",
            "style": "IPY_MODEL_f1e14d196fcf462ba35921db32ac7420",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "a4c91310b68441faae8f6dbf4ed4c0e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9139521fe0914103b37a78ce105add60",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea5764654b0f4257af468262b33a4f3a",
            "value": 190
          }
        },
        "ba5aa0194c2f4c869b547bed095ce9a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa85216e4dc043c4a988157539243a2b",
            "placeholder": "​",
            "style": "IPY_MODEL_f0e830b04d4242c582b3a463f4b293af",
            "value": " 190/190 [00:00&lt;00:00, 10.7kB/s]"
          }
        },
        "02f59cb878a7417cb77949c5895e3964": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc60df6711344d2ad22b80718c8ac48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1e14d196fcf462ba35921db32ac7420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9139521fe0914103b37a78ce105add60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5764654b0f4257af468262b33a4f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa85216e4dc043c4a988157539243a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0e830b04d4242c582b3a463f4b293af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}